<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
    <head>
        <meta data-react-helmet="true" charset="utf-8" />
        <link rel="preload" as="style" href="/allStyles?hash=623084ac09473955d791117732572249970c772b9f5a5512db642ad515f40e9d&amp;theme=%7B%22name%22%3A%22default%22%7D" />
        <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/icon?family=Material+Icons" />
        <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.css@7.0.0/themes/reset-min.css" />
        <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" />
        <link rel="stylesheet" type="text/css" href="https://use.typekit.net/jvr1gjm.css" />
        <script type="text/javascript" async="async" src="https://www.google-analytics.com/plugins/ua/linkid.js"></script>
        <script type="text/javascript" async="async" src="https://www.googletagmanager.com/gtag/js?id=G-X5J07GB66Y&amp;l=dataLayer&amp;cx=c"></script>
        <script type="text/javascript" async="async" src="https://www.google-analytics.com/analytics.js"></script>
        <script type="text/javascript" async="async" src="https://www.gstatic.com/recaptcha/releases/iRvKkcsnpNcOYYwhqaQxPITz/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-Lv4HRVLeeMVUjWoB3lBi6IVF9Kln/zF/3k51b1jOw+VXvsX+SWMdauP8tbQdeAGy"></script>
        <script async="async" src="https://www.googletagmanager.com/gtm.js?id=GTM-TRC765W"></script>
        <script>
        <![CDATA[
        window.publicInstanceSettings = {"forumType":"LessWrong","title":"LessWrong","siteNameWithArticle":"LessWrong","sentry":{"url":"https://1ab1949fc8d04608b43132f37bb2a1b0@sentry.io/1301611","environment":"production","release":"69f0f3c5d57b596e8249571383f8a280eff9bb23"},"debug":false,"aboutPostId":"bJ2haLkcGeLtTWaD5","faqPostId":"2rWKkWuPrgTMpLRbp","contactPostId":"ehcYkvyz7dh9L7Wt8","expectedDatabaseId":"production","tagline":"A community blog devoted to refining the art of rationality","faviconUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico","forumSettings":{"headerTitle":"LESSWRONG","shortForumTitle":"LW","tabTitle":"LessWrong"},"analytics":{"environment":"lesswrong.com"},"testServer":false,"fmCrosspost":{"siteName":"the EA Forum","baseUrl":"https://forum.effectivealtruism.org/"},"allowTypeIIIPlayer":true}
        ]]>
        </script>
        <script defer="defer" src="/js/bundle.js?hash=6497258acefa89ad8f7f227b5f015641bdaea7f737b6f41bbd141497d2d23aca"></script>
        <title>
            Predictable updating about AI risk — LessWrong
        </title>
        <meta data-react-helmet="true" http-equiv="Accept-CH" content="DPR, Viewport-Width, Width" />
        <meta data-react-helmet="true" property="og:title" content="Predictable updating about AI risk — LessWrong" />
        <meta data-react-helmet="true" name="description" content="&gt; &quot;This present moment used to be the unimaginable future.&quot; &gt; &gt; - Stewart Brand …" />
        <meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1" />
        <meta data-react-helmet="true" name="twitter:card" content="summary_large_image" />
        <meta data-react-helmet="true" name="twitter:image:src" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee" />
        <meta data-react-helmet="true" name="twitter:description" content="&gt; &quot;This present moment used to be the unimaginable future.&quot; &gt; &gt; - Stewart Brand …" />
        <meta data-react-helmet="true" property="og:type" content="article" />
        <meta data-react-helmet="true" property="og:url" content="https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk" />
        <meta data-react-helmet="true" property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee" />
        <meta data-react-helmet="true" property="og:description" content="&gt; &quot;This present moment used to be the unimaginable future.&quot; &gt; &gt; - Stewart Brand …" />
        <meta data-react-helmet="true" name="citation_title" content="Predictable updating about AI risk" />
        <meta data-react-helmet="true" name="citation_author" content="Joe Carlsmith" />
        <link data-react-helmet="true" rel="canonical" href="https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk" />
        <link data-react-helmet="true" rel="shortcut icon" href="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico" />
        <link data-react-helmet="true" rel="alternate" type="application/rss+xml" href="https://www.lesswrong.com/feed.xml" />
        <script data-react-helmet="true" type="application/ld+json">
        <![CDATA[
        {"@context":"http://schema.org","@type":"DiscussionForumPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.lesswrong.com/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk"},"headline":"Predictable updating about AI risk","description":"> \"This present moment used to be the unimaginable future.\"\n> \n> - Stewart Brand …","datePublished":"2023-05-08T21:53:34.730Z","about":[{"@type":"Thing","name":"World Modeling","url":"https://www.lesswrong.com/tag/world-modeling","description":"<p><strong>World Modeling<\/strong> is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, <i>finding out how the damn thing works,<\/i> and writing it down for the rest of us.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.<\/i><\/p><p>—<a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u>Twelve Virtues of Rationality<\/u><\/a><\/p><\/blockquote><hr><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics<\/a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics<\/a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology<\/a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics<\/a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming<\/a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers<\/a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews<\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History<\/a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics<\/a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies<\/a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease<\/a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis<\/a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution<\/a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness<\/a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution<\/a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine<\/a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience<\/a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus<\/a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence<\/a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review<\/a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models<\/a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science<\/a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets<\/a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism<\/a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><p>&nbsp;<\/p><h2>A definition by elimination<\/h2><p>Properly considered, the overwhelming majority of content LessWrong is about <i>modeling how the world is<\/i>, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a>, ... <\/p>"},{"@type":"Thing","name":"AI","url":"https://www.lesswrong.com/tag/ai","description":"<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment&nbsp;<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">Eliciting Latent Knowledge (ELK)<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability<\/a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://aisafety.world/map/\">Full map here<\/a><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center\">Alignment Resea<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>..."}],"author":[{"@type":"Person","name":"Joe Carlsmith","url":"https://www.lesswrong.com/users/joe-carlsmith"}],"interactionStatistic":[{"@type":"InteractionCounter","interactionType":{"@type":"http://schema.org/CommentAction"},"userInteractionCount":20},{"@type":"InteractionCounter","interactionType":{"@type":"http://schema.org/LikeAction"},"userInteractionCount":273}]}
        ]]>
        </script>
        <script>
        <![CDATA[
        var tabId = "sf9Y6ZchyzNA5TNnv"
        ]]>
        </script>
        <script>
        <![CDATA[
        var publicSettings = {"forum":{"numberOfDays":10,"postInterval":30,"numberOfWeeks":4,"numberOfYears":4,"maxPostsPerDay":5,"numberOfMonths":4},"type3":{"cutoffDate":"2023-07-01"},"locale":"en-US","mapbox":{"apiKey":"pk.eyJ1IjoiaGFicnlrYSIsImEiOiJjaWxvcnhidzgwOGlodHJrbmJ2bmVmdjRtIn0.inr-_5rWOOslGQxY8iDFOA"},"petrov":{"afterTime":1664247600000,"beforeTime":1664161200000,"petrovPostId":"KTEciTeFwL2tTujZk","petrovServerUrl":"https://forum.effectivealtruism.org/graphql","petrovGamePostId":"KTEciTeFwL2tTujZk"},"reacts":{"addNewReactKarmaThreshold":10,"downvoteExistingReactKarmaThreshold":20,"addNameToExistingReactKarmaThreshold":5},"stripe":{"publicKey":"pk_live_51HtKAwA2QvoATZCZiy9f2nc6hA52YS1BE81cFu9FEV1IKar0Bwx6hIpxxxYHnhaxO9KM7kRYofZId3sUUI7Q0NeO00tGni3Wza"},"algolia":{"appId":"Z0GR6EXQHD","searchKey":"0b1d20b957917dbb5e1c2f3ad1d04ee2","indexPrefix":"test_","autoSyncIndexes":false},"logoUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1498011194/LessWrong_Logo_skglnw.svg","ckEditor":{"uploadUrl":"https://39669.cke-cs.com/easyimage/upload/","webSocketUrl":"39669.cke-cs.com/ws"},"hasEvents":true,"logRocket":{"apiKey":"mtnxzn/lesswrong","sampleDensity":5},"reCaptcha":{"apiKey":"6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"},"siteImage":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg","cloudinary":{"cloudName":"lesswrong-2-0","uploadPresetBanner":"navcjwf7","uploadPresetGridImage":"tz0mgw2s","uploadPresetSocialPreview":"nn5tppry"},"googleMaps":{"apiKey":"AIzaSyA3C48rl26gynG3qIuNuS-3Bh_Zz9jFXkY"},"annualReview":{"end":"2023-02-07T12:00:00Z","start":"2022-12-01T18:00:00Z","reviewPhaseEnd":"2023-01-16T08:00:00Z","votingPhaseEnd":"2022-02-01T08:00:00Z","nominationPhaseEnd":"2022-12-15T08:00:00Z","votingResultsPostId":"TSaJ9Zcvc3KWh3bjX","announcementPostPath":"/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion"},"intercomAppId":"wtb8z7sj","commentInterval":15,"moderationEmail":"team@lesswrong.com","timeDecayFactor":1.15,"googleTagManager":{"apiKey":"GTM-TRC765W"},"gatherTownMessage":"Schelling social hours on Tues 1pm and Thurs 6pm PT","gardenOpenToPublic":false,"legacyRouteAcronym":"lw","frontpageScoreBonus":0,"defaultVisibilityTags":[{"tagId":"Ng8Gice9KNkncxqcj","tagName":"Rationality","filterMode":10},{"tagId":"3uE2pXvbcnS9nnZRE","tagName":"World Modeling","filterMode":10}],"enableGoodHeartProject":false,"maxDocumentsPerRequest":5000,"defaultModeratorComments":[{"id":"FfMok764BCY6ScqWm","label":"Option A"},{"id":"yMHoNoYZdk5cKa3wQ","label":"Option B"}],"newUserIconKarmaThreshold":50,"hideUnreviewedAuthorComments":"2023-04-04T18:54:35.895Z","gatherTownUserTrackingIsBroken":true,"postModerationWarningCommentId":"sLay9Tv65zeXaQzR4","commentModerationWarningCommentId":"LbGNE5Ssnvs6MYnLu","firstCommentAcknowledgeMessageCommentId":"QgwD7PkQHFp3nfhjj"}
        ]]>
        </script>
        <script>
        <![CDATA[
        window.themeOptions = {"name":"default"}
        ]]>
        </script>
        <style id="jss-insertion-point"></style>
        <style data-jss="" data-meta="MuiSvgIcon">
        <![CDATA[
        .MuiSvgIcon-root {
        fill: currentColor;
        width: 1em;
        height: 1em;
        display: inline-block;
        font-size: 24px;
        transition: fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
        user-select: none;
        flex-shrink: 0;
        }
        .MuiSvgIcon-colorPrimary {
        color: #5f9b65;
        }
        .MuiSvgIcon-colorSecondary {
        color: #5f9b65;
        }
        .MuiSvgIcon-colorAction {
        color: rgba(0, 0, 0, 0.54);
        }
        .MuiSvgIcon-colorError {
        color: #bf360c;
        }
        .MuiSvgIcon-colorDisabled {
        color: rgba(0, 0, 0, 0.26);
        }
        .MuiSvgIcon-fontSizeInherit {
        font-size: inherit;
        }
        .MuiSvgIcon-fontSizeSmall {
        font-size: 20px;
        }
        .MuiSvgIcon-fontSizeLarge {
        font-size: 36px;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiToolbar">
        <![CDATA[
        .MuiToolbar-root {
        display: flex;
        position: relative;
        align-items: center;
        }
        .MuiToolbar-gutters {
        padding-left: 16px;
        padding-right: 16px;
        }
        @media (min-width:600px) {
        .MuiToolbar-gutters {
        padding-left: 24px;
        padding-right: 24px;
        }
        }
        .MuiToolbar-regular {
        min-height: 56px;
        }
        @media (min-width:0px) and (orientation: landscape) {
        .MuiToolbar-regular {
        min-height: 48px;
        }
        }
        @media (min-width:600px) {
        .MuiToolbar-regular {
        min-height: 64px;
        }
        }
        .MuiToolbar-dense {
        min-height: 48px;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiTouchRipple">
        <![CDATA[
        .MuiTouchRipple-root {
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        display: block;
        z-index: 0;
        position: absolute;
        overflow: hidden;
        border-radius: inherit;
        pointer-events: none;
        }
        .MuiTouchRipple-ripple {
        top: 0;
        left: 0;
        width: 50px;
        height: 50px;
        opacity: 0;
        position: absolute;
        }
        .MuiTouchRipple-rippleVisible {
        opacity: 0.3;
        transform: scale(1);
        animation: mui-ripple-enter 550ms cubic-bezier(0.4, 0, 0.2, 1);
        }
        .MuiTouchRipple-ripplePulsate {
        animation-duration: 200ms;
        }
        .MuiTouchRipple-child {
        width: 100%;
        height: 100%;
        opacity: 1;
        display: block;
        border-radius: 50%;
        background-color: currentColor;
        }
        .MuiTouchRipple-childLeaving {
        opacity: 0;
        animation: mui-ripple-exit 550ms cubic-bezier(0.4, 0, 0.2, 1);
        }
        .MuiTouchRipple-childPulsate {
        top: 0;
        left: 0;
        position: absolute;
        animation: mui-ripple-pulsate 2500ms cubic-bezier(0.4, 0, 0.2, 1) 200ms infinite;
        }
        @-webkit-keyframes mui-ripple-enter {
        0% {
        opacity: 0.1;
        transform: scale(0);
        }
        100% {
        opacity: 0.3;
        transform: scale(1);
        }
        }
        @-webkit-keyframes mui-ripple-exit {
        0% {
        opacity: 1;
        }
        100% {
        opacity: 0;
        }
        }
        @-webkit-keyframes mui-ripple-pulsate {
        0% {
        transform: scale(1);
        }
        50% {
        transform: scale(0.92);
        }
        100% {
        transform: scale(1);
        }
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiButtonBase">
        <![CDATA[
        .MuiButtonBase-root {
        color: inherit;
        border: 0;
        margin: 0;
        cursor: pointer;
        display: inline-flex;
        outline: none;
        padding: 0;
        position: relative;
        align-items: center;
        user-select: none;
        border-radius: 0;
        vertical-align: middle;
        justify-content: center;
        -moz-appearance: none;
        text-decoration: none;
        background-color: transparent;
        -webkit-appearance: none;
        -webkit-tap-highlight-color: transparent;
        }
        .MuiButtonBase-root::-moz-focus-inner {
        border-style: none;
        }
        .MuiButtonBase-root.MuiButtonBase-disabled {
        cursor: default;
        pointer-events: none;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiIconButton">
        <![CDATA[
        .MuiIconButton-root {
        flex: 0 0 auto;
        color: rgba(0, 0, 0, 0.54);
        padding: 12px;
        overflow: visible;
        font-size: 1.5rem;
        text-align: center;
        transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
        border-radius: 50%;
        }
        .MuiIconButton-root:hover {
        background-color: rgba(0, 0, 0, 0.08);
        }
        .MuiIconButton-root.MuiIconButton-disabled {
        color: rgba(0, 0, 0, 0.26);
        }
        @media (hover: none) {
        .MuiIconButton-root:hover {
        background-color: transparent;
        }
        }
        .MuiIconButton-root:hover.MuiIconButton-disabled {
        background-color: transparent;
        }
        .MuiIconButton-colorInherit {
        color: inherit;
        }
        .MuiIconButton-colorPrimary {
        color: #5f9b65;
        }
        .MuiIconButton-colorPrimary:hover {
        background-color: rgba(95, 155, 101, 0.08);
        }
        @media (hover: none) {
        .MuiIconButton-colorPrimary:hover {
        background-color: transparent;
        }
        }
        .MuiIconButton-colorSecondary {
        color: #5f9b65;
        }
        .MuiIconButton-colorSecondary:hover {
        background-color: rgba(95, 155, 101, 0.08);
        }
        @media (hover: none) {
        .MuiIconButton-colorSecondary:hover {
        background-color: transparent;
        }
        }
        .MuiIconButton-label {
        width: 100%;
        display: flex;
        align-items: inherit;
        justify-content: inherit;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiSnackbar">
        <![CDATA[
        .MuiSnackbar-root {
        left: 0;
        right: 0;
        z-index: 1400;
        display: flex;
        position: fixed;
        align-items: center;
        justify-content: center;
        }
        .MuiSnackbar-anchorOriginTopCenter {
        top: 0;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginTopCenter {
        left: 50%;
        right: auto;
        transform: translateX(-50%);
        }
        }
        .MuiSnackbar-anchorOriginBottomCenter {
        bottom: 0;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginBottomCenter {
        left: 50%;
        right: auto;
        transform: translateX(-50%);
        }
        }
        .MuiSnackbar-anchorOriginTopRight {
        top: 0;
        justify-content: flex-end;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginTopRight {
        top: 24px;
        left: auto;
        right: 24px;
        }
        }
        .MuiSnackbar-anchorOriginBottomRight {
        bottom: 0;
        justify-content: flex-end;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginBottomRight {
        left: auto;
        right: 24px;
        bottom: 24px;
        }
        }
        .MuiSnackbar-anchorOriginTopLeft {
        top: 0;
        justify-content: flex-start;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginTopLeft {
        top: 24px;
        left: 24px;
        right: auto;
        }
        }
        .MuiSnackbar-anchorOriginBottomLeft {
        bottom: 0;
        justify-content: flex-start;
        }
        @media (min-width:960px) {
        .MuiSnackbar-anchorOriginBottomLeft {
        left: 24px;
        right: auto;
        bottom: 24px;
        }
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiButton">
        <![CDATA[
        .MuiButton-root {
        color: rgba(0,0,0,0.87);
        padding: 8px 16px;
        font-size: 0.875rem;
        min-width: 64px;
        box-sizing: border-box;
        min-height: 36px;
        transition: background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
        font-weight: 500;
        font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
        line-height: 1.4em;
        border-radius: 4px;
        text-transform: uppercase;
        }
        .MuiButton-root:hover {
        text-decoration: none;
        background-color: rgba(0, 0, 0, 0.08);
        }
        .MuiButton-root.MuiButton-disabled {
        color: rgba(0, 0, 0, 0.26);
        }
        @media (hover: none) {
        .MuiButton-root:hover {
        background-color: transparent;
        }
        }
        .MuiButton-root:hover.MuiButton-disabled {
        background-color: transparent;
        }
        .MuiButton-label {
        width: 100%;
        display: inherit;
        align-items: inherit;
        justify-content: inherit;
        }
        .MuiButton-textPrimary {
        color: #5f9b65;
        }
        .MuiButton-textPrimary:hover {
        background-color: rgba(95, 155, 101, 0.08);
        }
        @media (hover: none) {
        .MuiButton-textPrimary:hover {
        background-color: transparent;
        }
        }
        .MuiButton-textSecondary {
        color: #5f9b65;
        }
        .MuiButton-textSecondary:hover {
        background-color: rgba(95, 155, 101, 0.08);
        }
        @media (hover: none) {
        .MuiButton-textSecondary:hover {
        background-color: transparent;
        }
        }
        .MuiButton-outlined {
        border: 1px solid rgba(0, 0, 0, 0.23);
        }
        .MuiButton-outlinedPrimary {
        border: 1px solid rgba(95, 155, 101, 0.5);
        }
        .MuiButton-outlinedPrimary:hover {
        border: 1px solid #5f9b65;
        }
        .MuiButton-outlinedPrimary.MuiButton-disabled {
        border: 1px solid rgba(0, 0, 0, 0.26);
        }
        .MuiButton-outlinedSecondary {
        border: 1px solid rgba(95, 155, 101, 0.5);
        }
        .MuiButton-outlinedSecondary:hover {
        border: 1px solid #5f9b65;
        }
        .MuiButton-outlinedSecondary.MuiButton-disabled {
        border: 1px solid rgba(0, 0, 0, 0.26);
        }
        .MuiButton-contained {
        color: rgba(0, 0, 0, 0.87);
        box-shadow: 0px 1px 5px 0px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 3px 1px -2px rgba(0,0,0,0.12);
        background-color: #e0e0e0;
        }
        .MuiButton-contained.MuiButton-focusVisible {
        box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
        }
        .MuiButton-contained:active {
        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);
        }
        .MuiButton-contained.MuiButton-disabled {
        color: rgba(0, 0, 0, 0.26);
        box-shadow: none;
        background-color: rgba(0, 0, 0, 0.12);
        }
        .MuiButton-contained:hover {
        background-color: #d5d5d5;
        }
        @media (hover: none) {
        .MuiButton-contained:hover {
        background-color: #e0e0e0;
        }
        }
        .MuiButton-contained:hover.MuiButton-disabled {
        background-color: rgba(0, 0, 0, 0.12);
        }
        .MuiButton-containedPrimary {
        color: #fff;
        background-color: #5f9b65;
        }
        .MuiButton-containedPrimary:hover {
        background-color: #426c46;
        }
        @media (hover: none) {
        .MuiButton-containedPrimary:hover {
        background-color: #5f9b65;
        }
        }
        .MuiButton-containedSecondary {
        color: #fff;
        background-color: #5f9b65;
        }
        .MuiButton-containedSecondary:hover {
        background-color: #426c46;
        }
        @media (hover: none) {
        .MuiButton-containedSecondary:hover {
        background-color: #5f9b65;
        }
        }
        .MuiButton-fab {
        width: 56px;
        height: 56px;
        padding: 0;
        min-width: 0;
        box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
        border-radius: 50%;
        }
        .MuiButton-fab:active {
        box-shadow: 0px 7px 8px -4px rgba(0,0,0,0.2),0px 12px 17px 2px rgba(0,0,0,0.14),0px 5px 22px 4px rgba(0,0,0,0.12);
        }
        .MuiButton-extendedFab {
        width: auto;
        height: 48px;
        padding: 0 16px;
        min-width: 48px;
        border-radius: 24px;
        }
        .MuiButton-colorInherit {
        color: inherit;
        }
        .MuiButton-mini {
        width: 40px;
        height: 40px;
        }
        .MuiButton-sizeSmall {
        padding: 7px 8px;
        min-width: 64px;
        font-size: 0.8125rem;
        min-height: 32px;
        }
        .MuiButton-sizeLarge {
        padding: 8px 24px;
        min-width: 112px;
        font-size: 0.9375rem;
        min-height: 40px;
        }
        .MuiButton-fullWidth {
        width: 100%;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiModal">
        <![CDATA[
        .MuiModal-root {
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        z-index: 1300;
        position: fixed;
        }
        .MuiModal-hidden {
        visibility: hidden;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiPopover">
        <![CDATA[
        .MuiPopover-paper {
        outline: none;
        position: absolute;
        min-width: 16px;
        max-width: calc(100% - 32px);
        overflow-y: auto;
        overflow-x: hidden;
        min-height: 16px;
        max-height: calc(100% - 32px);
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiMenu">
        <![CDATA[
        .MuiMenu-paper {
        max-height: calc(100% - 96px);
        -webkit-overflow-scrolling: touch;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiDrawer">
        <![CDATA[
        .MuiDrawer-docked {
        flex: 0 0 auto;
        }
        .MuiDrawer-paper {
        top: 0;
        flex: 1 0 auto;
        height: 100%;
        display: flex;
        z-index: 1200;
        outline: none;
        position: fixed;
        overflow-y: auto;
        flex-direction: column;
        -webkit-overflow-scrolling: touch;
        }
        .MuiDrawer-paperAnchorLeft {
        left: 0;
        right: auto;
        }
        .MuiDrawer-paperAnchorRight {
        left: auto;
        right: 0;
        }
        .MuiDrawer-paperAnchorTop {
        top: 0;
        left: 0;
        right: 0;
        bottom: auto;
        height: auto;
        max-height: 100%;
        }
        .MuiDrawer-paperAnchorBottom {
        top: auto;
        left: 0;
        right: 0;
        bottom: 0;
        height: auto;
        max-height: 100%;
        }
        .MuiDrawer-paperAnchorDockedLeft {
        border-right: 1px solid rgba(0, 0, 0, 0.12);
        }
        .MuiDrawer-paperAnchorDockedTop {
        border-bottom: 1px solid rgba(0, 0, 0, 0.12);
        }
        .MuiDrawer-paperAnchorDockedRight {
        border-left: 1px solid rgba(0, 0, 0, 0.12);
        }
        .MuiDrawer-paperAnchorDockedBottom {
        border-top: 1px solid rgba(0, 0, 0, 0.12);
        }
        ]]>
        </style>
        <style data-jss="">
        <![CDATA[
        .jss85 {
        top: 0;
        left: 0;
        bottom: 0;
        z-index: 1199;
        position: fixed;
        }
        .jss86 {
        right: auto;
        }
        .jss87 {
        left: auto;
        right: 0;
        }
        .jss88 {
        right: 0;
        bottom: auto;
        }
        .jss89 {
        top: auto;
        right: 0;
        bottom: 0;
        }
        ]]>
        </style>
        <style data-jss="" data-meta="MuiTooltip">
        <![CDATA[
        .MuiTooltip-popper {
        z-index: 1500;
        opacity: 0.9;
        }
        .MuiTooltip-tooltip {
        color: #fff;
        padding: .7rem;
        z-index: 10000000;
        font-size: 1rem;
        max-width: 300px;
        font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
        line-height: 1.4em;
        border-radius: 4px;
        background-color: rgba(75,75,75,.94);
        }
        .MuiTooltip-touch {
        padding: 8px 16px;
        font-size: 0.875rem;
        line-height: 1.14286em;
        }
        .MuiTooltip-tooltipPlacementLeft {
        margin: 0 24px ;
        transform-origin: right center;
        }
        @media (min-width:600px) {
        .MuiTooltip-tooltipPlacementLeft {
        margin: 0 14px;
        }
        }
        .MuiTooltip-tooltipPlacementRight {
        margin: 0 24px;
        transform-origin: left center;
        }
        @media (min-width:600px) {
        .MuiTooltip-tooltipPlacementRight {
        margin: 0 14px;
        }
        }
        .MuiTooltip-tooltipPlacementTop {
        margin: 24px 0;
        transform-origin: center bottom;
        }
        @media (min-width:600px) {
        .MuiTooltip-tooltipPlacementTop {
        margin: 14px 0;
        }
        }
        .MuiTooltip-tooltipPlacementBottom {
        margin: 24px 0;
        transform-origin: center top;
        }
        @media (min-width:600px) {
        .MuiTooltip-tooltipPlacementBottom {
        margin: 14px 0;
        }
        }
        ]]>
        </style>
        <link id="main-styles" rel="stylesheet" type="text/css" onerror="window.missingMainStylesheet=true" href="/allStyles?hash=623084ac09473955d791117732572249970c772b9f5a5512db642ad515f40e9d&amp;theme=%7B%22name%22%3A%22default%22%7D" />
        <script async="async" src="https://widget.intercom.io/widget/wtb8z7sj"></script>
        <style>
        <![CDATA[
        .ck.ck-editor__editable span[data-ck-unsafe-element]{display:none}.ck-hidden{display:none!important}.ck.ck-reset,.ck.ck-reset_all,.ck.ck-reset_all *{box-sizing:border-box;width:auto;height:auto;position:static}:root{--ck-z-default:1;--ck-z-modal:calc(var(--ck-z-default) + 999)}.ck-transitions-disabled,.ck-transitions-disabled *{transition:none!important}:root{--ck-color-base-foreground:#fafafa;--ck-color-base-background:#fff;--ck-color-base-border:#c4c4c4;--ck-color-base-action:#61b045;--ck-color-base-focus:#6cb5f9;--ck-color-base-text:#333;--ck-color-base-active:#198cf0;--ck-color-base-active-focus:#0e7fe1;--ck-color-base-error:#db3700;--ck-color-focus-border-coordinates:208,79%,51%;--ck-color-focus-border:hsl(var(--ck-color-focus-border-coordinates));--ck-color-focus-outer-shadow:#bcdefb;--ck-color-focus-disabled-shadow:rgba(119,186,248,0.3);--ck-color-focus-error-shadow:rgba(255,64,31,0.3);--ck-color-text:var(--ck-color-base-text);--ck-color-shadow-drop:rgba(0,0,0,0.15);--ck-color-shadow-drop-active:rgba(0,0,0,0.2);--ck-color-shadow-inner:rgba(0,0,0,0.1);--ck-color-button-default-background:transparent;--ck-color-button-default-hover-background:#e6e6e6;--ck-color-button-default-active-background:#d9d9d9;--ck-color-button-default-active-shadow:#bfbfbf;--ck-color-button-default-disabled-background:transparent;--ck-color-button-on-background:#dedede;--ck-color-button-on-hover-background:#c4c4c4;--ck-color-button-on-active-background:#bababa;--ck-color-button-on-active-shadow:#a1a1a1;--ck-color-button-on-disabled-background:#dedede;--ck-color-button-action-background:var(--ck-color-base-action);--ck-color-button-action-hover-background:#579e3d;--ck-color-button-action-active-background:#53973b;--ck-color-button-action-active-shadow:#498433;--ck-color-button-action-disabled-background:#7ec365;--ck-color-button-action-text:var(--ck-color-base-background);--ck-color-button-save:#008a00;--ck-color-button-cancel:#db3700;--ck-color-switch-button-off-background:#b0b0b0;--ck-color-switch-button-off-hover-background:#a3a3a3;--ck-color-switch-button-on-background:var(--ck-color-button-action-background);--ck-color-switch-button-on-hover-background:#579e3d;--ck-color-switch-button-inner-background:var(--ck-color-base-background);--ck-color-switch-button-inner-shadow:rgba(0,0,0,0.1);--ck-color-dropdown-panel-background:var(--ck-color-base-background);--ck-color-dropdown-panel-border:var(--ck-color-base-border);--ck-color-input-background:var(--ck-color-base-background);--ck-color-input-border:#c7c7c7;--ck-color-input-error-border:var(--ck-color-base-error);--ck-color-input-text:var(--ck-color-base-text);--ck-color-input-disabled-background:#f2f2f2;--ck-color-input-disabled-border:#c7c7c7;--ck-color-input-disabled-text:#757575;--ck-color-list-background:var(--ck-color-base-background);--ck-color-list-button-hover-background:var(--ck-color-button-default-hover-background);--ck-color-list-button-on-background:var(--ck-color-base-active);--ck-color-list-button-on-background-focus:var(--ck-color-base-active-focus);--ck-color-list-button-on-text:var(--ck-color-base-background);--ck-color-panel-background:var(--ck-color-base-background);--ck-color-panel-border:var(--ck-color-base-border);--ck-color-toolbar-background:var(--ck-color-base-foreground);--ck-color-toolbar-border:var(--ck-color-base-border);--ck-color-tooltip-background:var(--ck-color-base-text);--ck-color-tooltip-text:var(--ck-color-base-background);--ck-color-engine-placeholder-text:#707070;--ck-color-upload-bar-background:#6cb5f9;--ck-color-link-default:#0000f0;--ck-color-link-selected-background:rgba(31,177,255,0.1);--ck-color-link-fake-selection:rgba(31,177,255,0.3);--ck-disabled-opacity:.5;--ck-focus-outer-shadow-geometry:0 0 0 3px;--ck-focus-outer-shadow:var(--ck-focus-outer-shadow-geometry) var(--ck-color-focus-outer-shadow);--ck-focus-disabled-outer-shadow:var(--ck-focus-outer-shadow-geometry) var(--ck-color-focus-disabled-shadow);--ck-focus-error-outer-shadow:var(--ck-focus-outer-shadow-geometry) var(--ck-color-focus-error-shadow);--ck-focus-ring:1px solid var(--ck-color-focus-border);--ck-font-size-base:13px;--ck-line-height-base:1.84615;--ck-font-face:Helvetica,Arial,Tahoma,Verdana,Sans-Serif;--ck-font-size-tiny:0.7em;--ck-font-size-small:0.75em;--ck-font-size-normal:1em;--ck-font-size-big:1.4em;--ck-font-size-large:1.8em;--ck-ui-component-min-height:2.3em}.ck.ck-reset,.ck.ck-reset_all,.ck.ck-reset_all *{margin:0;padding:0;border:0;background:transparent;text-decoration:none;vertical-align:middle;transition:none;word-wrap:break-word}.ck.ck-reset_all,.ck.ck-reset_all *{border-collapse:collapse;font:normal normal normal var(--ck-font-size-base)/var(--ck-line-height-base) var(--ck-font-face);color:var(--ck-color-text);text-align:left;white-space:nowrap;cursor:auto;float:none}.ck.ck-reset_all .ck-rtl *{text-align:right}.ck.ck-reset_all iframe{vertical-align:inherit}.ck.ck-reset_all textarea{white-space:pre-wrap}.ck.ck-reset_all input[type=password],.ck.ck-reset_all input[type=text],.ck.ck-reset_all textarea{cursor:text}.ck.ck-reset_all input[type=password][disabled],.ck.ck-reset_all input[type=text][disabled],.ck.ck-reset_all textarea[disabled]{cursor:default}.ck.ck-reset_all fieldset{padding:10px;border:2px groove #dfdee3}.ck.ck-reset_all button::-moz-focus-inner{padding:0;border:0}.ck[dir=rtl],.ck[dir=rtl] .ck{text-align:right}:root{--ck-border-radius:2px;--ck-inner-shadow:2px 2px 3px var(--ck-color-shadow-inner) inset;--ck-drop-shadow:0 1px 2px 1px var(--ck-color-shadow-drop);--ck-drop-shadow-active:0 3px 6px 1px var(--ck-color-shadow-drop-active);--ck-spacing-unit:0.6em;--ck-spacing-large:calc(var(--ck-spacing-unit)*1.5);--ck-spacing-standard:var(--ck-spacing-unit);--ck-spacing-medium:calc(var(--ck-spacing-unit)*0.8);--ck-spacing-small:calc(var(--ck-spacing-unit)*0.5);--ck-spacing-tiny:calc(var(--ck-spacing-unit)*0.3);--ck-spacing-extra-tiny:calc(var(--ck-spacing-unit)*0.16)}.ck.ck-icon{vertical-align:middle}:root{--ck-icon-size:calc(var(--ck-line-height-base)*var(--ck-font-size-normal))}.ck.ck-icon{width:var(--ck-icon-size);height:var(--ck-icon-size);font-size:.8333350694em;will-change:transform}.ck.ck-icon,.ck.ck-icon *{color:inherit;cursor:inherit}.ck.ck-icon :not([fill]){fill:currentColor}.ck.ck-tooltip,.ck.ck-tooltip .ck-tooltip__text:after{position:absolute;pointer-events:none;-webkit-backface-visibility:hidden}.ck.ck-tooltip{visibility:hidden;opacity:0;display:none;z-index:var(--ck-z-modal)}.ck.ck-tooltip .ck-tooltip__text{display:inline-block}.ck.ck-tooltip .ck-tooltip__text:after{content:"";width:0;height:0}:root{--ck-tooltip-arrow-size:5px}.ck.ck-tooltip{left:50%;top:0;transition:opacity .2s ease-in-out .2s}.ck.ck-tooltip .ck-tooltip__text{border-radius:0}.ck-rounded-corners .ck.ck-tooltip .ck-tooltip__text,.ck.ck-tooltip .ck-tooltip__text.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-tooltip .ck-tooltip__text{font-size:.9em;line-height:1.5;color:var(--ck-color-tooltip-text);padding:var(--ck-spacing-small) var(--ck-spacing-medium);background:var(--ck-color-tooltip-background);position:relative;left:-50%}.ck.ck-tooltip .ck-tooltip__text:after{transition:opacity .2s ease-in-out .2s;border-style:solid;left:50%}.ck.ck-tooltip.ck-tooltip_s,.ck.ck-tooltip.ck-tooltip_se,.ck.ck-tooltip.ck-tooltip_sw{bottom:calc(var(--ck-tooltip-arrow-size)*-1);transform:translateY(100%)}.ck.ck-tooltip.ck-tooltip_s .ck-tooltip__text:after,.ck.ck-tooltip.ck-tooltip_se .ck-tooltip__text:after,.ck.ck-tooltip.ck-tooltip_sw .ck-tooltip__text:after{top:calc(var(--ck-tooltip-arrow-size)*-1 + 1px);transform:translateX(-50%);border-left-color:transparent;border-bottom-color:var(--ck-color-tooltip-background);border-right-color:transparent;border-top-color:transparent;border-left-width:var(--ck-tooltip-arrow-size);border-bottom-width:var(--ck-tooltip-arrow-size);border-right-width:var(--ck-tooltip-arrow-size);border-top-width:0}.ck.ck-tooltip.ck-tooltip_sw{right:50%;left:auto}.ck.ck-tooltip.ck-tooltip_sw .ck-tooltip__text{left:auto;right:calc(var(--ck-tooltip-arrow-size)*-2)}.ck.ck-tooltip.ck-tooltip_sw .ck-tooltip__text:after{left:auto;right:0}.ck.ck-tooltip.ck-tooltip_se{left:50%;right:auto}.ck.ck-tooltip.ck-tooltip_se .ck-tooltip__text{right:auto;left:calc(var(--ck-tooltip-arrow-size)*-2)}.ck.ck-tooltip.ck-tooltip_se .ck-tooltip__text:after{right:auto;left:0;transform:translateX(50%)}.ck.ck-tooltip.ck-tooltip_n{top:calc(var(--ck-tooltip-arrow-size)*-1);transform:translateY(-100%)}.ck.ck-tooltip.ck-tooltip_n .ck-tooltip__text:after{bottom:calc(var(--ck-tooltip-arrow-size)*-1);transform:translateX(-50%);border-left-color:transparent;border-bottom-color:transparent;border-right-color:transparent;border-top-color:var(--ck-color-tooltip-background);border-left-width:var(--ck-tooltip-arrow-size);border-bottom-width:0;border-right-width:var(--ck-tooltip-arrow-size);border-top-width:var(--ck-tooltip-arrow-size)}.ck.ck-tooltip.ck-tooltip_e{left:calc(100% + var(--ck-tooltip-arrow-size));top:50%}.ck.ck-tooltip.ck-tooltip_e .ck-tooltip__text{left:0;transform:translateY(-50%)}.ck.ck-tooltip.ck-tooltip_e .ck-tooltip__text:after{left:calc(var(--ck-tooltip-arrow-size)*-1);top:calc(50% - var(--ck-tooltip-arrow-size)*1);border-left-color:transparent;border-bottom-color:transparent;border-right-color:var(--ck-color-tooltip-background);border-top-color:transparent;border-left-width:0;border-bottom-width:var(--ck-tooltip-arrow-size);border-right-width:var(--ck-tooltip-arrow-size);border-top-width:var(--ck-tooltip-arrow-size)}.ck.ck-tooltip.ck-tooltip_w{right:calc(100% + var(--ck-tooltip-arrow-size));left:auto;top:50%}.ck.ck-tooltip.ck-tooltip_w .ck-tooltip__text{left:0;transform:translateY(-50%)}.ck.ck-tooltip.ck-tooltip_w .ck-tooltip__text:after{left:100%;top:calc(50% - var(--ck-tooltip-arrow-size)*1);border-left-color:var(--ck-color-tooltip-background);border-bottom-color:transparent;border-right-color:transparent;border-top-color:transparent;border-left-width:var(--ck-tooltip-arrow-size);border-bottom-width:var(--ck-tooltip-arrow-size);border-right-width:0;border-top-width:var(--ck-tooltip-arrow-size)}.ck.ck-button,a.ck.ck-button{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.ck.ck-button .ck-tooltip,a.ck.ck-button .ck-tooltip{display:block}@media (hover:none){.ck.ck-button .ck-tooltip,a.ck.ck-button .ck-tooltip{display:none}}.ck.ck-button,a.ck.ck-button{position:relative;display:inline-flex;align-items:center;justify-content:left}.ck.ck-button .ck-button__label,a.ck.ck-button .ck-button__label{display:none}.ck.ck-button.ck-button_with-text .ck-button__label,a.ck.ck-button.ck-button_with-text .ck-button__label{display:inline-block}.ck.ck-button:not(.ck-button_with-text),a.ck.ck-button:not(.ck-button_with-text){justify-content:center}.ck.ck-button:hover .ck-tooltip,a.ck.ck-button:hover .ck-tooltip{visibility:visible;opacity:1}.ck.ck-button:focus:not(:hover) .ck-tooltip,a.ck.ck-button:focus:not(:hover) .ck-tooltip{display:none}.ck.ck-button,a.ck.ck-button{background:var(--ck-color-button-default-background)}.ck.ck-button:not(.ck-disabled):hover,a.ck.ck-button:not(.ck-disabled):hover{background:var(--ck-color-button-default-hover-background)}.ck.ck-button:not(.ck-disabled):active,a.ck.ck-button:not(.ck-disabled):active{background:var(--ck-color-button-default-active-background);box-shadow:inset 0 2px 2px var(--ck-color-button-default-active-shadow)}.ck.ck-button.ck-disabled,a.ck.ck-button.ck-disabled{background:var(--ck-color-button-default-disabled-background)}.ck.ck-button,a.ck.ck-button{border-radius:0}.ck-rounded-corners .ck.ck-button,.ck-rounded-corners a.ck.ck-button,.ck.ck-button.ck-rounded-corners,a.ck.ck-button.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-button,a.ck.ck-button{white-space:nowrap;cursor:default;vertical-align:middle;padding:var(--ck-spacing-tiny);text-align:center;min-width:var(--ck-ui-component-min-height);min-height:var(--ck-ui-component-min-height);line-height:1;font-size:inherit;border:1px solid transparent;transition:box-shadow .2s ease-in-out,border .2s ease-in-out;-webkit-appearance:none}.ck.ck-button:active,.ck.ck-button:focus,a.ck.ck-button:active,a.ck.ck-button:focus{outline:none;border:var(--ck-focus-ring);box-shadow:var(--ck-focus-outer-shadow),0 0}.ck.ck-button .ck-button__icon use,.ck.ck-button .ck-button__icon use *,a.ck.ck-button .ck-button__icon use,a.ck.ck-button .ck-button__icon use *{color:inherit}.ck.ck-button .ck-button__label,a.ck.ck-button .ck-button__label{font-size:inherit;font-weight:inherit;color:inherit;cursor:inherit;vertical-align:middle}[dir=ltr] .ck.ck-button .ck-button__label,[dir=ltr] a.ck.ck-button .ck-button__label{text-align:left}[dir=rtl] .ck.ck-button .ck-button__label,[dir=rtl] a.ck.ck-button .ck-button__label{text-align:right}.ck.ck-button .ck-button__keystroke,a.ck.ck-button .ck-button__keystroke{color:inherit}[dir=ltr] .ck.ck-button .ck-button__keystroke,[dir=ltr] a.ck.ck-button .ck-button__keystroke{margin-left:var(--ck-spacing-large)}[dir=rtl] .ck.ck-button .ck-button__keystroke,[dir=rtl] a.ck.ck-button .ck-button__keystroke{margin-right:var(--ck-spacing-large)}.ck.ck-button .ck-button__keystroke,a.ck.ck-button .ck-button__keystroke{font-weight:700;opacity:.7}.ck.ck-button.ck-disabled:active,.ck.ck-button.ck-disabled:focus,a.ck.ck-button.ck-disabled:active,a.ck.ck-button.ck-disabled:focus{box-shadow:var(--ck-focus-disabled-outer-shadow),0 0}.ck.ck-button.ck-disabled .ck-button__icon,a.ck.ck-button.ck-disabled .ck-button__icon{opacity:var(--ck-disabled-opacity)}.ck.ck-button.ck-disabled .ck-button__label,a.ck.ck-button.ck-disabled .ck-button__label{opacity:var(--ck-disabled-opacity)}.ck.ck-button.ck-disabled .ck-button__keystroke,a.ck.ck-button.ck-disabled .ck-button__keystroke{opacity:.3}.ck.ck-button.ck-button_with-text,a.ck.ck-button.ck-button_with-text{padding:var(--ck-spacing-tiny) var(--ck-spacing-standard)}[dir=ltr] .ck.ck-button.ck-button_with-text .ck-button__icon,[dir=ltr] a.ck.ck-button.ck-button_with-text .ck-button__icon{margin-left:calc(var(--ck-spacing-small)*-1);margin-right:var(--ck-spacing-small)}[dir=rtl] .ck.ck-button.ck-button_with-text .ck-button__icon,[dir=rtl] a.ck.ck-button.ck-button_with-text .ck-button__icon{margin-right:calc(var(--ck-spacing-small)*-1);margin-left:var(--ck-spacing-small)}.ck.ck-button.ck-button_with-keystroke .ck-button__label,a.ck.ck-button.ck-button_with-keystroke .ck-button__label{flex-grow:1}.ck.ck-button.ck-on,a.ck.ck-button.ck-on{background:var(--ck-color-button-on-background)}.ck.ck-button.ck-on:not(.ck-disabled):hover,a.ck.ck-button.ck-on:not(.ck-disabled):hover{background:var(--ck-color-button-on-hover-background)}.ck.ck-button.ck-on:not(.ck-disabled):active,a.ck.ck-button.ck-on:not(.ck-disabled):active{background:var(--ck-color-button-on-active-background);box-shadow:inset 0 2px 2px var(--ck-color-button-on-active-shadow)}.ck.ck-button.ck-on.ck-disabled,a.ck.ck-button.ck-on.ck-disabled{background:var(--ck-color-button-on-disabled-background)}.ck.ck-button.ck-button-save,a.ck.ck-button.ck-button-save{color:var(--ck-color-button-save)}.ck.ck-button.ck-button-cancel,a.ck.ck-button.ck-button-cancel{color:var(--ck-color-button-cancel)}.ck.ck-button-action,a.ck.ck-button-action{background:var(--ck-color-button-action-background)}.ck.ck-button-action:not(.ck-disabled):hover,a.ck.ck-button-action:not(.ck-disabled):hover{background:var(--ck-color-button-action-hover-background)}.ck.ck-button-action:not(.ck-disabled):active,a.ck.ck-button-action:not(.ck-disabled):active{background:var(--ck-color-button-action-active-background);box-shadow:inset 0 2px 2px var(--ck-color-button-action-active-shadow)}.ck.ck-button-action.ck-disabled,a.ck.ck-button-action.ck-disabled{background:var(--ck-color-button-action-disabled-background)}.ck.ck-button-action,a.ck.ck-button-action{color:var(--ck-color-button-action-text)}.ck.ck-button-bold,a.ck.ck-button-bold{font-weight:700}.ck.ck-button.ck-switchbutton .ck-button__toggle,.ck.ck-button.ck-switchbutton .ck-button__toggle .ck-button__toggle__inner{display:block}:root{--ck-switch-button-toggle-width:2.6153846154em;--ck-switch-button-toggle-inner-size:1.0769230769em;--ck-switch-button-toggle-spacing:1px;--ck-switch-button-translation:calc(var(--ck-switch-button-toggle-width) - var(--ck-switch-button-toggle-inner-size) - var(--ck-switch-button-toggle-spacing)*2)}[dir=ltr] .ck.ck-button.ck-switchbutton .ck-button__label{margin-right:calc(var(--ck-spacing-large)*2)}[dir=rtl] .ck.ck-button.ck-switchbutton .ck-button__label{margin-left:calc(var(--ck-spacing-large)*2)}.ck.ck-button.ck-switchbutton .ck-button__toggle{border-radius:0}.ck-rounded-corners .ck.ck-button.ck-switchbutton .ck-button__toggle,.ck.ck-button.ck-switchbutton .ck-button__toggle.ck-rounded-corners{border-radius:var(--ck-border-radius)}[dir=ltr] .ck.ck-button.ck-switchbutton .ck-button__toggle{margin-left:auto}[dir=rtl] .ck.ck-button.ck-switchbutton .ck-button__toggle{margin-right:auto}.ck.ck-button.ck-switchbutton .ck-button__toggle{transition:background .4s ease;width:var(--ck-switch-button-toggle-width);background:var(--ck-color-switch-button-off-background)}.ck.ck-button.ck-switchbutton .ck-button__toggle .ck-button__toggle__inner{border-radius:0}.ck-rounded-corners .ck.ck-button.ck-switchbutton .ck-button__toggle .ck-button__toggle__inner,.ck.ck-button.ck-switchbutton .ck-button__toggle .ck-button__toggle__inner.ck-rounded-corners{border-radius:var(--ck-border-radius);border-radius:calc(var(--ck-border-radius)*0.5)}.ck.ck-button.ck-switchbutton .ck-button__toggle .ck-button__toggle__inner{margin:var(--ck-switch-button-toggle-spacing);width:var(--ck-switch-button-toggle-inner-size);height:var(--ck-switch-button-toggle-inner-size);background:var(--ck-color-switch-button-inner-background);transition:all .3s ease}.ck.ck-button.ck-switchbutton .ck-button__toggle:hover{background:var(--ck-color-switch-button-off-hover-background)}.ck.ck-button.ck-switchbutton .ck-button__toggle:hover .ck-button__toggle__inner{box-shadow:0 0 0 5px var(--ck-color-switch-button-inner-shadow)}.ck.ck-button.ck-switchbutton.ck-disabled .ck-button__toggle{opacity:var(--ck-disabled-opacity)}.ck.ck-button.ck-switchbutton.ck-on .ck-button__toggle{background:var(--ck-color-switch-button-on-background)}.ck.ck-button.ck-switchbutton.ck-on .ck-button__toggle:hover{background:var(--ck-color-switch-button-on-hover-background)}[dir=ltr] .ck.ck-button.ck-switchbutton.ck-on .ck-button__toggle .ck-button__toggle__inner{transform:translateX(var(--ck-switch-button-translation))}[dir=rtl] .ck.ck-button.ck-switchbutton.ck-on .ck-button__toggle .ck-button__toggle__inner{transform:translateX(calc(var(--ck-switch-button-translation)*-1))}.ck.ck-color-grid{display:grid}:root{--ck-color-grid-tile-size:24px;--ck-color-color-grid-check-icon:#000}.ck.ck-color-grid{grid-gap:5px;padding:8px}.ck.ck-color-grid__tile{width:var(--ck-color-grid-tile-size);height:var(--ck-color-grid-tile-size);min-width:var(--ck-color-grid-tile-size);min-height:var(--ck-color-grid-tile-size);padding:0;transition:box-shadow .2s ease;border:0}.ck.ck-color-grid__tile.ck-disabled{cursor:unset;transition:unset}.ck.ck-color-grid__tile.ck-color-table__color-tile_bordered{box-shadow:0 0 0 1px var(--ck-color-base-border)}.ck.ck-color-grid__tile .ck.ck-icon{display:none;color:var(--ck-color-color-grid-check-icon)}.ck.ck-color-grid__tile.ck-on{box-shadow:inset 0 0 0 1px var(--ck-color-base-background),0 0 0 2px var(--ck-color-base-text)}.ck.ck-color-grid__tile.ck-on .ck.ck-icon{display:block}.ck.ck-color-grid__tile.ck-on,.ck.ck-color-grid__tile:focus:not(.ck-disabled),.ck.ck-color-grid__tile:hover:not(.ck-disabled){border:0}.ck.ck-color-grid__tile:focus:not(.ck-disabled),.ck.ck-color-grid__tile:hover:not(.ck-disabled){box-shadow:inset 0 0 0 1px var(--ck-color-base-background),0 0 0 2px var(--ck-color-focus-border)}.ck.ck-color-grid__label{padding:0 var(--ck-spacing-standard)}.ck.ck-splitbutton{font-size:inherit}.ck.ck-splitbutton .ck-splitbutton__action:focus{z-index:calc(var(--ck-z-default) + 1)}.ck.ck-splitbutton.ck-splitbutton_open>.ck-button .ck-tooltip{display:none}:root{--ck-color-split-button-hover-background:#ebebeb;--ck-color-split-button-hover-border:#b3b3b3}[dir=ltr] .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__action,[dir=ltr] .ck.ck-splitbutton:hover>.ck-splitbutton__action{border-top-right-radius:unset;border-bottom-right-radius:unset}[dir=rtl] .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__action,[dir=rtl] .ck.ck-splitbutton:hover>.ck-splitbutton__action{border-top-left-radius:unset;border-bottom-left-radius:unset}.ck.ck-splitbutton>.ck-splitbutton__arrow{min-width:unset}[dir=ltr] .ck.ck-splitbutton>.ck-splitbutton__arrow{border-top-left-radius:unset;border-bottom-left-radius:unset}[dir=rtl] .ck.ck-splitbutton>.ck-splitbutton__arrow{border-top-right-radius:unset;border-bottom-right-radius:unset}.ck.ck-splitbutton>.ck-splitbutton__arrow svg{width:var(--ck-dropdown-arrow-size)}.ck.ck-splitbutton.ck-splitbutton_open>.ck-button:not(.ck-on):not(.ck-disabled):not(:hover),.ck.ck-splitbutton:hover>.ck-button:not(.ck-on):not(.ck-disabled):not(:hover){background:var(--ck-color-split-button-hover-background)}.ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):after,.ck.ck-splitbutton:hover>.ck-splitbutton__arrow:not(.ck-disabled):after{content:"";position:absolute;width:1px;height:100%;background-color:var(--ck-color-split-button-hover-border)}[dir=ltr] .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):after,[dir=ltr] .ck.ck-splitbutton:hover>.ck-splitbutton__arrow:not(.ck-disabled):after{left:-1px}[dir=rtl] .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):after,[dir=rtl] .ck.ck-splitbutton:hover>.ck-splitbutton__arrow:not(.ck-disabled):after{right:-1px}.ck.ck-splitbutton.ck-splitbutton_open{border-radius:0}.ck-rounded-corners .ck.ck-splitbutton.ck-splitbutton_open,.ck.ck-splitbutton.ck-splitbutton_open.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck-rounded-corners .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__action,.ck.ck-splitbutton.ck-splitbutton_open.ck-rounded-corners>.ck-splitbutton__action{border-bottom-left-radius:0}.ck-rounded-corners .ck.ck-splitbutton.ck-splitbutton_open>.ck-splitbutton__arrow,.ck.ck-splitbutton.ck-splitbutton_open.ck-rounded-corners>.ck-splitbutton__arrow{border-bottom-right-radius:0}:root{--ck-dropdown-max-width:75vw}.ck.ck-dropdown{display:inline-block;position:relative}.ck.ck-dropdown .ck-dropdown__arrow{pointer-events:none;z-index:var(--ck-z-default)}.ck.ck-dropdown .ck-button.ck-dropdown__button{width:100%}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-on .ck-tooltip{display:none}.ck.ck-dropdown .ck-dropdown__panel{-webkit-backface-visibility:hidden;display:none;z-index:var(--ck-z-modal);max-width:var(--ck-dropdown-max-width);position:absolute}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel-visible{display:inline-block}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_n,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_ne,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nme,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nmw,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nw{bottom:100%}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_s,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_se,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_sme,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_smw,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_sw{top:100%;bottom:auto}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_ne,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_se{left:0}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nw,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_sw{right:0}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_n,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_s{left:50%;transform:translateX(-50%)}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nmw,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_smw{left:75%;transform:translateX(-75%)}.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_nme,.ck.ck-dropdown .ck-dropdown__panel.ck-dropdown__panel_sme{left:25%;transform:translateX(-25%)}.ck.ck-toolbar .ck-dropdown__panel{z-index:calc(var(--ck-z-modal) + 1)}:root{--ck-dropdown-arrow-size:calc(var(--ck-icon-size)*0.5)}.ck.ck-dropdown{font-size:inherit}.ck.ck-dropdown .ck-dropdown__arrow{width:var(--ck-dropdown-arrow-size)}[dir=ltr] .ck.ck-dropdown .ck-dropdown__arrow{right:var(--ck-spacing-standard);margin-left:var(--ck-spacing-standard)}[dir=rtl] .ck.ck-dropdown .ck-dropdown__arrow{left:var(--ck-spacing-standard);margin-right:var(--ck-spacing-small)}.ck.ck-dropdown.ck-disabled .ck-dropdown__arrow{opacity:var(--ck-disabled-opacity)}[dir=ltr] .ck.ck-dropdown .ck-button.ck-dropdown__button:not(.ck-button_with-text){padding-left:var(--ck-spacing-small)}[dir=rtl] .ck.ck-dropdown .ck-button.ck-dropdown__button:not(.ck-button_with-text){padding-right:var(--ck-spacing-small)}.ck.ck-dropdown .ck-button.ck-dropdown__button .ck-button__label{width:7em;overflow:hidden;text-overflow:ellipsis}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-disabled .ck-button__label{opacity:var(--ck-disabled-opacity)}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-on{border-bottom-left-radius:0;border-bottom-right-radius:0}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-dropdown__button_label-width_auto .ck-button__label{width:auto}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-off:active,.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-on:active{box-shadow:none}.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-off:active:focus,.ck.ck-dropdown .ck-button.ck-dropdown__button.ck-on:active:focus{box-shadow:var(--ck-focus-outer-shadow),0 0}.ck.ck-dropdown__panel{border-radius:0}.ck-rounded-corners .ck.ck-dropdown__panel,.ck.ck-dropdown__panel.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-dropdown__panel{box-shadow:var(--ck-drop-shadow),0 0;background:var(--ck-color-dropdown-panel-background);border:1px solid var(--ck-color-dropdown-panel-border);bottom:0;min-width:100%}.ck.ck-dropdown__panel.ck-dropdown__panel_se{border-top-left-radius:0}.ck.ck-dropdown__panel.ck-dropdown__panel_sw{border-top-right-radius:0}.ck.ck-dropdown__panel.ck-dropdown__panel_ne{border-bottom-left-radius:0}.ck.ck-dropdown__panel.ck-dropdown__panel_nw{border-bottom-right-radius:0}.ck.ck-toolbar{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none;display:flex;flex-flow:row nowrap;align-items:center}.ck.ck-toolbar>.ck-toolbar__items{display:flex;flex-flow:row wrap;align-items:center;flex-grow:1}.ck.ck-toolbar .ck.ck-toolbar__separator{display:inline-block}.ck.ck-toolbar .ck.ck-toolbar__separator:first-child,.ck.ck-toolbar .ck.ck-toolbar__separator:last-child{display:none}.ck.ck-toolbar .ck-toolbar__line-break{flex-basis:100%}.ck.ck-toolbar.ck-toolbar_grouping>.ck-toolbar__items{flex-wrap:nowrap}.ck.ck-toolbar.ck-toolbar_vertical>.ck-toolbar__items{flex-direction:column}.ck.ck-toolbar.ck-toolbar_floating>.ck-toolbar__items{flex-wrap:nowrap}.ck.ck-toolbar>.ck.ck-toolbar__grouped-dropdown>.ck-dropdown__button .ck-dropdown__arrow{display:none}.ck.ck-toolbar{border-radius:0}.ck-rounded-corners .ck.ck-toolbar,.ck.ck-toolbar.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-toolbar{background:var(--ck-color-toolbar-background);padding:0 var(--ck-spacing-small);border:1px solid var(--ck-color-toolbar-border)}.ck.ck-toolbar .ck.ck-toolbar__separator{align-self:stretch;width:1px;min-width:1px;background:var(--ck-color-toolbar-border);margin-top:var(--ck-spacing-small);margin-bottom:var(--ck-spacing-small)}.ck.ck-toolbar .ck-toolbar__line-break{height:0}.ck.ck-toolbar>.ck-toolbar__items>:not(.ck-toolbar__line-break){margin-right:var(--ck-spacing-small)}.ck.ck-toolbar>.ck-toolbar__items:empty+.ck.ck-toolbar__separator{display:none}.ck.ck-toolbar>.ck-toolbar__items>:not(.ck-toolbar__line-break),.ck.ck-toolbar>.ck.ck-toolbar__grouped-dropdown{margin-top:var(--ck-spacing-small);margin-bottom:var(--ck-spacing-small)}.ck.ck-toolbar.ck-toolbar_vertical{padding:0}.ck.ck-toolbar.ck-toolbar_vertical>.ck-toolbar__items>.ck{width:100%;margin:0;border-radius:0;border:0}.ck.ck-toolbar.ck-toolbar_compact{padding:0}.ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>*{margin:0}.ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>:not(:first-child):not(:last-child){border-radius:0}.ck.ck-toolbar>.ck.ck-toolbar__grouped-dropdown>.ck.ck-button.ck-dropdown__button{padding-left:var(--ck-spacing-tiny)}.ck-toolbar-container .ck.ck-toolbar{border:0}.ck.ck-toolbar[dir=rtl]>.ck-toolbar__items>.ck,[dir=rtl] .ck.ck-toolbar>.ck-toolbar__items>.ck{margin-right:0}.ck.ck-toolbar[dir=rtl]:not(.ck-toolbar_compact)>.ck-toolbar__items>.ck,[dir=rtl] .ck.ck-toolbar:not(.ck-toolbar_compact)>.ck-toolbar__items>.ck{margin-left:var(--ck-spacing-small)}.ck.ck-toolbar[dir=rtl]>.ck-toolbar__items>.ck:last-child,[dir=rtl] .ck.ck-toolbar>.ck-toolbar__items>.ck:last-child{margin-left:0}.ck.ck-toolbar[dir=rtl].ck-toolbar_compact>.ck-toolbar__items>.ck:first-child,[dir=rtl] .ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>.ck:first-child{border-top-left-radius:0;border-bottom-left-radius:0}.ck.ck-toolbar[dir=rtl].ck-toolbar_compact>.ck-toolbar__items>.ck:last-child,[dir=rtl] .ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>.ck:last-child{border-top-right-radius:0;border-bottom-right-radius:0}.ck.ck-toolbar[dir=rtl]>.ck.ck-toolbar__separator,[dir=rtl] .ck.ck-toolbar>.ck.ck-toolbar__separator{margin-left:var(--ck-spacing-small)}.ck.ck-toolbar[dir=rtl].ck-toolbar_grouping>.ck-toolbar__items:not(:empty):not(:only-child),[dir=rtl] .ck.ck-toolbar.ck-toolbar_grouping>.ck-toolbar__items:not(:empty):not(:only-child){margin-left:var(--ck-spacing-small)}.ck.ck-toolbar[dir=ltr]>.ck-toolbar__items>.ck:last-child,[dir=ltr] .ck.ck-toolbar>.ck-toolbar__items>.ck:last-child{margin-right:0}.ck.ck-toolbar[dir=ltr].ck-toolbar_compact>.ck-toolbar__items>.ck:first-child,[dir=ltr] .ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>.ck:first-child{border-top-right-radius:0;border-bottom-right-radius:0}.ck.ck-toolbar[dir=ltr].ck-toolbar_compact>.ck-toolbar__items>.ck:last-child,[dir=ltr] .ck.ck-toolbar.ck-toolbar_compact>.ck-toolbar__items>.ck:last-child{border-top-left-radius:0;border-bottom-left-radius:0}.ck.ck-toolbar[dir=ltr]>.ck.ck-toolbar__separator,[dir=ltr] .ck.ck-toolbar>.ck.ck-toolbar__separator{margin-right:var(--ck-spacing-small)}.ck.ck-toolbar[dir=ltr].ck-toolbar_grouping>.ck-toolbar__items:not(:empty):not(:only-child),[dir=ltr] .ck.ck-toolbar.ck-toolbar_grouping>.ck-toolbar__items:not(:empty):not(:only-child){margin-right:var(--ck-spacing-small)}.ck.ck-list{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none;display:flex;flex-direction:column}.ck.ck-list .ck-list__item,.ck.ck-list .ck-list__separator{display:block}.ck.ck-list .ck-list__item>:focus{position:relative;z-index:var(--ck-z-default)}.ck.ck-list{border-radius:0}.ck-rounded-corners .ck.ck-list,.ck.ck-list.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-list{list-style-type:none;background:var(--ck-color-list-background)}.ck.ck-list__item{cursor:default;min-width:12em}.ck.ck-list__item .ck-button{min-height:unset;width:100%;text-align:left;border-radius:0;padding:calc(var(--ck-line-height-base)*0.2*var(--ck-font-size-base)) calc(var(--ck-line-height-base)*0.4*var(--ck-font-size-base))}.ck.ck-list__item .ck-button .ck-button__label{line-height:calc(var(--ck-line-height-base)*1.2*var(--ck-font-size-base))}.ck.ck-list__item .ck-button:active{box-shadow:none}.ck.ck-list__item .ck-button.ck-on{background:var(--ck-color-list-button-on-background);color:var(--ck-color-list-button-on-text)}.ck.ck-list__item .ck-button.ck-on:active{box-shadow:none}.ck.ck-list__item .ck-button.ck-on:hover:not(.ck-disabled){background:var(--ck-color-list-button-on-background-focus)}.ck.ck-list__item .ck-button.ck-on:focus:not(.ck-disabled){border-color:var(--ck-color-base-background)}.ck.ck-list__item .ck-button:hover:not(.ck-disabled){background:var(--ck-color-list-button-hover-background)}.ck.ck-list__item .ck-switchbutton.ck-on{background:var(--ck-color-list-background);color:inherit}.ck.ck-list__item .ck-switchbutton.ck-on:hover:not(.ck-disabled){background:var(--ck-color-list-button-hover-background);color:inherit}.ck.ck-list__separator{height:1px;width:100%;background:var(--ck-color-base-border)}:root{--ck-toolbar-dropdown-max-width:60vw}.ck.ck-toolbar-dropdown>.ck-dropdown__panel{width:max-content;max-width:var(--ck-toolbar-dropdown-max-width)}.ck.ck-toolbar-dropdown>.ck-dropdown__panel .ck-button:focus{z-index:calc(var(--ck-z-default) + 1)}.ck.ck-toolbar-dropdown .ck-toolbar{border:0}.ck.ck-dropdown .ck-dropdown__panel .ck-list{border-radius:0}.ck-rounded-corners .ck.ck-dropdown .ck-dropdown__panel .ck-list,.ck.ck-dropdown .ck-dropdown__panel .ck-list.ck-rounded-corners{border-radius:var(--ck-border-radius);border-top-left-radius:0}.ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:first-child .ck-button{border-radius:0}.ck-rounded-corners .ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:first-child .ck-button,.ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:first-child .ck-button.ck-rounded-corners{border-radius:var(--ck-border-radius);border-top-left-radius:0;border-bottom-left-radius:0;border-bottom-right-radius:0}.ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:last-child .ck-button{border-radius:0}.ck-rounded-corners .ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:last-child .ck-button,.ck.ck-dropdown .ck-dropdown__panel .ck-list .ck-list__item:last-child .ck-button.ck-rounded-corners{border-radius:var(--ck-border-radius);border-top-left-radius:0;border-top-right-radius:0}:root{--ck-color-editable-blur-selection:#d9d9d9}.ck.ck-editor__editable:not(.ck-editor__nested-editable){border-radius:0}.ck-rounded-corners .ck.ck-editor__editable:not(.ck-editor__nested-editable),.ck.ck-editor__editable:not(.ck-editor__nested-editable).ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-editor__editable:not(.ck-editor__nested-editable).ck-focused{outline:none;border:var(--ck-focus-ring);box-shadow:var(--ck-inner-shadow),0 0}.ck.ck-editor__editable_inline{overflow:auto;padding:0 var(--ck-spacing-standard);border:1px solid transparent}.ck.ck-editor__editable_inline[dir=ltr]{text-align:left}.ck.ck-editor__editable_inline[dir=rtl]{text-align:right}.ck.ck-editor__editable_inline>:first-child{margin-top:var(--ck-spacing-large)}.ck.ck-editor__editable_inline>:last-child{margin-bottom:var(--ck-spacing-large)}.ck.ck-editor__editable_inline.ck-blurred ::selection{background:var(--ck-color-editable-blur-selection)}.ck.ck-balloon-panel.ck-toolbar-container[class*=arrow_n]:after{border-bottom-color:var(--ck-color-base-foreground)}.ck.ck-balloon-panel.ck-toolbar-container[class*=arrow_s]:after{border-top-color:var(--ck-color-base-foreground)}.ck.ck-label{display:block}.ck.ck-voice-label{display:none}.ck.ck-label{font-weight:700}.ck.ck-form__header{display:flex;flex-direction:row;flex-wrap:nowrap;align-items:center;justify-content:space-between}:root{--ck-form-header-height:38px}.ck.ck-form__header{padding:var(--ck-spacing-small) var(--ck-spacing-large);height:var(--ck-form-header-height);line-height:var(--ck-form-header-height);border-bottom:1px solid var(--ck-color-base-border)}.ck.ck-form__header .ck-form__header__label{font-weight:700}:root{--ck-input-text-width:18em}.ck.ck-input-text{border-radius:0}.ck-rounded-corners .ck.ck-input-text,.ck.ck-input-text.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-input-text{background:var(--ck-color-input-background);border:1px solid var(--ck-color-input-border);padding:var(--ck-spacing-extra-tiny) var(--ck-spacing-medium);min-width:var(--ck-input-text-width);min-height:var(--ck-ui-component-min-height);transition:box-shadow .1s ease-in-out,border .1s ease-in-out}.ck.ck-input-text:focus{outline:none;border:var(--ck-focus-ring);box-shadow:var(--ck-focus-outer-shadow),0 0}.ck.ck-input-text[readonly]{border:1px solid var(--ck-color-input-disabled-border);background:var(--ck-color-input-disabled-background);color:var(--ck-color-input-disabled-text)}.ck.ck-input-text[readonly]:focus{box-shadow:var(--ck-focus-disabled-outer-shadow),0 0}.ck.ck-input-text.ck-error{border-color:var(--ck-color-input-error-border);animation:ck-text-input-shake .3s ease both}.ck.ck-input-text.ck-error:focus{box-shadow:var(--ck-focus-error-outer-shadow),0 0}@keyframes ck-text-input-shake{20%{transform:translateX(-2px)}40%{transform:translateX(2px)}60%{transform:translateX(-1px)}80%{transform:translateX(1px)}}.ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper{display:flex;position:relative}.ck.ck-labeled-field-view .ck.ck-label{display:block;position:absolute}:root{--ck-labeled-field-view-transition:.1s cubic-bezier(0,0,0.24,0.95);--ck-labeled-field-empty-unfocused-max-width:100% - 2 * var(--ck-spacing-medium);--ck-color-labeled-field-label-background:var(--ck-color-base-background)}.ck.ck-labeled-field-view{border-radius:0}.ck-rounded-corners .ck.ck-labeled-field-view,.ck.ck-labeled-field-view.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper{width:100%}.ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{top:0}[dir=ltr] .ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{left:0}[dir=rtl] .ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{right:0}.ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{pointer-events:none;transform-origin:0 0;transform:translate(var(--ck-spacing-medium),-6px) scale(.75);background:var(--ck-color-labeled-field-label-background);padding:0 calc(var(--ck-font-size-tiny)*0.5);line-height:normal;font-weight:400;text-overflow:ellipsis;overflow:hidden;max-width:100%;transition:transform var(--ck-labeled-field-view-transition),padding var(--ck-labeled-field-view-transition),background var(--ck-labeled-field-view-transition)}.ck.ck-labeled-field-view.ck-error .ck-input:not([readonly])+.ck.ck-label,.ck.ck-labeled-field-view.ck-error>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{color:var(--ck-color-base-error)}.ck.ck-labeled-field-view .ck-labeled-field-view__status{font-size:var(--ck-font-size-small);margin-top:var(--ck-spacing-small);white-space:normal}.ck.ck-labeled-field-view .ck-labeled-field-view__status.ck-labeled-field-view__status_error{color:var(--ck-color-base-error)}.ck.ck-labeled-field-view.ck-disabled>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label,.ck.ck-labeled-field-view.ck-labeled-field-view_empty:not(.ck-labeled-field-view_focused)>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{color:var(--ck-color-input-disabled-text)}[dir=ltr] .ck.ck-labeled-field-view.ck-disabled.ck-labeled-field-view_empty>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label,[dir=ltr] .ck.ck-labeled-field-view.ck-labeled-field-view_empty:not(.ck-labeled-field-view_focused):not(.ck-labeled-field-view_placeholder)>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{transform:translate(var(--ck-spacing-medium),calc(var(--ck-font-size-base)*0.6)) scale(1)}[dir=rtl] .ck.ck-labeled-field-view.ck-disabled.ck-labeled-field-view_empty>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label,[dir=rtl] .ck.ck-labeled-field-view.ck-labeled-field-view_empty:not(.ck-labeled-field-view_focused):not(.ck-labeled-field-view_placeholder)>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{transform:translate(calc(var(--ck-spacing-medium)*-1),calc(var(--ck-font-size-base)*0.6)) scale(1)}.ck.ck-labeled-field-view.ck-disabled.ck-labeled-field-view_empty>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label,.ck.ck-labeled-field-view.ck-labeled-field-view_empty:not(.ck-labeled-field-view_focused):not(.ck-labeled-field-view_placeholder)>.ck.ck-labeled-field-view__input-wrapper>.ck.ck-label{max-width:calc(var(--ck-labeled-field-empty-unfocused-max-width));background:transparent;padding:0}.ck.ck-labeled-field-view>.ck.ck-labeled-field-view__input-wrapper>.ck-dropdown>.ck.ck-button{background:transparent}.ck.ck-labeled-field-view.ck-labeled-field-view_empty>.ck.ck-labeled-field-view__input-wrapper>.ck-dropdown>.ck-button>.ck-button__label{opacity:0}.ck.ck-labeled-field-view.ck-labeled-field-view_empty:not(.ck-labeled-field-view_focused):not(.ck-labeled-field-view_placeholder)>.ck.ck-labeled-field-view__input-wrapper>.ck-dropdown+.ck-label{max-width:calc(var(--ck-labeled-field-empty-unfocused-max-width) - var(--ck-dropdown-arrow-size) - var(--ck-spacing-standard))}:root{--ck-balloon-panel-arrow-z-index:calc(var(--ck-z-default) - 3)}.ck.ck-balloon-panel{display:none;position:absolute;z-index:var(--ck-z-modal)}.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:after,.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:before{content:"";position:absolute}.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:before{z-index:var(--ck-balloon-panel-arrow-z-index)}.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:after{z-index:calc(var(--ck-balloon-panel-arrow-z-index) + 1)}.ck.ck-balloon-panel[class*=arrow_n]:before{z-index:var(--ck-balloon-panel-arrow-z-index)}.ck.ck-balloon-panel[class*=arrow_n]:after{z-index:calc(var(--ck-balloon-panel-arrow-z-index) + 1)}.ck.ck-balloon-panel[class*=arrow_s]:before{z-index:var(--ck-balloon-panel-arrow-z-index)}.ck.ck-balloon-panel[class*=arrow_s]:after{z-index:calc(var(--ck-balloon-panel-arrow-z-index) + 1)}.ck.ck-balloon-panel.ck-balloon-panel_visible{display:block}:root{--ck-balloon-arrow-offset:2px;--ck-balloon-arrow-height:10px;--ck-balloon-arrow-half-width:8px;--ck-balloon-arrow-drop-shadow:0 2px 2px var(--ck-color-shadow-drop)}.ck.ck-balloon-panel{border-radius:0}.ck-rounded-corners .ck.ck-balloon-panel,.ck.ck-balloon-panel.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-balloon-panel{box-shadow:var(--ck-drop-shadow),0 0;min-height:15px;background:var(--ck-color-panel-background);border:1px solid var(--ck-color-panel-border)}.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:after,.ck.ck-balloon-panel.ck-balloon-panel_with-arrow:before{width:0;height:0;border-style:solid}.ck.ck-balloon-panel[class*=arrow_n]:after,.ck.ck-balloon-panel[class*=arrow_n]:before{border-left-width:var(--ck-balloon-arrow-half-width);border-bottom-width:var(--ck-balloon-arrow-height);border-right-width:var(--ck-balloon-arrow-half-width);border-top-width:0}.ck.ck-balloon-panel[class*=arrow_n]:before{border-bottom-color:var(--ck-color-panel-border)}.ck.ck-balloon-panel[class*=arrow_n]:after,.ck.ck-balloon-panel[class*=arrow_n]:before{border-left-color:transparent;border-right-color:transparent;border-top-color:transparent}.ck.ck-balloon-panel[class*=arrow_n]:after{border-bottom-color:var(--ck-color-panel-background);margin-top:var(--ck-balloon-arrow-offset)}.ck.ck-balloon-panel[class*=arrow_s]:after,.ck.ck-balloon-panel[class*=arrow_s]:before{border-left-width:var(--ck-balloon-arrow-half-width);border-bottom-width:0;border-right-width:var(--ck-balloon-arrow-half-width);border-top-width:var(--ck-balloon-arrow-height)}.ck.ck-balloon-panel[class*=arrow_s]:before{border-top-color:var(--ck-color-panel-border);filter:drop-shadow(var(--ck-balloon-arrow-drop-shadow))}.ck.ck-balloon-panel[class*=arrow_s]:after,.ck.ck-balloon-panel[class*=arrow_s]:before{border-left-color:transparent;border-bottom-color:transparent;border-right-color:transparent}.ck.ck-balloon-panel[class*=arrow_s]:after{border-top-color:var(--ck-color-panel-background);margin-bottom:var(--ck-balloon-arrow-offset)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_n:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_n:before{left:50%;margin-left:calc(var(--ck-balloon-arrow-half-width)*-1);top:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_nw:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_nw:before{left:calc(var(--ck-balloon-arrow-half-width)*2);top:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_ne:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_ne:before{right:calc(var(--ck-balloon-arrow-half-width)*2);top:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_s:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_s:before{left:50%;margin-left:calc(var(--ck-balloon-arrow-half-width)*-1);bottom:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_sw:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_sw:before{left:calc(var(--ck-balloon-arrow-half-width)*2);bottom:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_se:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_se:before{right:calc(var(--ck-balloon-arrow-half-width)*2);bottom:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_sme:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_sme:before{right:25%;margin-right:calc(var(--ck-balloon-arrow-half-width)*2);bottom:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_smw:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_smw:before{left:25%;margin-left:calc(var(--ck-balloon-arrow-half-width)*2);bottom:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_nme:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_nme:before{right:25%;margin-right:calc(var(--ck-balloon-arrow-half-width)*2);top:calc(var(--ck-balloon-arrow-height)*-1)}.ck.ck-balloon-panel.ck-balloon-panel_arrow_nmw:after,.ck.ck-balloon-panel.ck-balloon-panel_arrow_nmw:before{left:25%;margin-left:calc(var(--ck-balloon-arrow-half-width)*2);top:calc(var(--ck-balloon-arrow-height)*-1)}.ck .ck-balloon-rotator__navigation{display:flex;align-items:center;justify-content:center}.ck .ck-balloon-rotator__content .ck-toolbar{justify-content:center}.ck .ck-balloon-rotator__navigation{background:var(--ck-color-toolbar-background);border-bottom:1px solid var(--ck-color-toolbar-border);padding:0 var(--ck-spacing-small)}.ck .ck-balloon-rotator__navigation>*{margin-right:var(--ck-spacing-small);margin-top:var(--ck-spacing-small);margin-bottom:var(--ck-spacing-small)}.ck .ck-balloon-rotator__navigation .ck-balloon-rotator__counter{margin-right:var(--ck-spacing-standard);margin-left:var(--ck-spacing-small)}.ck .ck-balloon-rotator__content .ck.ck-annotation-wrapper{box-shadow:none}.ck .ck-fake-panel{position:absolute;z-index:calc(var(--ck-z-modal) - 1)}.ck .ck-fake-panel div{position:absolute}.ck .ck-fake-panel div:first-child{z-index:2}.ck .ck-fake-panel div:nth-child(2){z-index:1}:root{--ck-balloon-fake-panel-offset-horizontal:6px;--ck-balloon-fake-panel-offset-vertical:6px}.ck .ck-fake-panel div{box-shadow:var(--ck-drop-shadow),0 0;min-height:15px;background:var(--ck-color-panel-background);border:1px solid var(--ck-color-panel-border);border-radius:var(--ck-border-radius);width:100%;height:100%}.ck .ck-fake-panel div:first-child{margin-left:var(--ck-balloon-fake-panel-offset-horizontal);margin-top:var(--ck-balloon-fake-panel-offset-vertical)}.ck .ck-fake-panel div:nth-child(2){margin-left:calc(var(--ck-balloon-fake-panel-offset-horizontal)*2);margin-top:calc(var(--ck-balloon-fake-panel-offset-vertical)*2)}.ck .ck-fake-panel div:nth-child(3){margin-left:calc(var(--ck-balloon-fake-panel-offset-horizontal)*3);margin-top:calc(var(--ck-balloon-fake-panel-offset-vertical)*3)}.ck .ck-balloon-panel_arrow_s+.ck-fake-panel,.ck .ck-balloon-panel_arrow_se+.ck-fake-panel,.ck .ck-balloon-panel_arrow_sw+.ck-fake-panel{--ck-balloon-fake-panel-offset-vertical:-6px}.ck.ck-sticky-panel .ck-sticky-panel__content_sticky{z-index:var(--ck-z-modal);position:fixed;top:0}.ck.ck-sticky-panel .ck-sticky-panel__content_sticky_bottom-limit{top:auto;position:absolute}.ck.ck-sticky-panel .ck-sticky-panel__content_sticky{box-shadow:var(--ck-drop-shadow),0 0;border-width:0 1px 1px;border-top-left-radius:0;border-top-right-radius:0}.ck.ck-block-toolbar-button{position:absolute;z-index:var(--ck-z-default)}:root{--ck-color-block-toolbar-button:var(--ck-color-text);--ck-block-toolbar-button-size:var(--ck-font-size-normal)}.ck.ck-block-toolbar-button{color:var(--ck-color-block-toolbar-button);font-size:var(--ck-block-toolbar-size)}.ck.ck-placeholder,.ck .ck-placeholder{position:relative}.ck.ck-placeholder:before,.ck .ck-placeholder:before{position:absolute;left:0;right:0;content:attr(data-placeholder);pointer-events:none}.ck.ck-read-only .ck-placeholder:before{display:none}.ck.ck-placeholder:before,.ck .ck-placeholder:before{cursor:text;color:var(--ck-color-engine-placeholder-text)}.ck-content blockquote{overflow:hidden;padding-right:1.5em;padding-left:1.5em;margin-left:0;margin-right:0;font-style:italic;border-left:5px solid #ccc}.ck-content[dir=rtl] blockquote{border-left:0;border-right:5px solid #ccc}.ck .ck-widget .ck-widget__type-around__button{display:block;position:absolute;overflow:hidden;z-index:var(--ck-z-default)}.ck .ck-widget .ck-widget__type-around__button svg{position:absolute;top:50%;left:50%;z-index:calc(var(--ck-z-default) + 2)}.ck .ck-widget .ck-widget__type-around__button.ck-widget__type-around__button_before{top:calc(var(--ck-widget-outline-thickness)*-0.5);left:min(10%,30px);transform:translateY(-50%)}.ck .ck-widget .ck-widget__type-around__button.ck-widget__type-around__button_after{bottom:calc(var(--ck-widget-outline-thickness)*-0.5);right:min(10%,30px);transform:translateY(50%)}.ck .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button:after,.ck .ck-widget>.ck-widget__type-around>.ck-widget__type-around__button:hover:after{content:"";display:block;position:absolute;top:1px;left:1px;z-index:calc(var(--ck-z-default) + 1)}.ck .ck-widget>.ck-widget__type-around>.ck-widget__type-around__fake-caret{display:none;position:absolute;left:0;right:0}.ck .ck-widget:hover>.ck-widget__type-around>.ck-widget__type-around__fake-caret{left:calc(var(--ck-widget-outline-thickness)*-1);right:calc(var(--ck-widget-outline-thickness)*-1)}.ck .ck-widget.ck-widget_type-around_show-fake-caret_before>.ck-widget__type-around>.ck-widget__type-around__fake-caret{top:calc(var(--ck-widget-outline-thickness)*-1 - 1px);display:block}.ck .ck-widget.ck-widget_type-around_show-fake-caret_after>.ck-widget__type-around>.ck-widget__type-around__fake-caret{bottom:calc(var(--ck-widget-outline-thickness)*-1 - 1px);display:block}.ck.ck-editor__editable.ck-read-only .ck-widget__type-around,.ck.ck-editor__editable.ck-restricted-editing_mode_restricted .ck-widget__type-around,.ck.ck-editor__editable.ck-widget__type-around_disabled .ck-widget__type-around{display:none}:root{--ck-widget-type-around-button-size:20px;--ck-color-widget-type-around-button-active:var(--ck-color-focus-border);--ck-color-widget-type-around-button-hover:var(--ck-color-widget-hover-border);--ck-color-widget-type-around-button-blurred-editable:var(--ck-color-widget-blurred-border);--ck-color-widget-type-around-button-radar-start-alpha:0;--ck-color-widget-type-around-button-radar-end-alpha:.3;--ck-color-widget-type-around-button-icon:var(--ck-color-base-background)}.ck .ck-widget .ck-widget__type-around__button{width:var(--ck-widget-type-around-button-size);height:var(--ck-widget-type-around-button-size);background:var(--ck-color-widget-type-around-button);border-radius:100px;transition:opacity var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve),background var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve);opacity:0;pointer-events:none}.ck .ck-widget .ck-widget__type-around__button svg{width:10px;height:8px;transform:translate(-50%,-50%);transition:transform .5s ease;margin-top:1px}.ck .ck-widget .ck-widget__type-around__button svg *{stroke-dasharray:10;stroke-dashoffset:0;fill:none;stroke:var(--ck-color-widget-type-around-button-icon);stroke-width:1.5px;stroke-linecap:round;stroke-linejoin:round}.ck .ck-widget .ck-widget__type-around__button svg line{stroke-dasharray:7}.ck .ck-widget .ck-widget__type-around__button:hover{animation:ck-widget-type-around-button-sonar 1s ease infinite}.ck .ck-widget .ck-widget__type-around__button:hover svg polyline{animation:ck-widget-type-around-arrow-dash 2s linear}.ck .ck-widget .ck-widget__type-around__button:hover svg line{animation:ck-widget-type-around-arrow-tip-dash 2s linear}.ck .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button,.ck .ck-widget:hover>.ck-widget__type-around>.ck-widget__type-around__button{opacity:1;pointer-events:auto}.ck .ck-widget:not(.ck-widget_selected)>.ck-widget__type-around>.ck-widget__type-around__button{background:var(--ck-color-widget-type-around-button-hover)}.ck .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button,.ck .ck-widget>.ck-widget__type-around>.ck-widget__type-around__button:hover{background:var(--ck-color-widget-type-around-button-active)}.ck .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button:after,.ck .ck-widget>.ck-widget__type-around>.ck-widget__type-around__button:hover:after{width:calc(var(--ck-widget-type-around-button-size) - 2px);height:calc(var(--ck-widget-type-around-button-size) - 2px);border-radius:100px;background:linear-gradient(135deg,hsla(0,0%,100%,0),hsla(0,0%,100%,.3))}.ck .ck-widget.ck-widget_with-selection-handle>.ck-widget__type-around>.ck-widget__type-around__button_before{margin-left:20px}.ck .ck-widget .ck-widget__type-around__fake-caret{pointer-events:none;height:1px;animation:ck-widget-type-around-fake-caret-pulse 1s linear infinite normal forwards;outline:1px solid hsla(0,0%,100%,.5);background:var(--ck-color-base-text)}.ck .ck-widget.ck-widget_selected.ck-widget_type-around_show-fake-caret_after,.ck .ck-widget.ck-widget_selected.ck-widget_type-around_show-fake-caret_before{outline-color:transparent}.ck .ck-widget.ck-widget_type-around_show-fake-caret_after.ck-widget_selected:hover,.ck .ck-widget.ck-widget_type-around_show-fake-caret_before.ck-widget_selected:hover{outline-color:var(--ck-color-widget-hover-border)}.ck .ck-widget.ck-widget_type-around_show-fake-caret_after>.ck-widget__type-around>.ck-widget__type-around__button,.ck .ck-widget.ck-widget_type-around_show-fake-caret_before>.ck-widget__type-around>.ck-widget__type-around__button{opacity:0;pointer-events:none}.ck .ck-widget.ck-widget_type-around_show-fake-caret_after.ck-widget_with-selection-handle.ck-widget_selected:hover>.ck-widget__selection-handle,.ck .ck-widget.ck-widget_type-around_show-fake-caret_after.ck-widget_with-selection-handle.ck-widget_selected>.ck-widget__selection-handle,.ck .ck-widget.ck-widget_type-around_show-fake-caret_before.ck-widget_with-selection-handle.ck-widget_selected:hover>.ck-widget__selection-handle,.ck .ck-widget.ck-widget_type-around_show-fake-caret_before.ck-widget_with-selection-handle.ck-widget_selected>.ck-widget__selection-handle{opacity:0}.ck .ck-widget.ck-widget_type-around_show-fake-caret_after.ck-widget_selected.ck-widget_with-resizer>.ck-widget__resizer,.ck .ck-widget.ck-widget_type-around_show-fake-caret_before.ck-widget_selected.ck-widget_with-resizer>.ck-widget__resizer{opacity:0}.ck[dir=rtl] .ck-widget.ck-widget_with-selection-handle .ck-widget__type-around>.ck-widget__type-around__button_before{margin-left:0;margin-right:20px}.ck-editor__nested-editable.ck-editor__editable_selected .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button,.ck-editor__nested-editable.ck-editor__editable_selected .ck-widget:hover>.ck-widget__type-around>.ck-widget__type-around__button{opacity:0;pointer-events:none}.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button:not(:hover){background:var(--ck-color-widget-type-around-button-blurred-editable)}.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected>.ck-widget__type-around>.ck-widget__type-around__button:not(:hover) svg *{stroke:#999}@keyframes ck-widget-type-around-arrow-dash{0%{stroke-dashoffset:10}20%,to{stroke-dashoffset:0}}@keyframes ck-widget-type-around-arrow-tip-dash{0%,20%{stroke-dashoffset:7}40%,to{stroke-dashoffset:0}}@keyframes ck-widget-type-around-button-sonar{0%{box-shadow:0 0 0 0 hsla(var(--ck-color-focus-border-coordinates),var(--ck-color-widget-type-around-button-radar-start-alpha))}50%{box-shadow:0 0 0 5px hsla(var(--ck-color-focus-border-coordinates),var(--ck-color-widget-type-around-button-radar-end-alpha))}to{box-shadow:0 0 0 5px hsla(var(--ck-color-focus-border-coordinates),var(--ck-color-widget-type-around-button-radar-start-alpha))}}@keyframes ck-widget-type-around-fake-caret-pulse{0%{opacity:1}49%{opacity:1}50%{opacity:0}99%{opacity:0}to{opacity:1}}:root{--ck-color-resizer:var(--ck-color-focus-border);--ck-color-resizer-tooltip-background:#262626;--ck-color-resizer-tooltip-text:#f2f2f2;--ck-resizer-border-radius:var(--ck-border-radius);--ck-resizer-tooltip-offset:10px;--ck-resizer-tooltip-height:calc(var(--ck-spacing-small)*2 + 10px)}.ck .ck-widget,.ck .ck-widget.ck-widget_with-selection-handle{position:relative}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle{position:absolute}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle .ck-icon{display:block}.ck .ck-widget.ck-widget_with-selection-handle.ck-widget_selected>.ck-widget__selection-handle,.ck .ck-widget.ck-widget_with-selection-handle:hover>.ck-widget__selection-handle{visibility:visible}.ck .ck-size-view{background:var(--ck-color-resizer-tooltip-background);color:var(--ck-color-resizer-tooltip-text);border:1px solid var(--ck-color-resizer-tooltip-text);border-radius:var(--ck-resizer-border-radius);font-size:var(--ck-font-size-tiny);display:block;padding:0 var(--ck-spacing-small);height:var(--ck-resizer-tooltip-height);line-height:var(--ck-resizer-tooltip-height)}.ck .ck-size-view.ck-orientation-above-center,.ck .ck-size-view.ck-orientation-bottom-left,.ck .ck-size-view.ck-orientation-bottom-right,.ck .ck-size-view.ck-orientation-top-left,.ck .ck-size-view.ck-orientation-top-right{position:absolute}.ck .ck-size-view.ck-orientation-top-left{top:var(--ck-resizer-tooltip-offset);left:var(--ck-resizer-tooltip-offset)}.ck .ck-size-view.ck-orientation-top-right{top:var(--ck-resizer-tooltip-offset);right:var(--ck-resizer-tooltip-offset)}.ck .ck-size-view.ck-orientation-bottom-right{bottom:var(--ck-resizer-tooltip-offset);right:var(--ck-resizer-tooltip-offset)}.ck .ck-size-view.ck-orientation-bottom-left{bottom:var(--ck-resizer-tooltip-offset);left:var(--ck-resizer-tooltip-offset)}.ck .ck-size-view.ck-orientation-above-center{top:calc(var(--ck-resizer-tooltip-height)*-1);left:50%;transform:translate(-50%)}:root{--ck-widget-outline-thickness:3px;--ck-widget-handler-icon-size:16px;--ck-widget-handler-animation-duration:200ms;--ck-widget-handler-animation-curve:ease;--ck-color-widget-blurred-border:#dedede;--ck-color-widget-hover-border:#ffc83d;--ck-color-widget-editable-focus-background:var(--ck-color-base-background);--ck-color-widget-drag-handler-icon-color:var(--ck-color-base-background)}.ck .ck-widget{outline-width:var(--ck-widget-outline-thickness);outline-style:solid;outline-color:transparent;transition:outline-color var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve)}.ck .ck-widget.ck-widget_selected,.ck .ck-widget.ck-widget_selected:hover{outline:var(--ck-widget-outline-thickness) solid var(--ck-color-focus-border)}.ck .ck-widget:hover{outline-color:var(--ck-color-widget-hover-border)}.ck .ck-editor__nested-editable{border:1px solid transparent}.ck .ck-editor__nested-editable.ck-editor__nested-editable_focused,.ck .ck-editor__nested-editable:focus{outline:none;border:var(--ck-focus-ring);box-shadow:var(--ck-inner-shadow),0 0;background-color:var(--ck-color-widget-editable-focus-background)}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle{padding:4px;box-sizing:border-box;background-color:transparent;opacity:0;transition:background-color var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve),visibility var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve),opacity var(--ck-widget-handler-animation-duration) var(--ck-widget-handler-animation-curve);border-radius:var(--ck-border-radius) var(--ck-border-radius) 0 0;transform:translateY(-100%);left:calc(0px - var(--ck-widget-outline-thickness));top:0}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle .ck-icon{width:var(--ck-widget-handler-icon-size);height:var(--ck-widget-handler-icon-size);color:var(--ck-color-widget-drag-handler-icon-color)}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle .ck-icon .ck-icon__selected-indicator{opacity:0;transition:opacity .3s var(--ck-widget-handler-animation-curve)}.ck .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle:hover .ck-icon .ck-icon__selected-indicator{opacity:1}.ck .ck-widget.ck-widget_with-selection-handle:hover>.ck-widget__selection-handle{opacity:1;background-color:var(--ck-color-widget-hover-border)}.ck .ck-widget.ck-widget_with-selection-handle.ck-widget_selected:hover>.ck-widget__selection-handle,.ck .ck-widget.ck-widget_with-selection-handle.ck-widget_selected>.ck-widget__selection-handle{opacity:1;background-color:var(--ck-color-focus-border)}.ck .ck-widget.ck-widget_with-selection-handle.ck-widget_selected:hover>.ck-widget__selection-handle .ck-icon .ck-icon__selected-indicator,.ck .ck-widget.ck-widget_with-selection-handle.ck-widget_selected>.ck-widget__selection-handle .ck-icon .ck-icon__selected-indicator{opacity:1}.ck[dir=rtl] .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle{left:auto;right:calc(0px - var(--ck-widget-outline-thickness))}.ck.ck-editor__editable.ck-read-only .ck-widget{transition:none}.ck.ck-editor__editable.ck-read-only .ck-widget:not(.ck-widget_selected){--ck-widget-outline-thickness:0px}.ck.ck-editor__editable.ck-read-only .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle,.ck.ck-editor__editable.ck-read-only .ck-widget.ck-widget_with-selection-handle .ck-widget__selection-handle:hover{background:var(--ck-color-widget-blurred-border)}.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected,.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected:hover{outline-color:var(--ck-color-widget-blurred-border)}.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected.ck-widget_with-selection-handle>.ck-widget__selection-handle,.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected.ck-widget_with-selection-handle>.ck-widget__selection-handle:hover,.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected:hover.ck-widget_with-selection-handle>.ck-widget__selection-handle,.ck.ck-editor__editable.ck-blurred .ck-widget.ck-widget_selected:hover.ck-widget_with-selection-handle>.ck-widget__selection-handle:hover{background:var(--ck-color-widget-blurred-border)}.ck.ck-editor__editable>.ck-widget.ck-widget_with-selection-handle:first-child,.ck.ck-editor__editable blockquote>.ck-widget.ck-widget_with-selection-handle:first-child{margin-top:calc(1em + var(--ck-widget-handler-icon-size))}.ck.ck-editor__editable .ck.ck-clipboard-drop-target-position{display:inline;position:relative;pointer-events:none}.ck.ck-editor__editable .ck.ck-clipboard-drop-target-position span{position:absolute;width:0}.ck.ck-editor__editable .ck-widget:-webkit-drag>.ck-widget__selection-handle,.ck.ck-editor__editable .ck-widget:-webkit-drag>.ck-widget__type-around{display:none}:root{--ck-clipboard-drop-target-dot-width:12px;--ck-clipboard-drop-target-dot-height:8px;--ck-clipboard-drop-target-color:var(--ck-color-focus-border)}.ck.ck-editor__editable .ck.ck-clipboard-drop-target-position span{bottom:calc(var(--ck-clipboard-drop-target-dot-height)*-0.5);top:calc(var(--ck-clipboard-drop-target-dot-height)*-0.5);border:1px solid var(--ck-clipboard-drop-target-color);background:var(--ck-clipboard-drop-target-color);margin-left:-1px}.ck.ck-editor__editable .ck.ck-clipboard-drop-target-position span:after{content:"";width:0;height:0;display:block;position:absolute;left:50%;top:calc(var(--ck-clipboard-drop-target-dot-height)*-0.5);transform:translateX(-50%);border-left:calc(var(--ck-clipboard-drop-target-dot-width)*0.5) solid transparent;border-bottom:0 solid transparent;border-right:calc(var(--ck-clipboard-drop-target-dot-width)*0.5) solid transparent;border-top:calc(var(--ck-clipboard-drop-target-dot-height)) solid var(--ck-clipboard-drop-target-color)}.ck.ck-editor__editable .ck-widget.ck-clipboard-drop-target-range{outline:var(--ck-widget-outline-thickness) solid var(--ck-clipboard-drop-target-color)!important}.ck.ck-editor__editable .ck-widget:-webkit-drag{zoom:.6;outline:none!important}.ck .ck-button.ck-color-table__remove-color{display:flex;align-items:center;width:100%}label.ck.ck-color-grid__label{font-weight:unset}.ck .ck-button.ck-color-table__remove-color{padding:calc(var(--ck-spacing-standard)/2) var(--ck-spacing-standard);border-bottom-left-radius:0;border-bottom-right-radius:0}.ck .ck-button.ck-color-table__remove-color:not(:focus){border-bottom:1px solid var(--ck-color-base-border)}[dir=ltr] .ck .ck-button.ck-color-table__remove-color .ck.ck-icon{margin-right:var(--ck-spacing-standard)}[dir=rtl] .ck .ck-button.ck-color-table__remove-color .ck.ck-icon{margin-left:var(--ck-spacing-standard)}.ck-content .text-tiny{font-size:.7em}.ck-content .text-small{font-size:.85em}.ck-content .text-big{font-size:1.4em}.ck-content .text-huge{font-size:1.8em}.ck.ck-heading_heading1{font-size:20px}.ck.ck-heading_heading2{font-size:17px}.ck.ck-heading_heading3{font-size:14px}.ck[class*=ck-heading_heading]{font-weight:700}.ck.ck-dropdown.ck-heading-dropdown .ck-dropdown__button .ck-button__label{width:8em}.ck.ck-dropdown.ck-heading-dropdown .ck-dropdown__panel .ck-list__item{min-width:18em}.ck .ck-widget_with-resizer{position:relative}.ck .ck-widget__resizer{display:none;position:absolute;pointer-events:none;left:0;top:0}.ck-focused .ck-widget_with-resizer.ck-widget_selected>.ck-widget__resizer{display:block}.ck .ck-widget__resizer__handle{position:absolute;pointer-events:all}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-bottom-right,.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-top-left{cursor:nwse-resize}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-bottom-left,.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-top-right{cursor:nesw-resize}:root{--ck-resizer-size:10px;--ck-resizer-offset:calc(var(--ck-resizer-size)/-2 - 2px);--ck-resizer-border-width:1px}.ck .ck-widget__resizer{outline:1px solid var(--ck-color-resizer)}.ck .ck-widget__resizer__handle{width:var(--ck-resizer-size);height:var(--ck-resizer-size);background:var(--ck-color-focus-border);border:var(--ck-resizer-border-width) solid #fff;border-radius:var(--ck-resizer-border-radius)}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-top-left{top:var(--ck-resizer-offset);left:var(--ck-resizer-offset)}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-top-right{top:var(--ck-resizer-offset);right:var(--ck-resizer-offset)}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-bottom-right{bottom:var(--ck-resizer-offset);right:var(--ck-resizer-offset)}.ck .ck-widget__resizer__handle.ck-widget__resizer__handle-bottom-left{bottom:var(--ck-resizer-offset);left:var(--ck-resizer-offset)}.ck-editor__editable .ck-horizontal-line{display:flow-root}.ck-content hr{margin:15px 0;height:4px;background:#dedede;border:0}.ck.ck-text-alternative-form{display:flex;flex-direction:row;flex-wrap:nowrap}.ck.ck-text-alternative-form .ck-labeled-field-view{display:inline-block}.ck.ck-text-alternative-form .ck-label{display:none}@media screen and (max-width:600px){.ck.ck-text-alternative-form{flex-wrap:wrap}.ck.ck-text-alternative-form .ck-labeled-field-view{flex-basis:100%}.ck.ck-text-alternative-form .ck-button{flex-basis:50%}}.ck-vertical-form .ck-button:after{content:"";width:0;position:absolute;right:-1px;top:var(--ck-spacing-small);bottom:var(--ck-spacing-small);z-index:1}@media screen and (max-width:600px){.ck.ck-responsive-form .ck-button:after{content:"";width:0;position:absolute;right:-1px;top:var(--ck-spacing-small);bottom:var(--ck-spacing-small);z-index:1}}.ck-vertical-form>.ck-button:nth-last-child(2):after{border-right:1px solid var(--ck-color-base-border)}.ck.ck-responsive-form{padding:var(--ck-spacing-large)}.ck.ck-responsive-form:focus{outline:none}[dir=ltr] .ck.ck-responsive-form>:not(:first-child),[dir=rtl] .ck.ck-responsive-form>:not(:last-child){margin-left:var(--ck-spacing-standard)}@media screen and (max-width:600px){.ck.ck-responsive-form{padding:0;width:calc(var(--ck-input-text-width)*0.8)}.ck.ck-responsive-form .ck-labeled-field-view{margin:var(--ck-spacing-large) var(--ck-spacing-large) 0}.ck.ck-responsive-form .ck-labeled-field-view .ck-input-text{min-width:0;width:100%}.ck.ck-responsive-form .ck-labeled-field-view .ck-labeled-field-view__error{white-space:normal}.ck.ck-responsive-form>.ck-button:last-child,.ck.ck-responsive-form>.ck-button:nth-last-child(2){padding:var(--ck-spacing-standard);margin-top:var(--ck-spacing-large);border-radius:0;border:0;border-top:1px solid var(--ck-color-base-border)}[dir=ltr] .ck.ck-responsive-form>.ck-button:last-child,[dir=ltr] .ck.ck-responsive-form>.ck-button:nth-last-child(2),[dir=rtl] .ck.ck-responsive-form>.ck-button:last-child,[dir=rtl] .ck.ck-responsive-form>.ck-button:nth-last-child(2){margin-left:0}.ck.ck-responsive-form>.ck-button:nth-last-child(2):after,[dir=rtl] .ck.ck-responsive-form>.ck-button:last-child:last-of-type,[dir=rtl] .ck.ck-responsive-form>.ck-button:nth-last-child(2):last-of-type{border-right:1px solid var(--ck-color-base-border)}}.ck-content .image{display:table;clear:both;text-align:center;margin:.9em auto;min-width:50px}.ck-content .image img{display:block;margin:0 auto;max-width:100%;min-width:100%}.ck-content .image-inline{display:inline-flex;max-width:100%;align-items:flex-start}.ck-content .image-inline picture{display:flex}.ck-content .image-inline img,.ck-content .image-inline picture{flex-grow:1;flex-shrink:1;max-width:100%}.ck.ck-editor__editable .image>figcaption.ck-placeholder:before{padding-left:inherit;padding-right:inherit;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.ck.ck-editor__editable .image-inline.ck-widget_selected,.ck.ck-editor__editable .image.ck-widget_selected{z-index:1}.ck.ck-editor__editable .image-inline.ck-widget_selected ::selection{display:none}.ck.ck-editor__editable td .image-inline img,.ck.ck-editor__editable th .image-inline img{max-width:none}:root{--ck-color-image-caption-background:#f7f7f7;--ck-color-image-caption-text:#333;--ck-color-image-caption-highligted-background:#fd0}.ck-content .image>figcaption{display:table-caption;caption-side:bottom;word-break:break-word;color:var(--ck-color-image-caption-text);background-color:var(--ck-color-image-caption-background);padding:.6em;font-size:.75em;outline-offset:-1px}.ck.ck-editor__editable .image>figcaption.image__caption_highlighted{animation:ck-image-caption-highlight .6s ease-out}@keyframes ck-image-caption-highlight{0%{background-color:var(--ck-color-image-caption-highligted-background)}to{background-color:var(--ck-color-image-caption-background)}}:root{--ck-image-style-spacing:1.5em;--ck-inline-image-style-spacing:calc(var(--ck-image-style-spacing)/2)}.ck-content .image-style-block-align-left,.ck-content .image-style-block-align-right{max-width:calc(100% - var(--ck-image-style-spacing))}.ck-content .image-style-align-left,.ck-content .image-style-align-right{clear:none}.ck-content .image-style-side{float:right;margin-left:var(--ck-image-style-spacing);max-width:50%}.ck-content .image-style-align-left{float:left;margin-right:var(--ck-image-style-spacing)}.ck-content .image-style-align-center{margin-left:auto;margin-right:auto}.ck-content .image-style-align-right{float:right;margin-left:var(--ck-image-style-spacing)}.ck-content .image-style-block-align-right{margin-right:0;margin-left:auto}.ck-content .image-style-block-align-left{margin-left:0;margin-right:auto}.ck-content p+.image-style-align-left,.ck-content p+.image-style-align-right,.ck-content p+.image-style-side{margin-top:0}.ck-content .image-inline.image-style-align-left,.ck-content .image-inline.image-style-align-right{margin-top:var(--ck-inline-image-style-spacing);margin-bottom:var(--ck-inline-image-style-spacing)}.ck-content .image-inline.image-style-align-left{margin-right:var(--ck-inline-image-style-spacing)}.ck-content .image-inline.image-style-align-right{margin-left:var(--ck-inline-image-style-spacing)}.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__action:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):not(:hover),.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__action:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__arrow:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__arrow:not(.ck-disabled):not(:hover){background-color:var(--ck-color-button-on-background)}.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__action:not(.ck-disabled):after,.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):after,.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open>.ck-splitbutton__arrow:not(.ck-disabled):not(:hover):after,.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__action:not(.ck-disabled):after,.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__arrow:not(.ck-disabled):after,.ck.ck-splitbutton.ck-splitbutton_flatten:hover>.ck-splitbutton__arrow:not(.ck-disabled):not(:hover):after{display:none}.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open:hover>.ck-splitbutton__action:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open:hover>.ck-splitbutton__arrow:not(.ck-disabled),.ck.ck-splitbutton.ck-splitbutton_flatten.ck-splitbutton_open:hover>.ck-splitbutton__arrow:not(.ck-disabled):not(:hover){background-color:var(--ck-color-button-on-hover-background)}.ck.ck-editor__editable .image,.ck.ck-editor__editable .image-inline{position:relative}.ck.ck-editor__editable .image-inline .ck-progress-bar,.ck.ck-editor__editable .image .ck-progress-bar{position:absolute;top:0;left:0}.ck.ck-editor__editable .image-inline.ck-appear,.ck.ck-editor__editable .image.ck-appear{animation:fadeIn .7s}.ck.ck-editor__editable .image-inline .ck-progress-bar,.ck.ck-editor__editable .image .ck-progress-bar{height:2px;width:0;background:var(--ck-color-upload-bar-background);transition:width .1s}@keyframes fadeIn{0%{opacity:0}to{opacity:1}}.ck-image-upload-complete-icon{display:block;position:absolute;top:min(var(--ck-spacing-medium),6%);right:min(var(--ck-spacing-medium),6%);border-radius:50%;z-index:1}.ck-image-upload-complete-icon:after{content:"";position:absolute}:root{--ck-color-image-upload-icon:#fff;--ck-color-image-upload-icon-background:#008a00;--ck-image-upload-icon-size:20;--ck-image-upload-icon-width:2px;--ck-image-upload-icon-is-visible:clamp(0px,100% - 50px,1px)}.ck-image-upload-complete-icon{opacity:0;background:var(--ck-color-image-upload-icon-background);animation-name:ck-upload-complete-icon-show,ck-upload-complete-icon-hide;animation-fill-mode:forwards,forwards;animation-duration:.5s,.5s;font-size:calc(1px*var(--ck-image-upload-icon-size));animation-delay:0ms,3s;overflow:hidden;width:calc(var(--ck-image-upload-icon-is-visible)*var(--ck-image-upload-icon-size));height:calc(var(--ck-image-upload-icon-is-visible)*var(--ck-image-upload-icon-size))}.ck-image-upload-complete-icon:after{left:25%;top:50%;opacity:0;height:0;width:0;transform:scaleX(-1) rotate(135deg);transform-origin:left top;border-top:var(--ck-image-upload-icon-width) solid var(--ck-color-image-upload-icon);border-right:var(--ck-image-upload-icon-width) solid var(--ck-color-image-upload-icon);animation-name:ck-upload-complete-icon-check;animation-duration:.5s;animation-delay:.5s;animation-fill-mode:forwards;box-sizing:border-box}@keyframes ck-upload-complete-icon-show{0%{opacity:0}to{opacity:1}}@keyframes ck-upload-complete-icon-hide{0%{opacity:1}to{opacity:0}}@keyframes ck-upload-complete-icon-check{0%{opacity:1;width:0;height:0}33%{width:.3em;height:0}to{opacity:1;width:.3em;height:.45em}}.ck .ck-upload-placeholder-loader{position:absolute;display:flex;align-items:center;justify-content:center;top:0;left:0}.ck .ck-upload-placeholder-loader:before{content:"";position:relative}:root{--ck-color-upload-placeholder-loader:#b3b3b3;--ck-upload-placeholder-loader-size:32px;--ck-upload-placeholder-image-aspect-ratio:2.8}.ck .ck-image-upload-placeholder{width:100%;margin:0}.ck .ck-image-upload-placeholder.image-inline{width:calc(var(--ck-upload-placeholder-loader-size)*2*var(--ck-upload-placeholder-image-aspect-ratio))}.ck .ck-image-upload-placeholder img{aspect-ratio:var(--ck-upload-placeholder-image-aspect-ratio)}.ck .ck-upload-placeholder-loader{width:100%;height:100%}.ck .ck-upload-placeholder-loader:before{width:var(--ck-upload-placeholder-loader-size);height:var(--ck-upload-placeholder-loader-size);border-radius:50%;border-top:3px solid var(--ck-color-upload-placeholder-loader);border-right:2px solid transparent;animation:ck-upload-placeholder-loader 1s linear infinite}@keyframes ck-upload-placeholder-loader{to{transform:rotate(1turn)}}.ck-content .image.image_resized{max-width:100%;display:block;box-sizing:border-box}.ck-content .image.image_resized img{width:100%}.ck-content .image.image_resized>figcaption{display:block}.ck.ck-editor__editable td .image-inline.image_resized img,.ck.ck-editor__editable th .image-inline.image_resized img{max-width:100%}[dir=ltr] .ck.ck-button.ck-button_with-text.ck-resize-image-button .ck-button__icon{margin-right:var(--ck-spacing-standard)}[dir=rtl] .ck.ck-button.ck-button_with-text.ck-resize-image-button .ck-button__icon{margin-left:var(--ck-spacing-standard)}.ck.ck-dropdown .ck-button.ck-resize-image-button .ck-button__label{width:4em}.ck-content code{background-color:hsla(0,0%,78%,.3);padding:.15em;border-radius:2px}.ck.ck-editor__editable .ck-code_selected{background-color:hsla(0,0%,78%,.5)}.ck-content pre{padding:1em;color:#353535;background:hsla(0,0%,78%,.3);border:1px solid #c4c4c4;border-radius:2px;text-align:left;direction:ltr;tab-size:4;white-space:pre-wrap;font-style:normal;min-width:200px}.ck-content pre code{background:unset;padding:0;border-radius:0}.ck.ck-editor__editable pre{position:relative}.ck.ck-editor__editable pre[data-language]:after{content:attr(data-language);position:absolute}:root{--ck-color-code-block-label-background:#757575}.ck.ck-editor__editable pre[data-language]:after{top:-1px;right:10px;background:var(--ck-color-code-block-label-background);font-size:10px;font-family:var(--ck-font-face);line-height:16px;padding:var(--ck-spacing-tiny) var(--ck-spacing-medium);color:#fff;white-space:nowrap}.ck.ck-code-block-dropdown .ck-dropdown__panel{max-height:250px;overflow-y:auto;overflow-x:hidden}.ck .ck-link_selected{background:var(--ck-color-link-selected-background)}.ck .ck-link_selected span.image-inline{outline:var(--ck-widget-outline-thickness) solid var(--ck-color-link-selected-background)}.ck .ck-fake-link-selection{background:var(--ck-color-link-fake-selection)}.ck .ck-fake-link-selection_collapsed{height:100%;border-right:1px solid var(--ck-color-base-text);margin-right:-1px;outline:1px solid hsla(0,0%,100%,.5)}.ck.ck-link-form{display:flex}.ck.ck-link-form .ck-label{display:none}@media screen and (max-width:600px){.ck.ck-link-form{flex-wrap:wrap}.ck.ck-link-form .ck-labeled-field-view{flex-basis:100%}.ck.ck-link-form .ck-button{flex-basis:50%}}.ck.ck-link-form_layout-vertical{display:block}.ck.ck-link-form_layout-vertical .ck-button.ck-button-cancel,.ck.ck-link-form_layout-vertical .ck-button.ck-button-save{margin-top:var(--ck-spacing-medium)}.ck.ck-link-form_layout-vertical{padding:0;min-width:var(--ck-input-text-width)}.ck.ck-link-form_layout-vertical .ck-labeled-field-view{margin:var(--ck-spacing-large) var(--ck-spacing-large) var(--ck-spacing-small)}.ck.ck-link-form_layout-vertical .ck-labeled-field-view .ck-input-text{min-width:0;width:100%}.ck.ck-link-form_layout-vertical .ck-button{padding:var(--ck-spacing-standard);margin:0;border-radius:0;border:0;border-top:1px solid var(--ck-color-base-border);width:50%}[dir=ltr] .ck.ck-link-form_layout-vertical .ck-button,[dir=rtl] .ck.ck-link-form_layout-vertical .ck-button{margin-left:0}[dir=rtl] .ck.ck-link-form_layout-vertical .ck-button:last-of-type{border-right:1px solid var(--ck-color-base-border)}.ck.ck-link-form_layout-vertical .ck.ck-list{margin:var(--ck-spacing-standard) var(--ck-spacing-large)}.ck.ck-link-form_layout-vertical .ck.ck-list .ck-button.ck-switchbutton{border:0;padding:0;width:100%}.ck.ck-link-form_layout-vertical .ck.ck-list .ck-button.ck-switchbutton:hover{background:none}.ck.ck-link-actions{display:flex;flex-direction:row;flex-wrap:nowrap}.ck.ck-link-actions .ck-link-actions__preview{display:inline-block}.ck.ck-link-actions .ck-link-actions__preview .ck-button__label{overflow:hidden}@media screen and (max-width:600px){.ck.ck-link-actions{flex-wrap:wrap}.ck.ck-link-actions .ck-link-actions__preview{flex-basis:100%}.ck.ck-link-actions .ck-button:not(.ck-link-actions__preview){flex-basis:50%}}.ck.ck-link-actions .ck-button.ck-link-actions__preview{padding-left:0;padding-right:0}.ck.ck-link-actions .ck-button.ck-link-actions__preview .ck-button__label{padding:0 var(--ck-spacing-medium);color:var(--ck-color-link-default);text-overflow:ellipsis;cursor:pointer;max-width:var(--ck-input-text-width);min-width:3em;text-align:center}.ck.ck-link-actions .ck-button.ck-link-actions__preview .ck-button__label:hover{text-decoration:underline}.ck.ck-link-actions .ck-button.ck-link-actions__preview,.ck.ck-link-actions .ck-button.ck-link-actions__preview:active,.ck.ck-link-actions .ck-button.ck-link-actions__preview:focus,.ck.ck-link-actions .ck-button.ck-link-actions__preview:hover{background:none}.ck.ck-link-actions .ck-button.ck-link-actions__preview:active{box-shadow:none}.ck.ck-link-actions .ck-button.ck-link-actions__preview:focus .ck-button__label{text-decoration:underline}[dir=ltr] .ck.ck-link-actions .ck-button:not(:first-child),[dir=rtl] .ck.ck-link-actions .ck-button:not(:last-child){margin-left:var(--ck-spacing-standard)}@media screen and (max-width:600px){.ck.ck-link-actions .ck-button.ck-link-actions__preview{margin:var(--ck-spacing-standard) var(--ck-spacing-standard) 0}.ck.ck-link-actions .ck-button.ck-link-actions__preview .ck-button__label{min-width:0;max-width:100%}[dir=ltr] .ck.ck-link-actions .ck-button:not(.ck-link-actions__preview),[dir=rtl] .ck.ck-link-actions .ck-button:not(.ck-link-actions__preview){margin-left:0}}.ck.ck-list-styles-dropdown>.ck-dropdown__panel>.ck-toolbar>.ck-toolbar__items{display:grid}:root{--ck-list-style-button-size:44px}.ck.ck-list-styles-dropdown>.ck-dropdown__panel>.ck-toolbar{background:none;padding:0}.ck.ck-list-styles-dropdown>.ck-dropdown__panel>.ck-toolbar>.ck-toolbar__items{grid-template-columns:repeat(3,auto);row-gap:var(--ck-spacing-medium);column-gap:var(--ck-spacing-medium);padding:var(--ck-spacing-medium)}.ck.ck-list-styles-dropdown>.ck-dropdown__panel>.ck-toolbar>.ck-toolbar__items .ck-button{width:var(--ck-list-style-button-size);height:var(--ck-list-style-button-size);padding:0;margin:0;box-sizing:content-box}.ck.ck-list-styles-dropdown>.ck-dropdown__panel>.ck-toolbar>.ck-toolbar__items .ck-button .ck-icon{width:var(--ck-list-style-button-size);height:var(--ck-list-style-button-size)}.ck-media__wrapper .ck-media__placeholder{display:flex;flex-direction:column;align-items:center}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url .ck-tooltip{display:block}@media (hover:none){.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url .ck-tooltip{display:none}}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url{max-width:100%;position:relative}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url:hover .ck-tooltip{visibility:visible;opacity:1}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url .ck-media__placeholder__url__text{overflow:hidden;display:block}.ck-media__wrapper[data-oembed-url*="facebook.com"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="goo.gl/maps"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="google.com/maps"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="instagram.com"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="maps.app.goo.gl"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="maps.google.com"] .ck-media__placeholder__icon *,.ck-media__wrapper[data-oembed-url*="twitter.com"] .ck-media__placeholder__icon *{display:none}.ck-editor__editable:not(.ck-read-only) .ck-media__wrapper>:not(.ck-media__placeholder),.ck-editor__editable:not(.ck-read-only) .ck-widget:not(.ck-widget_selected) .ck-media__placeholder{pointer-events:none}:root{--ck-media-embed-placeholder-icon-size:3em;--ck-color-media-embed-placeholder-url-text:#757575;--ck-color-media-embed-placeholder-url-text-hover:var(--ck-color-base-text)}.ck-media__wrapper{margin:0 auto}.ck-media__wrapper .ck-media__placeholder{padding:calc(var(--ck-spacing-standard)*3);background:var(--ck-color-base-foreground)}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__icon{min-width:var(--ck-media-embed-placeholder-icon-size);height:var(--ck-media-embed-placeholder-icon-size);margin-bottom:var(--ck-spacing-large);background-position:50%;background-size:cover}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__icon .ck-icon{width:100%;height:100%}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url__text{color:var(--ck-color-media-embed-placeholder-url-text);white-space:nowrap;text-align:center;font-style:italic;text-overflow:ellipsis}.ck-media__wrapper .ck-media__placeholder .ck-media__placeholder__url__text:hover{color:var(--ck-color-media-embed-placeholder-url-text-hover);cursor:pointer;text-decoration:underline}.ck-media__wrapper[data-oembed-url*="open.spotify.com"]{max-width:300px;max-height:380px}.ck-media__wrapper[data-oembed-url*="goo.gl/maps"] .ck-media__placeholder__icon,.ck-media__wrapper[data-oembed-url*="google.com/maps"] .ck-media__placeholder__icon,.ck-media__wrapper[data-oembed-url*="maps.app.goo.gl"] .ck-media__placeholder__icon,.ck-media__wrapper[data-oembed-url*="maps.google.com"] .ck-media__placeholder__icon{background-image:url(data:image/svg+xml;base64,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)}.ck-media__wrapper[data-oembed-url*="facebook.com"] .ck-media__placeholder{background:#4268b3}.ck-media__wrapper[data-oembed-url*="facebook.com"] .ck-media__placeholder .ck-media__placeholder__icon{background-image:url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTAyNCIgaGVpZ2h0PSIxMDI0IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Ik05NjcuNDg0IDBINTYuNTE3QzI1LjMwNCAwIDAgMjUuMzA0IDAgNTYuNTE3djkxMC45NjZDMCA5OTguNjk0IDI1LjI5NyAxMDI0IDU2LjUyMiAxMDI0SDU0N1Y2MjhINDE0VjQ3M2gxMzNWMzU5LjAyOWMwLTEzMi4yNjIgODAuNzczLTIwNC4yODIgMTk4Ljc1Ni0yMDQuMjgyIDU2LjUxMyAwIDEwNS4wODYgNC4yMDggMTE5LjI0NCA2LjA4OVYyOTlsLTgxLjYxNi4wMzdjLTYzLjk5MyAwLTc2LjM4NCAzMC40OTItNzYuMzg0IDc1LjIzNlY0NzNoMTUzLjQ4N2wtMTkuOTg2IDE1NUg3MDd2Mzk2aDI2MC40ODRjMzEuMjEzIDAgNTYuNTE2LTI1LjMwMyA1Ni41MTYtNTYuNTE2VjU2LjUxNUMxMDI0IDI1LjMwMyA5OTguNjk3IDAgOTY3LjQ4NCAwIiBmaWxsPSIjRkZGRkZFIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiLz48L3N2Zz4=)}.ck-media__wrapper[data-oembed-url*="facebook.com"] .ck-media__placeholder .ck-media__placeholder__url__text{color:#cdf}.ck-media__wrapper[data-oembed-url*="facebook.com"] .ck-media__placeholder .ck-media__placeholder__url__text:hover{color:#fff}.ck-media__wrapper[data-oembed-url*="instagram.com"] .ck-media__placeholder{background:linear-gradient(-135deg,#1400c7,#b800b1,#f50000)}.ck-media__wrapper[data-oembed-url*="instagram.com"] .ck-media__placeholder .ck-media__placeholder__icon{background-image:url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNTA0IiBoZWlnaHQ9IjUwNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+PGRlZnM+PHBhdGggaWQ9ImEiIGQ9Ik0wIC4xNTloNTAzLjg0MVY1MDMuOTRIMHoiLz48L2RlZnM+PGcgZmlsbD0ibm9uZSIgZmlsbC1ydWxlPSJldmVub2RkIj48bWFzayBpZD0iYiIgZmlsbD0iI2ZmZiI+PHVzZSB4bGluazpocmVmPSIjYSIvPjwvbWFzaz48cGF0aCBkPSJNMjUxLjkyMS4xNTljLTY4LjQxOCAwLTc2Ljk5Ny4yOS0xMDMuODY3IDEuNTE2LTI2LjgxNCAxLjIyMy00NS4xMjcgNS40ODItNjEuMTUxIDExLjcxLTE2LjU2NiA2LjQzNy0zMC42MTUgMTUuMDUxLTQ0LjYyMSAyOS4wNTYtMTQuMDA1IDE0LjAwNi0yMi42MTkgMjguMDU1LTI5LjA1NiA0NC42MjEtNi4yMjggMTYuMDI0LTEwLjQ4NyAzNC4zMzctMTEuNzEgNjEuMTUxQy4yOSAxNzUuMDgzIDAgMTgzLjY2MiAwIDI1Mi4wOGMwIDY4LjQxNy4yOSA3Ni45OTYgMS41MTYgMTAzLjg2NiAxLjIyMyAyNi44MTQgNS40ODIgNDUuMTI3IDExLjcxIDYxLjE1MSA2LjQzNyAxNi41NjYgMTUuMDUxIDMwLjYxNSAyOS4wNTYgNDQuNjIxIDE0LjAwNiAxNC4wMDUgMjguMDU1IDIyLjYxOSA0NC42MjEgMjkuMDU3IDE2LjAyNCA2LjIyNyAzNC4zMzcgMTAuNDg2IDYxLjE1MSAxMS43MDkgMjYuODcgMS4yMjYgMzUuNDQ5IDEuNTE2IDEwMy44NjcgMS41MTYgNjguNDE3IDAgNzYuOTk2LS4yOSAxMDMuODY2LTEuNTE2IDI2LjgxNC0xLjIyMyA0NS4xMjctNS40ODIgNjEuMTUxLTExLjcwOSAxNi41NjYtNi40MzggMzAuNjE1LTE1LjA1MiA0NC42MjEtMjkuMDU3IDE0LjAwNS0xNC4wMDYgMjIuNjE5LTI4LjA1NSAyOS4wNTctNDQuNjIxIDYuMjI3LTE2LjAyNCAxMC40ODYtMzQuMzM3IDExLjcwOS02MS4xNTEgMS4yMjYtMjYuODcgMS41MTYtMzUuNDQ5IDEuNTE2LTEwMy44NjYgMC02OC40MTgtLjI5LTc2Ljk5Ny0xLjUxNi0xMDMuODY3LTEuMjIzLTI2LjgxNC01LjQ4Mi00NS4xMjctMTEuNzA5LTYxLjE1MS02LjQzOC0xNi41NjYtMTUuMDUyLTMwLjYxNS0yOS4wNTctNDQuNjIxLTE0LjAwNi0xNC4wMDUtMjguMDU1LTIyLjYxOS00NC42MjEtMjkuMDU2LTE2LjAyNC02LjIyOC0zNC4zMzctMTAuNDg3LTYxLjE1MS0xMS43MUMzMjguOTE3LjQ0OSAzMjAuMzM4LjE1OSAyNTEuOTIxLjE1OXptMCA0NS4zOTFjNjcuMjY1IDAgNzUuMjMzLjI1NyAxMDEuNzk3IDEuNDY5IDI0LjU2MiAxLjEyIDM3LjkwMSA1LjIyNCA0Ni43NzggOC42NzQgMTEuNzU5IDQuNTcgMjAuMTUxIDEwLjAyOSAyOC45NjYgMTguODQ1IDguODE2IDguODE1IDE0LjI3NSAxNy4yMDcgMTguODQ1IDI4Ljk2NiAzLjQ1IDguODc3IDcuNTU0IDIyLjIxNiA4LjY3NCA0Ni43NzggMS4yMTIgMjYuNTY0IDEuNDY5IDM0LjUzMiAxLjQ2OSAxMDEuNzk4IDAgNjcuMjY1LS4yNTcgNzUuMjMzLTEuNDY5IDEwMS43OTctMS4xMiAyNC41NjItNS4yMjQgMzcuOTAxLTguNjc0IDQ2Ljc3OC00LjU3IDExLjc1OS0xMC4wMjkgMjAuMTUxLTE4Ljg0NSAyOC45NjYtOC44MTUgOC44MTYtMTcuMjA3IDE0LjI3NS0yOC45NjYgMTguODQ1LTguODc3IDMuNDUtMjIuMjE2IDcuNTU0LTQ2Ljc3OCA4LjY3NC0yNi41NiAxLjIxMi0zNC41MjcgMS40NjktMTAxLjc5NyAxLjQ2OS02Ny4yNzEgMC03NS4yMzctLjI1Ny0xMDEuNzk4LTEuNDY5LTI0LjU2Mi0xLjEyLTM3LjkwMS01LjIyNC00Ni43NzgtOC42NzQtMTEuNzU5LTQuNTctMjAuMTUxLTEwLjAyOS0yOC45NjYtMTguODQ1LTguODE1LTguODE1LTE0LjI3NS0xNy4yMDctMTguODQ1LTI4Ljk2Ni0zLjQ1LTguODc3LTcuNTU0LTIyLjIxNi04LjY3NC00Ni43NzgtMS4yMTItMjYuNTY0LTEuNDY5LTM0LjUzMi0xLjQ2OS0xMDEuNzk3IDAtNjcuMjY2LjI1Ny03NS4yMzQgMS40NjktMTAxLjc5OCAxLjEyLTI0LjU2MiA1LjIyNC0zNy45MDEgOC42NzQtNDYuNzc4IDQuNTctMTEuNzU5IDEwLjAyOS0yMC4xNTEgMTguODQ1LTI4Ljk2NiA4LjgxNS04LjgxNiAxNy4yMDctMTQuMjc1IDI4Ljk2Ni0xOC44NDUgOC44NzctMy40NSAyMi4yMTYtNy41NTQgNDYuNzc4LTguNjc0IDI2LjU2NC0xLjIxMiAzNC41MzItMS40NjkgMTAxLjc5OC0xLjQ2OXoiIGZpbGw9IiNGRkYiIG1hc2s9InVybCgjYikiLz48cGF0aCBkPSJNMjUxLjkyMSAzMzYuMDUzYy00Ni4zNzggMC04My45NzQtMzcuNTk2LTgzLjk3NC04My45NzMgMC00Ni4zNzggMzcuNTk2LTgzLjk3NCA4My45NzQtODMuOTc0IDQ2LjM3NyAwIDgzLjk3MyAzNy41OTYgODMuOTczIDgzLjk3NCAwIDQ2LjM3Ny0zNy41OTYgODMuOTczLTgzLjk3MyA4My45NzN6bTAtMjEzLjMzOGMtNzEuNDQ3IDAtMTI5LjM2NSA1Ny45MTgtMTI5LjM2NSAxMjkuMzY1IDAgNzEuNDQ2IDU3LjkxOCAxMjkuMzY0IDEyOS4zNjUgMTI5LjM2NCA3MS40NDYgMCAxMjkuMzY0LTU3LjkxOCAxMjkuMzY0LTEyOS4zNjQgMC03MS40NDctNTcuOTE4LTEyOS4zNjUtMTI5LjM2NC0xMjkuMzY1ek00MTYuNjI3IDExNy42MDRjMCAxNi42OTYtMTMuNTM1IDMwLjIzLTMwLjIzMSAzMC4yMy0xNi42OTUgMC0zMC4yMy0xMy41MzQtMzAuMjMtMzAuMjMgMC0xNi42OTYgMTMuNTM1LTMwLjIzMSAzMC4yMy0zMC4yMzEgMTYuNjk2IDAgMzAuMjMxIDEzLjUzNSAzMC4yMzEgMzAuMjMxIiBmaWxsPSIjRkZGIi8+PC9nPjwvc3ZnPg==)}.ck-media__wrapper[data-oembed-url*="instagram.com"] .ck-media__placeholder .ck-media__placeholder__url__text{color:#ffe0fe}.ck-media__wrapper[data-oembed-url*="instagram.com"] .ck-media__placeholder .ck-media__placeholder__url__text:hover{color:#fff}.ck-media__wrapper[data-oembed-url*="twitter.com"] .ck.ck-media__placeholder{background:linear-gradient(90deg,#71c6f4,#0d70a5)}.ck-media__wrapper[data-oembed-url*="twitter.com"] .ck.ck-media__placeholder .ck-media__placeholder__icon{background-image:url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0MDAgNDAwIj48cGF0aCBkPSJNNDAwIDIwMGMwIDExMC41LTg5LjUgMjAwLTIwMCAyMDBTMCAzMTAuNSAwIDIwMCA4OS41IDAgMjAwIDBzMjAwIDg5LjUgMjAwIDIwMHpNMTYzLjQgMzA1LjVjODguNyAwIDEzNy4yLTczLjUgMTM3LjItMTM3LjIgMC0yLjEgMC00LjItLjEtNi4yIDkuNC02LjggMTcuNi0xNS4zIDI0LjEtMjUtOC42IDMuOC0xNy45IDYuNC0yNy43IDcuNiAxMC02IDE3LjYtMTUuNCAyMS4yLTI2LjctOS4zIDUuNS0xOS42IDkuNS0zMC42IDExLjctOC44LTkuNC0yMS4zLTE1LjItMzUuMi0xNS4yLTI2LjYgMC00OC4yIDIxLjYtNDguMiA0OC4yIDAgMy44LjQgNy41IDEuMyAxMS00MC4xLTItNzUuNi0yMS4yLTk5LjQtNTAuNC00LjEgNy4xLTYuNSAxNS40LTYuNSAyNC4yIDAgMTYuNyA4LjUgMzEuNSAyMS41IDQwLjEtNy45LS4yLTE1LjMtMi40LTIxLjgtNnYuNmMwIDIzLjQgMTYuNiA0Mi44IDM4LjcgNDcuMy00IDEuMS04LjMgMS43LTEyLjcgMS43LTMuMSAwLTYuMS0uMy05LjEtLjkgNi4xIDE5LjIgMjMuOSAzMy4xIDQ1IDMzLjUtMTYuNSAxMi45LTM3LjMgMjAuNi01OS45IDIwLjYtMy45IDAtNy43LS4yLTExLjUtLjcgMjEuMSAxMy44IDQ2LjUgMjEuOCA3My43IDIxLjgiIGZpbGw9IiNmZmYiLz48L3N2Zz4=)}.ck-media__wrapper[data-oembed-url*="twitter.com"] .ck.ck-media__placeholder .ck-media__placeholder__url__text{color:#b8e6ff}.ck-media__wrapper[data-oembed-url*="twitter.com"] .ck.ck-media__placeholder .ck-media__placeholder__url__text:hover{color:#fff}.ck.ck-media-form{display:flex;align-items:flex-start;flex-direction:row;flex-wrap:nowrap}.ck.ck-media-form .ck-labeled-field-view{display:inline-block}.ck.ck-media-form .ck-label{display:none}@media screen and (max-width:600px){.ck.ck-media-form{flex-wrap:wrap}.ck.ck-media-form .ck-labeled-field-view{flex-basis:100%}.ck.ck-media-form .ck-button{flex-basis:50%}}.ck-content .media{clear:both;margin:.9em 0;display:block;min-width:15em}:root{--ck-mention-list-max-height:300px}.ck.ck-mentions{max-height:var(--ck-mention-list-max-height);overflow-y:auto;overflow-x:hidden;overscroll-behavior:contain}.ck.ck-mentions>.ck-list__item{overflow:hidden;flex-shrink:0}:root{--ck-user-colors--0:#6d78e8;--ck-user-colors--0-alpha:rgba(109,120,232,0.15);--ck-user-colors--1:#007a6c;--ck-user-colors--1-alpha:rgba(0,122,108,0.15);--ck-user-colors--2:#ba4545;--ck-user-colors--2-alpha:rgba(186,69,69,0.15);--ck-user-colors--3:#5635b1;--ck-user-colors--3-alpha:rgba(86,53,177,0.15);--ck-user-colors--4:#548a2e;--ck-user-colors--4-alpha:rgba(84,138,46,0.15);--ck-user-colors--5:#c3185d;--ck-user-colors--5-alpha:rgba(195,24,93,0.15);--ck-user-colors--6:#ea4343;--ck-user-colors--6-alpha:rgba(234,67,67,0.15);--ck-user-colors--7:#0bc3d0;--ck-user-colors--7-alpha:rgba(11,195,208,0.15);--ck-user-dot-size:6px}.ck .ck-user__color--0{color:var(--ck-user-colors--0)}.ck .ck-user__bg-color--0{background-color:var(--ck-user-colors--0)}.ck .ck-user__selection--0{background-color:var(--ck-user-colors--0-alpha)}.ck .ck-user__selection--0.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--0)}.ck .ck-user__marker--0,.ck .ck-user__marker--0 .ck-user__marker-dot,.ck .ck-user__marker--0:after{background-color:var(--ck-user-colors--0)}.ck .ck-user__marker--0 .ck-user__marker-line{border:1px solid var(--ck-user-colors--0)}.ck .ck-user__color--1{color:var(--ck-user-colors--1)}.ck .ck-user__bg-color--1{background-color:var(--ck-user-colors--1)}.ck .ck-user__selection--1{background-color:var(--ck-user-colors--1-alpha)}.ck .ck-user__selection--1.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--1)}.ck .ck-user__marker--1,.ck .ck-user__marker--1 .ck-user__marker-dot,.ck .ck-user__marker--1:after{background-color:var(--ck-user-colors--1)}.ck .ck-user__marker--1 .ck-user__marker-line{border:1px solid var(--ck-user-colors--1)}.ck .ck-user__color--2{color:var(--ck-user-colors--2)}.ck .ck-user__bg-color--2{background-color:var(--ck-user-colors--2)}.ck .ck-user__selection--2{background-color:var(--ck-user-colors--2-alpha)}.ck .ck-user__selection--2.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--2)}.ck .ck-user__marker--2,.ck .ck-user__marker--2 .ck-user__marker-dot,.ck .ck-user__marker--2:after{background-color:var(--ck-user-colors--2)}.ck .ck-user__marker--2 .ck-user__marker-line{border:1px solid var(--ck-user-colors--2)}.ck .ck-user__color--3{color:var(--ck-user-colors--3)}.ck .ck-user__bg-color--3{background-color:var(--ck-user-colors--3)}.ck .ck-user__selection--3{background-color:var(--ck-user-colors--3-alpha)}.ck .ck-user__selection--3.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--3)}.ck .ck-user__marker--3,.ck .ck-user__marker--3 .ck-user__marker-dot,.ck .ck-user__marker--3:after{background-color:var(--ck-user-colors--3)}.ck .ck-user__marker--3 .ck-user__marker-line{border:1px solid var(--ck-user-colors--3)}.ck .ck-user__color--4{color:var(--ck-user-colors--4)}.ck .ck-user__bg-color--4{background-color:var(--ck-user-colors--4)}.ck .ck-user__selection--4{background-color:var(--ck-user-colors--4-alpha)}.ck .ck-user__selection--4.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--4)}.ck .ck-user__marker--4,.ck .ck-user__marker--4 .ck-user__marker-dot,.ck .ck-user__marker--4:after{background-color:var(--ck-user-colors--4)}.ck .ck-user__marker--4 .ck-user__marker-line{border:1px solid var(--ck-user-colors--4)}.ck .ck-user__color--5{color:var(--ck-user-colors--5)}.ck .ck-user__bg-color--5{background-color:var(--ck-user-colors--5)}.ck .ck-user__selection--5{background-color:var(--ck-user-colors--5-alpha)}.ck .ck-user__selection--5.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--5)}.ck .ck-user__marker--5,.ck .ck-user__marker--5 .ck-user__marker-dot,.ck .ck-user__marker--5:after{background-color:var(--ck-user-colors--5)}.ck .ck-user__marker--5 .ck-user__marker-line{border:1px solid var(--ck-user-colors--5)}.ck .ck-user__color--6{color:var(--ck-user-colors--6)}.ck .ck-user__bg-color--6{background-color:var(--ck-user-colors--6)}.ck .ck-user__selection--6{background-color:var(--ck-user-colors--6-alpha)}.ck .ck-user__selection--6.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--6)}.ck .ck-user__marker--6,.ck .ck-user__marker--6 .ck-user__marker-dot,.ck .ck-user__marker--6:after{background-color:var(--ck-user-colors--6)}.ck .ck-user__marker--6 .ck-user__marker-line{border:1px solid var(--ck-user-colors--6)}.ck .ck-user__color--7{color:var(--ck-user-colors--7)}.ck .ck-user__bg-color--7{background-color:var(--ck-user-colors--7)}.ck .ck-user__selection--7{background-color:var(--ck-user-colors--7-alpha)}.ck .ck-user__selection--7.ck-widget{background-color:transparent;border:3px solid var(--ck-user-colors--7)}.ck .ck-user__marker--7,.ck .ck-user__marker--7 .ck-user__marker-dot,.ck .ck-user__marker--7:after{background-color:var(--ck-user-colors--7)}.ck .ck-user__marker--7 .ck-user__marker-line{border:1px solid var(--ck-user-colors--7)}.ck .ck-user__marker{display:inline;position:relative;cursor:default}.ck .ck-user__marker:after{content:attr(data-name);display:none;position:absolute;font-family:Georgia,sans-serif;font-size:var(--ck-font-size-base);line-height:1;font-style:normal;font-weight:400;left:0;opacity:0;top:-20px;padding:var(--ck-spacing-small);color:#fff;white-space:nowrap;transition:opacity .2s linear,transform .3s ease-in-out;z-index:99999}.ck .ck-user__marker:hover .ck-user__marker-dot{width:0}.ck .ck-user__marker:hover:after{display:block;opacity:1;animation:fadeInLeft .3s ease-in-out}.ck .ck-user__marker-line{position:absolute;bottom:0;height:100%;width:0}.ck .ck-user__marker-dot{display:block;position:absolute;left:50%;top:calc(var(--ck-user-dot-size)/2*-1);width:var(--ck-user-dot-size);height:var(--ck-user-dot-size);border-radius:50%;transform:translate(-50%,-50%);cursor:default}.ck .ck-placeholder .ck-user__marker{display:none}@keyframes fadeInLeft{0%{opacity:0;transform:translateX(-10px)}to{opacity:1;transform:translateX(0)}}:root{--ck-user-avatar-size:40px;--ck-user-avatar-background:#3670ab;--ck-user-avatar-color:#fff}.ck.ck-user{display:flex;justify-content:center;align-items:center;position:relative;border-radius:50%;overflow:hidden;background-color:var(--ck-user-avatar-background);height:var(--ck-user-avatar-size);width:var(--ck-user-avatar-size);min-width:var(--ck-user-avatar-size);min-height:var(--ck-user-avatar-size)}.ck.ck-user__name{color:var(--ck-user-avatar-color);cursor:default;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.ck.ck-user__name.ck-user__name--hidden{display:none}.ck.ck-user__img{display:none;width:inherit;height:inherit;background-position:50%;background-repeat:no-repeat;background-size:cover;background-color:var(--ck-user-avatar-background);border-radius:50%;border:2px solid var(--ck-user-avatar-background)}.ck.ck-user__img.ck-user__anonymous{display:block;background-image:url("data:image/svg+xml;charset=utf-8,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 20 20'%3E%3Cpath fill='%23FFF' d='M9.8 20H2c0-5.7 5-5.7 6-7l.2-.8c-1.6-.8-2.7-2.7-2.7-5 0-2.8 2-5.2 4.3-5.2S14 4.4 14 7.3c0 2.2-1 4-2.6 5l.2.6c1.2 1.3 6 1.4 6 7H9.8z'/%3E%3C/svg%3E")}.ck.ck-user__img.ck-user__anonymous+.ck-user__name{display:none}.ck.ck-user__img.ck-user__avatar{display:block;background-color:#fff}.ck.ck-user__img.ck-user__img--hidden{display:none}:root{--ck-color-comment-background:#f3f7fb;--ck-color-comment-separator:#cddeef;--ck-color-comment-remove-background:#fcc;--ck-color-comment-input-background:var(--ck-color-comment-background)}.ck .ck-comment__wrapper{font-size:var(--ck-font-size-base);position:relative;outline:0;transition:all .3s linear}.ck .ck-comment__wrapper.ck-comment--remove-confirmation{transform:translate3d(-100%,0,0)}.ck .ck-comment__wrapper:first-of-type{border-top-right-radius:var(--ck-border-radius);border-top-left-radius:var(--ck-border-radius)}.ck .ck-comment:after{content:"";display:block;position:absolute;opacity:1;top:calc(var(--ck-user-avatar-size) + var(--ck-spacing-small));left:calc(var(--ck-spacing-standard) + var(--ck-user-avatar-size)/2 - 2px);width:4px;height:100%;background-color:var(--ck-color-comment-separator)}.ck .ck-comment--remove{background-color:var(--ck-color-comment-remove-background)}.ck .ck-comment--remove:after{opacity:0}.ck .ck-comment--edit{background-color:var(--ck-color-comment-input-background)}.ck .ck-comment--edit:after{opacity:0}.ck .ck-comment__main .ck-comment__input-wrapper{display:flex;flex-direction:column;position:relative;transition:all .3s linear}.ck .ck-comment__main .ck-comment__input-wrapper .ck-comment__input{padding:0}.ck .ck-comment__content{word-break:break-word}.ck .ck-comment__user{z-index:var(--ck-z-default)}.ck-comment__input-actions .ck.ck-button.ck-comment__input-actions--submit{color:var(--ck-color-button-save)}.ck-comment__input-actions .ck.ck-button.ck-comment__input-actions--cancel{color:var(--ck-color-button-cancel)}.ck .ck-comment__input p,.ck .ck-comment p{margin:0}.ck .ck-comment__input ol,.ck .ck-comment__input ul,.ck .ck-comment ol,.ck .ck-comment ul{padding-left:10px;margin-left:10px}.ck .ck-comment__input ul,.ck .ck-comment ul{list-style:disc;margin-bottom:5px;margin-top:0}.ck .ck-comment__input ul ul,.ck .ck-comment ul ul{list-style:circle}.ck .ck-comment__input ul ul ul,.ck .ck-comment ul ul ul{list-style:square}:root{--ck-color-thread-remove-background:var(--ck-color-comment-remove-background);--ck-color-comment-count:#5891ca}.ck .ck-thread{width:100%;color:var(--ck-color-base-text);outline:0;background-color:var(--ck-color-base-background);overflow:hidden;transition:all .3s ease;border-radius:var(--ck-border-radius)}.ck .ck-thread__comments .ck-comment__wrapper:last-of-type .ck-comment:after{opacity:0}.ck .ck-thread__container{transition:transform .3s ease}.ck .ck-thread__comments{position:relative;z-index:var(--ck-z-default);list-style:none;margin:0;padding:0;border-radius:var(--ck-border-radius);overflow:hidden}.ck .ck-thread__comment-count{--ck-user-avatar-size:28px;padding-left:calc(var(--ck-user-avatar-size) + var(--ck-spacing-standard)*2);color:var(--ck-color-comment-count);font-size:var(--ck-font-size-base);font-weight:700;cursor:pointer;transition:background-color .3s ease}.ck .ck-thread__comment-count:before{content:"\25BC";margin-right:var(--ck-spacing-small)}.ck .ck-thread--active .ck-comment__options.ck-dropdown{display:block}.ck .ck-thread--active .ck-thread__input{display:block;background-color:var(--ck-color-comment-background)}.ck .ck-thread--remove .ck-comment{background-color:var(--ck-color-comment-remove-background)}.ck .ck-thread--remove .ck-thread__input--active{background-color:var(--ck-color-base-background)}.ck .ck-thread--remove .ck-thread__comment-count,.ck .ck-thread--remove .ck-thread__input--active.ck-thread__input{background-color:var(--ck-color-comment-remove-background)}.ck .ck-thread--remove .ck-comment__wrapper .ck-comment:after{opacity:0}.ck .ck-thread--remove-confirmation .ck-thread__container{transform:translate3d(-100%,0,0)}.ck.ck-button.ck-thread__remove-confirm-submit{color:var(--ck-color-button-save)}.ck.ck-button.ck-thread__remove-confirm-cancel{color:var(--ck-color-button-cancel)}.ck .ck-thread__remove-confirm{font-size:var(--ck-font-size-base);position:absolute;display:flex;justify-content:flex-start;top:0;left:100%;width:100%;height:100%;background-color:var(--ck-color-comment-remove-background)}.ck .ck-thread__remove-confirm .ck-thread__remove-confirm-inner{display:flex;flex-direction:column;align-items:center;justify-content:center;padding-top:var(--ck-spacing-standard);height:100%;width:100%;max-height:400px}.ck .ck-thread__remove-confirm p{font-weight:700;margin:0}.ck .ck-thread__input{width:calc(100% - var(--ck-user-avatar-size));transition:background-color .2s ease-in-out;border-radius:var(--ck-border-radius)}.ck .ck-comment__input-container{--ck-user-avatar-size:28px;font-size:var(--ck-font-size-base);display:none;padding:var(--ck-spacing-standard);background:var(--ck-color-comment-background)}.ck .ck-comment__input-container--active{display:flex}.ck .ck-comment__input{padding:var(--ck-spacing-standard);padding-top:var(--ck-spacing-medium);color:var(--ck-color-base-text);border-width:0;resize:none;background-color:transparent;transition:color .3s ease-in-out}.ck .ck-comment__input:focus{outline:none}.ck .ck-comment__input .ck-editor__editable_inline{--ck-focus-ring:none;--ck-inner-shadow:none;padding:0;overflow:visible;border:0;background-color:transparent;word-break:break-word}.ck-comment__input-wrapper .ck.ck-editor__editable_inline :first-child{margin-top:0}.ck-comment__input-wrapper .ck.ck-editor__editable_inline :last-child{margin-bottom:0}.ck .ck-comment__input-actions{width:100%;text-align:right;display:none}.ck .ck-comment__input-actions.ck-comment__input-actions--active{display:block}.ck .ck-comment__input-actions .ck-button{background-color:transparent;min-width:unset;min-height:unset;cursor:pointer;color:var(--ck-color-annotations-icon);border-radius:var(--ck-border-radius);font-size:var(--ck-annotation-button-size)}.ck .ck-comment__input-actions .ck-button.ck-disabled{opacity:.3}.ck .ck-comment__input-textarea{display:block}:root{--ck-color-annotation-icon:grey;--ck-color-annotation-info:#757575;--ck-annotation-button-size:0.85em}.ck .ck-annotation{--ck-user-avatar-size:28px;font-size:var(--ck-font-size-base);display:flex;position:relative;animation:fadeInLeft .3s;transition:background-color .3s ease;padding:var(--ck-spacing-standard)}.ck .ck-annotation .ck-button{background-color:transparent;min-width:unset;min-height:unset;cursor:pointer;color:var(--ck-color-annotation-icon);border-radius:var(--ck-border-radius);font-size:var(--ck-annotation-button-size)}.ck .ck-annotation__main{display:flex;flex-wrap:wrap;justify-content:space-between;width:calc(100% - var(--ck-user-avatar-size));transition:all .2s ease;border-radius:var(--ck-border-radius);padding-left:var(--ck-spacing-standard)}.ck .ck-annotation__main p{margin:0;white-space:normal;font-size:var(--ck-font-size-base);line-height:1.5em}.ck .ck-annotation__user{margin-top:var(--ck-spacing-small)}.ck .ck-annotation__info{display:flex;align-items:center;color:var(--ck-color-annotation-info);min-height:2.4em}.ck .ck-annotation__info-name,.ck .ck-annotation__info-time{font-weight:700;max-width:150px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;color:var(--ck-color-annotation-info);font-size:var(--ck-font-size-small)}.ck .ck-annotation__info-time{margin-left:var(--ck-spacing-large)}.ck .ck-annotation__actions{transition:opacity .2s ease;opacity:.5}.ck .ck-annotation__content-wrapper{width:100%}@keyframes fadeInLeft{0%{transform:translate3d(-5%,0,0);opacity:0}to{transform:translateZ(0);opacity:1}}:root{--ck-color-comment-marker:#fef7a9;--ck-color-comment-marker-active:#fdf05d}.ck-comment-marker{background:var(--ck-color-comment-marker);border-top:3px solid transparent;border-bottom:3px solid transparent}.ck-comment-marker.ck-widget{background-color:transparent;border:3px solid var(--ck-color-comment-marker)}.ck-comment-marker--active{background:var(--ck-color-comment-marker-active)}.ck-comment-marker--active.ck-widget{border-color:var(--ck-color-comment-marker-active)}.ck-comment-marker--active.ck-widget .ck-comment-marker{background-color:var(--ck-color-comment-marker)}.ck-comment-marker--active .ck-comment-marker{background:var(--ck-color-comment-marker-active)}:root{--ck-color-annotation-wrapper-background:#fff;--ck-color-annotation-wrapper-drop-shadow:0 1px 1px 1px #e6e6e6}.ck.ck-sidebar{position:relative;transition:min-height .25s ease;outline:0}.ck-sidebar-item{position:absolute;width:100%;transition:top .3s ease,box-shadow .3s ease}.ck-sidebar-item--no-animation{transition:top 0s ease!important}.ck.ck-annotation-wrapper{outline:0;border-radius:var(--ck-border-radius);background:var(--ck-color-annotation-wrapper-background);box-shadow:var(--ck-color-annotation-wrapper-drop-shadow)}.ck.ck-annotation-wrapper:not(.ck-annotation-wrapper--active):hover{box-shadow:var(--ck-drop-shadow)}.ck.ck-annotation-wrapper--active{box-shadow:var(--ck-drop-shadow-active)}.ck.ck-annotation-wrapper--active .ck-annotation__actions,.ck.ck-annotation-wrapper:hover .ck-annotation__actions{opacity:1}.ck.ck-annotation-wrapper--active .ck-suggestion--accept,.ck.ck-annotation-wrapper:hover .ck-suggestion--accept{color:var(--ck-color-button-save)}.ck.ck-annotation-wrapper--active .ck-suggestion--discard,.ck.ck-annotation-wrapper:hover .ck-suggestion--discard{color:var(--ck-color-button-cancel)}:root{--ck-annotation-counter-icon-size:16px;--ck-annotation-counter-number-size:10px;--ck-color-annotation-counter-comment:#f2de02;--ck-color-annotation-counter-suggestion-insertion:#5ad86b;--ck-color-annotation-counter-suggestion-deletion:#d85a79;--ck-color-annotation-counter-suggestion-format:#5ac1d8}.ck.ck-sidebar--narrow .ck-sidebar-item{display:flex;justify-content:center}.ck.ck-sidebar--narrow .ck-annotation-counter{font-size:var(--ck-annotation-counter-icon-size);min-height:unset;padding:0;cursor:pointer;color:grey}.ck.ck-sidebar--narrow .ck-annotation-counter:focus,.ck.ck-sidebar--narrow .ck-annotation-counter:hover{background:transparent;box-shadow:none;color:#4d4d4d}.ck.ck-sidebar--narrow .ck-annotation-counter__badge{border-radius:50%;position:absolute;width:16px;height:16px;background:#333;font-weight:700;color:#fff;font-size:var(--ck-annotation-counter-number-size);font-family:var(--ck-font-face);right:0;top:-6px;text-shadow:-1px 0 2px #444,1px 0 2px #444;display:flex;justify-content:center;line-height:16px;z-index:1}.ck.ck-sidebar--narrow .ck-annotation-counter__badge--edit-mode{line-height:1}.ck.ck-sidebar--narrow .ck-annotation-counter__badge--suggestion-insertion,.ck.ck-sidebar--narrow .ck-annotation-counter__badge--suggestion-replace{background:var(--ck-color-annotation-counter-suggestion-insertion)}.ck.ck-sidebar--narrow .ck-annotation-counter__badge--suggestion-deletion{background:var(--ck-color-annotation-counter-suggestion-deletion)}.ck.ck-sidebar--narrow .ck-annotation-counter__badge--suggestion-format{background:var(--ck-color-annotation-counter-suggestion-format)}.ck.ck-sidebar--narrow .ck-annotation-counter__badge--comment{background:var(--ck-color-annotation-counter-comment)}:root{--ck-inline-annotation-container-width:300px;--ck-inline-annotation-container-max-height:400px}.ck.ck-balloon-panel .ck-annotation-wrapper{position:static;width:var(--ck-inline-annotation-container-width);max-height:var(--ck-inline-annotation-container-max-height);overflow-y:auto;overscroll-behavior-y:contain;overflow-x:hidden}.ck.ck-balloon-panel .ck-annotation{animation:none}.ck.ck-balloon-panel .ck-annotation-wrapper *{white-space:normal}.ck.ck-balloon-panel .ck-annotation-wrapper strong{font-weight:700}.ck.ck-balloon-panel .ck-annotation-wrapper i{font-style:italic}.ck.ck-balloon-panel button.ck-suggestion--accept .ck-tooltip__text,.ck.ck-balloon-panel button.ck-suggestion--discard .ck-tooltip__text{text-align:center}.ck.ck-balloon-panel button.ck-suggestion--discard .ck-tooltip{left:unset;right:0}.ck.ck-balloon-panel button.ck-suggestion--discard .ck-tooltip__text{left:unset}.ck.ck-balloon-panel button.ck-suggestion--discard .ck-tooltip__text:after{left:calc(100% - 10px)}:root{--ck-color-comment-box-border:#f2de02;--ck-color-suggestion-box-deletion-border:#d85a79;--ck-color-suggestion-box-insertion-border:#5ad86b;--ck-color-suggestion-box-format-border:#5ac1d8}.ck .ck-suggestion{border-top-left-radius:var(--ck-border-radius)}.ck .ck-annotation{border-left:3px solid transparent}.ck .ck-suggestion--disabled-comments .ck-annotation{min-height:80px}.ck .ck-comment,.ck .ck-comment__input-container,.ck .ck-thread__comment-count{border-left:3px solid var(--ck-color-comment-box-border)}.ck .ck-suggestion-insertion .ck-suggestion{border-left-color:var(--ck-color-suggestion-box-insertion-border)}.ck .ck-suggestion-deletion .ck-suggestion{border-left-color:var(--ck-color-suggestion-box-deletion-border)}.ck .ck-suggestion-replace .ck-suggestion{border-left-color:var(--ck-color-suggestion-box-insertion-border)}.ck .ck-suggestion-format .ck-suggestion{border-left-color:var(--ck-color-suggestion-box-format-border)}.ck .ck-suggestion-wrapper{outline:0}.ck .ck-suggestion-type{font-weight:700}.ck .ck-suggestion-color{display:inline-block;width:14px;height:14px;border-radius:14px;vertical-align:text-bottom;margin:0 5px;box-shadow:0 0 0 1px #444}.ck .ck-suggestion-insertion .ck-comment,.ck .ck-suggestion-insertion .ck-comment__input-container,.ck .ck-suggestion-insertion .ck-thread__comment-count,.ck .ck-suggestion-replace .ck-comment,.ck .ck-suggestion-replace .ck-comment__input-container,.ck .ck-suggestion-replace .ck-thread__comment-count{border-left-color:var(--ck-color-suggestion-box-insertion-border)}.ck .ck-suggestion-deletion .ck-comment,.ck .ck-suggestion-deletion .ck-comment__input-container,.ck .ck-suggestion-deletion .ck-thread__comment-count{border-left-color:var(--ck-color-suggestion-box-deletion-border)}.ck .ck-suggestion-format .ck-comment,.ck .ck-suggestion-format .ck-comment__input-container,.ck .ck-suggestion-format .ck-thread__comment-count{border-left-color:var(--ck-color-suggestion-box-format-border)}:root{--ck-color-suggestion-marker-insertion-border:rgba(30,174,49,0.35);--ck-color-suggestion-marker-insertion-border-active:rgba(19,109,31,0.5);--ck-color-suggestion-marker-insertion-background:rgba(102,229,119,0.35);--ck-color-suggestion-marker-insertion-background-active:rgba(37,218,61,0.5);--ck-color-suggestion-marker-deletion-border:rgba(174,30,66,0.35);--ck-color-suggestion-marker-deletion-border-active:rgba(109,19,41,0.5);--ck-color-suggestion-marker-deletion-background:rgba(229,102,134,0.35);--ck-color-suggestion-marker-deletion-background-active:rgba(218,37,82,0.5);--ck-color-suggestion-marker-deletion-stroke:rgba(87,15,33,0.5);--ck-color-suggestion-marker-format-border:rgba(10,160,194,0.4);--ck-color-suggestion-marker-format-border-active:rgba(10,160,194,0.65);--ck-color-suggestion-widget-insertion-background:rgba(102,229,119,0.05);--ck-color-suggestion-widget-insertion-background-active:rgba(37,218,61,0.07);--ck-color-suggestion-widget-deletion-background:rgba(229,102,134,0.05);--ck-color-suggestion-widget-deletion-background-active:rgba(196,33,74,0.07);--ck-color-suggestion-widget-format-background:rgba(10,160,194,0.09);--ck-color-suggestion-widget-format-background-active:rgba(10,160,194,0.16)}.ck-content .ck-suggestion-marker-insertion{border-top:3px solid var(--ck-color-suggestion-marker-insertion-border);border-bottom:3px solid var(--ck-color-suggestion-marker-insertion-border);background:var(--ck-color-suggestion-marker-insertion-background)}.ck-content .ck-suggestion-marker-insertion.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-insertion-border-active);background:var(--ck-color-suggestion-marker-insertion-background-active)}.ck-content .ck-suggestion-marker-insertion.ck-widget{background:none;text-decoration:none;border:3px solid var(--ck-color-suggestion-marker-insertion-border)}.ck-content .ck-suggestion-marker-insertion.ck-widget.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-insertion-border-active)}.ck-content .ck-suggestion-marker-deletion{border-top:3px solid var(--ck-color-suggestion-marker-deletion-border);border-bottom:3px solid var(--ck-color-suggestion-marker-deletion-border);text-decoration:line-through;text-decoration-color:var(--ck-color-suggestion-marker-deletion-stroke);text-decoration-thickness:3px;background:var(--ck-color-suggestion-marker-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-deletion-border-active);background:var(--ck-color-suggestion-marker-deletion-background-active)}.ck-content .ck-suggestion-marker-deletion.ck-widget{background:none;text-decoration:none;border:3px solid var(--ck-color-suggestion-marker-deletion-border)}.ck-content .ck-suggestion-marker-deletion.ck-widget.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-deletion-border-active)}.ck-content .ck-suggestion-marker-merge:after{border-top:3px solid var(--ck-color-suggestion-marker-deletion-border);border-bottom:3px solid var(--ck-color-suggestion-marker-deletion-border);text-decoration:line-through;text-decoration-color:var(--ck-color-suggestion-marker-deletion-stroke);background:var(--ck-color-suggestion-marker-deletion-background);color:var(--ck-color-suggestion-marker-deletion-stroke);content:"¶"}.ck-content .ck-suggestion-marker-merge.ck-suggestion-marker--active:after{border-color:var(--ck-color-suggestion-marker-deletion-border-active);background:var(--ck-color-suggestion-marker-deletion-background-active)}.ck-content .ck-suggestion-marker-split:after{border-top:3px solid var(--ck-color-suggestion-marker-insertion-border);border-bottom:3px solid var(--ck-color-suggestion-marker-insertion-border);background:var(--ck-color-suggestion-marker-insertion-background);color:var(--ck-color-suggestion-marker-insertion-border);content:"¶"}.ck-content .ck-suggestion-marker-split.ck-suggestion-marker--active:after{border-color:var(--ck-color-suggestion-marker-insertion-border-active);background:var(--ck-color-suggestion-marker-insertion-background-active)}.ck-content .ck-suggestion-marker-formatInline{border-top:3px solid var(--ck-color-suggestion-marker-format-border);border-bottom:3px solid var(--ck-color-suggestion-marker-format-border)}.ck-content .ck-suggestion-marker-formatInline.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-format-border-active)}.ck-content .ck-suggestion-marker-formatInline.ck-widget{background:none;border:3px solid var(--ck-color-suggestion-marker-format-border)}.ck-content .ck-suggestion-marker-formatInline.ck-widget.ck-suggestion-marker--active{border-color:var(--ck-color-suggestion-marker-format-border-active)}.ck-content .ck-suggestion-marker-formatBlock{border:0;background:none;box-shadow:-7px 0 0 0 var(--ck-color-base-background),-10px 0 0 0 var(--ck-color-suggestion-marker-format-border)}.ck-content .ck-suggestion-marker-formatBlock.ck-suggestion-marker--active{box-shadow:-5px 0 0 0 var(--ck-color-base-background),-8px 0 0 0 var(--ck-color-suggestion-marker-format-border-active)}.ck-content ol .ck-suggestion-marker-formatBlock,.ck-content ul .ck-suggestion-marker-formatBlock{box-shadow:-2px 0 0 0 var(--ck-color-base-background),-5px 0 0 0 var(--ck-color-suggestion-marker-format-border)}.ck-content ol .ck-suggestion-marker-formatBlock.ck-suggestion-marker--active,.ck-content ul .ck-suggestion-marker-formatBlock.ck-suggestion-marker--active{box-shadow:-2px 0 0 0 var(--ck-color-base-background),-5px 0 0 0 var(--ck-color-suggestion-marker-format-border-active)}.ck-content .ck-suggestion-marker-deletion.ck-widget.ck-horizontal-line{background-color:var(--ck-color-suggestion-widget-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-widget.ck-horizontal-line.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .ck-suggestion-marker-insertion.ck-widget.ck-horizontal-line{background-color:var(--ck-color-suggestion-widget-insertion-background)}.ck-content .ck-suggestion-marker-insertion.ck-widget.ck-horizontal-line.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}.ck-content .ck-suggestion-marker-deletion.ck-widget.image{background-color:var(--ck-color-suggestion-widget-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-widget.image img{opacity:.6}.ck-content .ck-suggestion-marker-deletion.ck-widget.image figcaption{background-color:var(--ck-color-suggestion-widget-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-widget.image.ck-suggestion-marker--active,.ck-content .ck-suggestion-marker-deletion.ck-widget.image.ck-suggestion-marker--active figcaption{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .ck-suggestion-marker-insertion.ck-widget.image,.ck-content .ck-suggestion-marker-insertion.ck-widget.image figcaption{background-color:var(--ck-color-suggestion-widget-insertion-background)}.ck-content .ck-suggestion-marker-insertion.ck-widget.image.ck-suggestion-marker--active,.ck-content .ck-suggestion-marker-insertion.ck-widget.image.ck-suggestion-marker--active figcaption{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}.ck-content .ck-widget.image>figcaption.ck-suggestion-marker-deletion{background-color:var(--ck-color-suggestion-widget-deletion-background);border:none}.ck-content .ck-widget.image>figcaption.ck-suggestion-marker-deletion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .ck-widget.image>figcaption.ck-suggestion-marker-insertion{background-color:var(--ck-color-suggestion-widget-insertion-background);border:none}.ck-content .ck-widget.image>figcaption.ck-suggestion-marker-insertion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}.ck-content .ck-suggestion-marker-deletion.ck-widget.media{background-color:var(--ck-color-suggestion-widget-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-widget.media .ck-media__wrapper{opacity:.6}.ck-content .ck-suggestion-marker-deletion.ck-widget.media.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .ck-suggestion-marker-insertion.ck-widget.media{background-color:var(--ck-color-suggestion-widget-insertion-background)}.ck-content .ck-suggestion-marker-insertion.ck-widget.media.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}.ck-content .ck-suggestion-marker-deletion.ck-widget.page-break{background-color:var(--ck-color-suggestion-widget-deletion-background)}.ck-content .ck-suggestion-marker-deletion.ck-widget.page-break.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .ck-suggestion-marker-insertion.ck-widget.page-break{background-color:var(--ck-color-suggestion-widget-insertion-background)}.ck-content .ck-suggestion-marker-insertion.ck-widget.page-break.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}:root{--ck-color-suggestion-widget-th-insertion-background:rgba(102,229,119,0.12);--ck-color-suggestion-widget-th-insertion-background-active:rgba(37,218,61,0.14);--ck-color-suggestion-widget-th-deletion-background:rgba(229,102,134,0.14);--ck-color-suggestion-widget-th-deletion-background-active:rgba(196,33,74,0.16)}.ck-content figure.table .ck-suggestion-marker-formatBlock{box-shadow:-3px 0 0 0 var(--ck-color-suggestion-marker-format-border)}.ck-content figure.table .ck-suggestion-marker-formatBlock.ck-suggestion-marker--active{box-shadow:-3px 0 0 0 var(--ck-color-suggestion-marker-format-border-active)}.ck-content .ck-suggestion-marker-insertion.table>table{border-color:var(--ck-color-suggestion-marker-insertion-border)}.ck-content .ck-suggestion-marker-insertion.table>table>tbody>tr>th,.ck-content .ck-suggestion-marker-insertion.table>table>thead>tr>th{background-color:var(--ck-color-suggestion-widget-th-insertion-background);border-color:var(--ck-color-suggestion-marker-insertion-border)}.ck-content .ck-suggestion-marker-insertion.table>table>tbody>tr>td{background-color:var(--ck-color-suggestion-widget-insertion-background);border-color:var(--ck-color-suggestion-marker-insertion-border)}.ck-content .ck-suggestion-marker-insertion.ck-suggestion-marker--active.table>table{border-color:var(--ck-color-suggestion-marker-insertion-border-active)}.ck-content .ck-suggestion-marker-insertion.ck-suggestion-marker--active.table>table>tbody>tr>th,.ck-content .ck-suggestion-marker-insertion.ck-suggestion-marker--active.table>table>thead>tr>th{background-color:var(--ck-color-suggestion-widget-th-insertion-background-active);border-color:var(--ck-color-suggestion-marker-insertion-border-active)}.ck-content .ck-suggestion-marker-insertion.ck-suggestion-marker--active.table>table>tbody>tr>td{background-color:var(--ck-color-suggestion-widget-insertion-background-active);border-color:var(--ck-color-suggestion-marker-insertion-border-active)}.ck-content .ck-suggestion-marker-deletion.table>table{border-color:var(--ck-color-suggestion-marker-deletion-border)}.ck-content .ck-suggestion-marker-deletion.table>table>tbody>tr>th,.ck-content .ck-suggestion-marker-deletion.table>table>thead>tr>th{background-color:var(--ck-color-suggestion-widget-th-deletion-background);border-color:var(--ck-color-suggestion-marker-deletion-border);text-decoration:none}.ck-content .ck-suggestion-marker-deletion.table>table>tbody>tr>td{background-color:var(--ck-color-suggestion-widget-deletion-background);border-color:var(--ck-color-suggestion-marker-deletion-border);text-decoration:none}.ck-content .ck-suggestion-marker-deletion.ck-suggestion-marker--active.table>table{border-color:var(--ck-color-suggestion-marker-deletion-border-active)}.ck-content .ck-suggestion-marker-deletion.ck-suggestion-marker--active.table>table>tbody>tr>th,.ck-content .ck-suggestion-marker-deletion.ck-suggestion-marker--active.table>table>thead>tr>th{background-color:var(--ck-color-suggestion-widget-th-deletion-background-active);border-color:var(--ck-color-suggestion-marker-deletion-border-active)}.ck-content .ck-suggestion-marker-deletion.ck-suggestion-marker--active.table>table>tbody>tr>td{background-color:var(--ck-color-suggestion-widget-deletion-background-active);border-color:var(--ck-color-suggestion-marker-deletion-border-active)}.ck-content .table th.ck-suggestion-marker-insertion{background-color:var(--ck-color-suggestion-widget-th-insertion-background)}.ck-content .table th.ck-suggestion-marker-insertion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-th-insertion-background-active)}.ck-content .table th.ck-suggestion-marker-deletion{background-color:var(--ck-color-suggestion-widget-th-deletion-background);text-decoration:none}.ck-content .table th.ck-suggestion-marker-deletion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-th-deletion-background-active)}.ck-content .table td.ck-suggestion-marker-insertion{background-color:var(--ck-color-suggestion-widget-insertion-background)}.ck-content .table td.ck-suggestion-marker-insertion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-insertion-background-active)}.ck-content .table td.ck-suggestion-marker-deletion{background-color:var(--ck-color-suggestion-widget-deletion-background);text-decoration:none}.ck-content .table td.ck-suggestion-marker-deletion.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-deletion-background-active)}.ck-content .table td.ck-suggestion-marker-formatBlock,.ck-content .table td.ck-suggestion-marker-formatInline,.ck-content .table th.ck-suggestion-marker-formatBlock,.ck-content .table th.ck-suggestion-marker-formatInline{background-color:var(--ck-color-suggestion-widget-format-background);box-shadow:none}.ck-content .table td.ck-suggestion-marker-formatBlock.ck-suggestion-marker--active,.ck-content .table td.ck-suggestion-marker-formatInline.ck-suggestion-marker--active,.ck-content .table th.ck-suggestion-marker-formatBlock.ck-suggestion-marker--active,.ck-content .table th.ck-suggestion-marker-formatInline.ck-suggestion-marker--active{background-color:var(--ck-color-suggestion-widget-format-background-active);box-shadow:none}:root{--ck-table-selected-cell-background:rgba(158,207,250,0.3)}.ck.ck-editor__editable .table table td.ck-editor__editable_selected,.ck.ck-editor__editable .table table th.ck-editor__editable_selected{position:relative;caret-color:transparent;outline:unset;box-shadow:unset}.ck.ck-editor__editable .table table td.ck-editor__editable_selected:after,.ck.ck-editor__editable .table table th.ck-editor__editable_selected:after{content:"";pointer-events:none;background-color:var(--ck-table-selected-cell-background);position:absolute;top:0;left:0;right:0;bottom:0}.ck.ck-editor__editable .table table td.ck-editor__editable_selected ::selection,.ck.ck-editor__editable .table table td.ck-editor__editable_selected:focus,.ck.ck-editor__editable .table table th.ck-editor__editable_selected ::selection,.ck.ck-editor__editable .table table th.ck-editor__editable_selected:focus{background-color:transparent}.ck.ck-editor__editable .table table td.ck-editor__editable_selected .ck-widget,.ck.ck-editor__editable .table table th.ck-editor__editable_selected .ck-widget{outline:unset}.ck.ck-editor__editable .table table td.ck-editor__editable_selected .ck-widget>.ck-widget__selection-handle,.ck.ck-editor__editable .table table th.ck-editor__editable_selected .ck-widget>.ck-widget__selection-handle{display:none}.ck.ck-tooltip,.ck.ck-tooltip .ck-tooltip__text:after{position:absolute;pointer-events:none;-webkit-backface-visibility:hidden}.ck.ck-tooltip{visibility:hidden;opacity:0;display:none;z-index:var(--ck-z-modal)}.ck.ck-tooltip .ck-tooltip__text{display:inline-block}.ck.ck-tooltip .ck-tooltip__text:after{content:"";width:0;height:0}:root{--ck-color-presence-list-dropdown-background:#fff;--ck-color-presence-list-dropdown-arrow-border:#ebebeb;--ck-presence-list-dropdown-list-max-width:250px;--ck-presence-list-dropdown-list-min-width:180px;--ck-presence-list-users-tooltip-max-width:120px;--ck-presence-list-users-tooltip-min-width:25px}.ck.ck-presence-list{display:flex;align-items:center;margin-right:var(--ck-spacing-standard);transition:margin-right .2s ease-in-out;font-size:var(--ck-font-size-base)}.ck.ck-presence-list *{box-sizing:border-box}.ck.ck-presence-list__list{display:flex;justify-content:flex-end}.ck.ck-presence-list__counter{color:var(--ck-color-base-text)}.ck.ck-presence-list__counter--hidden{display:none}.ck.ck-presence-list__list-item{font:inherit;color:inherit;border:0;padding:0;outline:none;background:none;position:relative;margin-left:var(--ck-spacing-standard)}.ck.ck-presence-list__list-item:hover .ck-tooltip{visibility:visible;opacity:1}.ck.ck-presence-list__list-item .ck.ck-tooltip{display:block}.ck.ck-presence-list__list-item .ck-tooltip__text span{display:block;max-width:var(--ck-presence-list-users-tooltip-max-width);overflow:hidden;text-align:center;text-overflow:ellipsis;white-space:nowrap}.ck.ck-presence-list__marker{position:relative;display:block;width:100%;height:3px;margin-top:3px}.ck.ck-presence-list--collapsed .ck-presence-list__list{min-width:calc(var(--ck-user-avatar-size)*1.5);margin-left:var(--ck-spacing-standard)}.ck.ck-presence-list--collapsed .ck-presence-list__list *{cursor:pointer;transition:all .4s ease-in-out}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item{margin-left:calc(var(--ck-user-avatar-size)*-1 + 5px)}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item .ck-tooltip{display:none}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item:nth-last-child(5){opacity:.15}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item:nth-last-child(4){opacity:.35}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item:nth-last-child(3){opacity:.55}.ck.ck-presence-list--collapsed .ck.ck-presence-list__list-item:nth-last-child(2){opacity:.75}.ck.ck-presence-list__balloon{--ck-color-panel-border:var(--ck-color-presence-list-dropdown-arrow-border);--ck-color-panel-background:var(--ck-color-presence-list-dropdown-background);background-color:var(--ck-color-presence-list-dropdown-background);border:0;opacity:1;animation:fadeIn .3s ease-in-out}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list-wrapper{max-height:80vh;min-width:var(--ck-presence-list-dropdown-list-min-width);max-width:var(--ck-presence-list-dropdown-list-max-width);overflow-y:auto;overflow-x:hidden;border-radius:var(--ck-border-radius)}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list-item{font:inherit;color:inherit;border:0;padding:0;outline:none;background:none;display:flex;align-items:center;position:relative;padding:var(--ck-spacing-standard)}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list-item:first-of-type .ck-presence-list__marker{border-top-left-radius:var(--ck-border-radius)}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list-item:last-of-type .ck-presence-list__marker{border-bottom-left-radius:var(--ck-border-radius)}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list .ck-user__full-name{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;color:var(--ck-color-base-text);margin-left:var(--ck-spacing-standard)}.ck.ck-presence-list__balloon .ck.ck-presence-list__dropdown-list .ck.ck-presence-list__marker{position:absolute;left:0;margin:0;height:100%;width:3px}.ck-presence-list__balloon button.ck-presence-list__dropdown-list-item *,.ck-presence-list__list button.ck-presence-list__list-item *{cursor:pointer}@keyframes fadeIn{0%{opacity:0}to{opacity:1}}:root{--ck-color-table-focused-cell-background:rgba(158,207,250,0.3)}.ck-widget.table td.ck-editor__nested-editable.ck-editor__nested-editable_focused,.ck-widget.table td.ck-editor__nested-editable:focus,.ck-widget.table th.ck-editor__nested-editable.ck-editor__nested-editable_focused,.ck-widget.table th.ck-editor__nested-editable:focus{background:var(--ck-color-table-focused-cell-background);border-style:none;outline:1px solid var(--ck-color-focus-border);outline-offset:-1px}.ck .ck-insert-table-dropdown__grid{display:flex;flex-direction:row;flex-wrap:wrap}:root{--ck-insert-table-dropdown-padding:10px;--ck-insert-table-dropdown-box-height:11px;--ck-insert-table-dropdown-box-width:12px;--ck-insert-table-dropdown-box-margin:1px}.ck .ck-insert-table-dropdown__grid{width:calc(var(--ck-insert-table-dropdown-box-width)*10 + var(--ck-insert-table-dropdown-box-margin)*20 + var(--ck-insert-table-dropdown-padding)*2);padding:var(--ck-insert-table-dropdown-padding) var(--ck-insert-table-dropdown-padding) 0}.ck .ck-insert-table-dropdown__label{text-align:center}.ck .ck-insert-table-dropdown-grid-box{width:var(--ck-insert-table-dropdown-box-width);height:var(--ck-insert-table-dropdown-box-height);margin:var(--ck-insert-table-dropdown-box-margin);border:1px solid var(--ck-color-base-border);border-radius:1px}.ck .ck-insert-table-dropdown-grid-box.ck-on{border-color:var(--ck-color-focus-border);background:var(--ck-color-focus-outer-shadow)}.ck-content .table{margin:.9em auto;display:table}.ck-content .table table{border-collapse:collapse;border-spacing:0;width:100%;height:100%;border:1px double #b3b3b3}.ck-content .table table td,.ck-content .table table th{min-width:2em;padding:.4em;border:1px solid #bfbfbf}.ck-content .table table th{font-weight:700;background:hsla(0,0%,0%,5%)}.ck-content[dir=rtl] .table th{text-align:right}.ck-content[dir=ltr] .table th{text-align:left}.ck-editor__editable .ck-table-bogus-paragraph{display:inline-block;width:100%}.ck.ck-input-color{width:100%;display:flex;flex-direction:row-reverse}.ck.ck-input-color>input.ck.ck-input-text{min-width:auto;flex-grow:1}.ck.ck-input-color>div.ck.ck-dropdown{min-width:auto}.ck.ck-input-color>div.ck.ck-dropdown>.ck-input-color__button .ck-dropdown__arrow{display:none}.ck.ck-input-color .ck.ck-input-color__button{display:flex}.ck.ck-input-color .ck.ck-input-color__button .ck.ck-input-color__button__preview{position:relative;overflow:hidden}.ck.ck-input-color .ck.ck-input-color__button .ck.ck-input-color__button__preview>.ck.ck-input-color__button__preview__no-color-indicator{position:absolute;display:block}[dir=ltr] .ck.ck-input-color>.ck.ck-input-text{border-top-right-radius:0;border-bottom-right-radius:0}[dir=rtl] .ck.ck-input-color>.ck.ck-input-text{border-top-left-radius:0;border-bottom-left-radius:0}.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button{padding:0}[dir=ltr] .ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button{border-left-width:0;border-top-left-radius:0;border-bottom-left-radius:0}[dir=rtl] .ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button{border-right-width:0;border-top-right-radius:0;border-bottom-right-radius:0}.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button.ck-disabled{background:var(--ck-color-input-disabled-background)}.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button>.ck.ck-input-color__button__preview{border-radius:0}.ck-rounded-corners .ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button>.ck.ck-input-color__button__preview,.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button>.ck.ck-input-color__button__preview.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button>.ck.ck-input-color__button__preview{width:20px;height:20px;border:1px solid var(--ck-color-input-border)}.ck.ck-input-color>.ck.ck-dropdown>.ck.ck-button.ck-input-color__button>.ck.ck-input-color__button__preview>.ck.ck-input-color__button__preview__no-color-indicator{top:-30%;left:50%;height:150%;width:8%;background:red;border-radius:2px;transform:rotate(45deg);transform-origin:50%}.ck.ck-input-color .ck.ck-input-color__remove-color{width:100%;border-bottom:1px solid var(--ck-color-input-border);padding:calc(var(--ck-spacing-standard)/2) var(--ck-spacing-standard);border-bottom-left-radius:0;border-bottom-right-radius:0}[dir=ltr] .ck.ck-input-color .ck.ck-input-color__remove-color{border-top-right-radius:0}[dir=rtl] .ck.ck-input-color .ck.ck-input-color__remove-color{border-top-left-radius:0}.ck.ck-input-color .ck.ck-input-color__remove-color .ck.ck-icon{margin-right:var(--ck-spacing-standard)}[dir=rtl] .ck.ck-input-color .ck.ck-input-color__remove-color .ck.ck-icon{margin-right:0;margin-left:var(--ck-spacing-standard)}.ck.ck-form__row{display:flex;flex-direction:row;flex-wrap:nowrap;justify-content:space-between}.ck.ck-form__row>:not(.ck-label){flex-grow:1}.ck.ck-form__row.ck-table-form__action-row .ck-button-cancel,.ck.ck-form__row.ck-table-form__action-row .ck-button-save{justify-content:center}.ck.ck-form__row{padding:var(--ck-spacing-standard) var(--ck-spacing-large) 0}[dir=ltr] .ck.ck-form__row>:not(.ck-label)+*{margin-left:var(--ck-spacing-large)}[dir=rtl] .ck.ck-form__row>:not(.ck-label)+*{margin-right:var(--ck-spacing-large)}.ck.ck-form__row>.ck-label{width:100%;min-width:100%}.ck.ck-form__row.ck-table-form__action-row{margin-top:var(--ck-spacing-large)}.ck.ck-form__row.ck-table-form__action-row .ck-button .ck-button__label{color:var(--ck-color-text)}.ck.ck-form{padding:0 0 var(--ck-spacing-large)}.ck.ck-form:focus{outline:none}.ck.ck-form .ck.ck-input-text{min-width:100%;width:0}.ck.ck-form .ck.ck-dropdown{min-width:100%}.ck.ck-form .ck.ck-dropdown .ck-dropdown__button:not(:focus){border:1px solid var(--ck-color-base-border)}.ck.ck-form .ck.ck-dropdown .ck-dropdown__button .ck-button__label{width:100%}.ck.ck-table-form .ck-form__row.ck-table-form__background-row,.ck.ck-table-form .ck-form__row.ck-table-form__border-row{flex-wrap:wrap}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row{flex-wrap:wrap;align-items:center}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-labeled-field-view{display:flex;flex-direction:column-reverse;align-items:center}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-labeled-field-view .ck.ck-dropdown,.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-table-form__dimension-operator{flex-grow:0}.ck.ck-table-form .ck.ck-labeled-field-view{position:relative}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status{position:absolute;left:50%;bottom:calc(var(--ck-table-properties-error-arrow-size)*-1);transform:translate(-50%,100%);z-index:1}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status:after{content:"";position:absolute;top:calc(var(--ck-table-properties-error-arrow-size)*-1);left:50%;transform:translateX(-50%)}:root{--ck-table-properties-error-arrow-size:6px;--ck-table-properties-min-error-width:150px}.ck.ck-table-form .ck-form__row.ck-table-form__border-row .ck-labeled-field-view>.ck-label{font-size:var(--ck-font-size-tiny);text-align:center}.ck.ck-table-form .ck-form__row.ck-table-form__border-row .ck-table-form__border-style,.ck.ck-table-form .ck-form__row.ck-table-form__border-row .ck-table-form__border-width{width:80px;min-width:80px;max-width:80px}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row{padding:0}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-table-form__dimensions-row__height,.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-table-form__dimensions-row__width{margin:0}.ck.ck-table-form .ck-form__row.ck-table-form__dimensions-row .ck-table-form__dimension-operator{align-self:flex-end;display:inline-block;height:var(--ck-ui-component-min-height);line-height:var(--ck-ui-component-min-height);margin:0 var(--ck-spacing-small)}.ck.ck-table-form .ck.ck-labeled-field-view{padding-top:var(--ck-spacing-standard)}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status{border-radius:0}.ck-rounded-corners .ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status,.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status.ck-rounded-corners{border-radius:var(--ck-border-radius)}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status{background:var(--ck-color-base-error);color:var(--ck-color-base-background);padding:var(--ck-spacing-small) var(--ck-spacing-medium);min-width:var(--ck-table-properties-min-error-width);text-align:center}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status:after{border-left:var(--ck-table-properties-error-arrow-size) solid transparent;border-bottom:var(--ck-table-properties-error-arrow-size) solid var(--ck-color-base-error);border-right:var(--ck-table-properties-error-arrow-size) solid transparent;border-top:0 solid transparent}.ck.ck-table-form .ck.ck-labeled-field-view .ck.ck-labeled-field-view__status{animation:ck-table-form-labeled-view-status-appear .15s ease both}.ck.ck-table-form .ck.ck-labeled-field-view .ck-input.ck-error:not(:focus)+.ck.ck-labeled-field-view__status{display:none}@keyframes ck-table-form-labeled-view-status-appear{0%{opacity:0}to{opacity:1}}.ck.ck-table-properties-form .ck-form__row.ck-table-properties-form__alignment-row{flex-wrap:wrap;flex-basis:0;align-content:baseline}.ck.ck-table-properties-form .ck-form__row.ck-table-properties-form__alignment-row .ck.ck-toolbar .ck-toolbar__items{flex-wrap:nowrap}.ck.ck-table-properties-form{width:320px}.ck.ck-table-properties-form .ck-form__row.ck-table-properties-form__alignment-row{align-self:flex-end;padding:0}.ck.ck-table-properties-form .ck-form__row.ck-table-properties-form__alignment-row .ck.ck-toolbar{background:none;margin-top:var(--ck-spacing-standard)}.ck.ck-table-properties-form .ck-form__row.ck-table-properties-form__alignment-row .ck.ck-toolbar .ck-toolbar__items>*{width:40px}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__alignment-row{flex-wrap:wrap}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__alignment-row .ck.ck-toolbar:first-of-type{flex-grow:0.57}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__alignment-row .ck.ck-toolbar:last-of-type{flex-grow:0.43}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__alignment-row .ck.ck-toolbar .ck-button{flex-grow:1}.ck.ck-table-cell-properties-form{width:320px}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__padding-row{align-self:flex-end;padding:0;width:25%}.ck.ck-table-cell-properties-form .ck-form__row.ck-table-cell-properties-form__alignment-row .ck.ck-toolbar{background:none;margin-top:var(--ck-spacing-standard)}.ck.ck.ck.ck.ck.ck-math-preview{padding:8px;background-color:#fafafa;margin-top:20px;width:fit-content;border-radius:8px;border:1px solid rgba(0,0,0,.3)}.ck-math-view .ck.ck.ck.ck.ck.ck-input-text,.ck.ck.ck.ck.ck.ck-math-preview{margin-left:auto;margin-right:auto;display:block;box-shadow:0 10px 20px rgba(0,0,0,.19),0 6px 6px rgba(0,0,0,.23)}.ck-math-view .ck.ck.ck.ck.ck.ck-input-text{color:#fff;background-color:#4f4f4f;min-width:20px;border:none;outline:none;font-family:monospace}.ck.ck.ck.ck.ck mjx-merror{font-size:14px;color:#e64a19;background-color:transparent}.ck.ck-math-form{display:flex;align-items:flex-start;flex-direction:row;flex-wrap:nowrap}.ck.ck.ck.ck.ck.ck-math-balloon{box-shadow:none;background:none;border:none}.ck-math-balloon:after,.ck-math-balloon:before{display:none}.placeholder{padding:2px;outline-offset:-2px;line-height:1em;margin:0 1px}.placeholder::selection{display:none}.ck .footnote-section{padding:10px;margin:1em 0;background:rgba(0,0,0,.02);border:1px solid #c4c4c4;border-radius:2px;counter-reset:footnote-counter}.ck .footnote-item{list-style:none;counter-increment:footnote-counter;margin-left:.5em;display:flex}.ck .footnote-item>*{vertical-align:text-top}.ck .footnote-back-link{position:relative;margin-right:.1em;top:-.2em}.ck .footnotes .footnote-back-link>sup{margin-right:0}.ck .footnote-item:before{content:counter(footnote-counter) ". ";display:inline-block;position:relative;right:.2em;min-width:fit-content;text-align:right}.ck .footnote-content{display:inline-block;padding:0 .3em;width:95%;border-radius:2px;flex-grow:1}.ck .ck-widget.footnote-section .ck-widget__type-around__button_after{display:none}
        ]]>
        </style>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
        <style>
        <![CDATA[
        .mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
        .MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
        .mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
        .mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
        .mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
        .mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
        .mjx-numerator {display: block; text-align: center}
        .mjx-denominator {display: block; text-align: center}
        .MJXc-stacked {height: 0; position: relative}
        .MJXc-stacked > * {position: absolute}
        .MJXc-bevelled > * {display: inline-block}
        .mjx-stack {display: inline-block}
        .mjx-op {display: block}
        .mjx-under {display: table-cell}
        .mjx-over {display: block}
        .mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
        .mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
        .mjx-stack > .mjx-sup {display: block}
        .mjx-stack > .mjx-sub {display: block}
        .mjx-prestack > .mjx-presup {display: block}
        .mjx-prestack > .mjx-presub {display: block}
        .mjx-delim-h > .mjx-char {display: inline-block}
        .mjx-surd {vertical-align: top}
        .mjx-surd + .mjx-box {display: inline-flex}
        .mjx-mphantom * {visibility: hidden}
        .mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
        .mjx-annotation-xml {line-height: normal}
        .mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}
        .mjx-mtr {display: table-row}
        .mjx-mlabeledtr {display: table-row}
        .mjx-mtd {display: table-cell; text-align: center}
        .mjx-label {display: table-row}
        .mjx-box {display: inline-block}
        .mjx-block {display: block}
        .mjx-span {display: inline}
        .mjx-char {display: block; white-space: pre}
        .mjx-itable {display: inline-table; width: auto}
        .mjx-row {display: table-row}
        .mjx-cell {display: table-cell}
        .mjx-table {display: table; width: 100%}
        .mjx-line {display: block; height: 0}
        .mjx-strut {width: 0; padding-top: 1em}
        .mjx-vsize {width: 0}
        .MJXc-space1 {margin-left: .167em}
        .MJXc-space2 {margin-left: .222em}
        .MJXc-space3 {margin-left: .278em}
        .mjx-test.mjx-test-display {display: table!important}
        .mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}
        .mjx-test.mjx-test-default {display: block!important; clear: both}
        .mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}
        .mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}
        .mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}
        .mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
        .MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}
        .MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}
        .MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}
        .MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}
        .MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
        .MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
        .MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
        .MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
        .MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
        .MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
        .MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
        .MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
        .MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
        .MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
        .MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
        .MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
        .MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
        .MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
        .MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
        .MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
        .MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
        .MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
        .MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
        .MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}
        .MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}
        @font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
        @font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
        @font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
        @font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
        @font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
        @font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
        @font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
        @font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
        @font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
        @font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
        @font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
        @font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
        @font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
        @font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
        @font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
        @font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
        @font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
        @font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
        @font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
        @font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
        @font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
        @font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
        @font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
        @font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
        @font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
        @font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
        @font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
        @font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}
        @font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}
        @font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}
        @font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}
        @font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}
        ]]>
        </style>
        <style id="intercom-lightweight-app-style" type="text/css">
        /*<![CDATA[*/
        @keyframes intercom-lightweight-app-launcher {
        from {
        opacity: 0;
        transform: scale(0.5);
        }
        to {
        opacity: 1;
        transform: scale(1);
        }
        }

        @keyframes intercom-lightweight-app-gradient {
        from {
        opacity: 0;
        }
        to {
        opacity: 1;
        }
        }

        @keyframes intercom-lightweight-app-messenger {
        0% {
        opacity: 0;
        transform: scale(0);
        }
        40% {
        opacity: 1;
        }
        100% {
        transform: scale(1);
        }
        }

        .intercom-lightweight-app {
        position: fixed;
        z-index: 2147483001;
        width: 0;
        height: 0;
        font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif;
        }

        .intercom-lightweight-app-gradient {
        position: fixed;
        z-index: 2147483002;
        width: 500px;
        height: 500px;
        bottom: 0;
        right: 0;
        pointer-events: none;
        background: radial-gradient(
        ellipse at bottom right,
        rgba(29, 39, 54, 0.16) 0%,
        rgba(29, 39, 54, 0) 72%);
        animation: intercom-lightweight-app-gradient 200ms ease-out;
        }

        .intercom-lightweight-app-launcher {
        position: fixed;
        z-index: 2147483003;
        padding: 0 !important;
        margin: 0 !important;
        border: none;
        bottom: 20px;
        right: 20px;
        max-width: 48px;
        width: 48px;
        max-height: 48px;
        height: 48px;
        border-radius: 50%;
        background: #f5f5f5;
        cursor: pointer;
        box-shadow: 0 1px 6px 0 rgba(0, 0, 0, 0.06), 0 2px 32px 0 rgba(0, 0, 0, 0.16);
        transition: transform 167ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
        box-sizing: content-box;
        }


        .intercom-lightweight-app-launcher:hover {
        transition: transform 250ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
        transform: scale(1.1)
        }

        .intercom-lightweight-app-launcher:active {
        transform: scale(0.85);
        transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1);
        }


        .intercom-lightweight-app-launcher:focus {
        outline: none;


        }

        .intercom-lightweight-app-launcher-icon {
        display: flex;
        align-items: center;
        justify-content: center;
        position: absolute;
        top: 0;
        left: 0;
        width: 48px;
        height: 48px;
        transition: transform 100ms linear, opacity 80ms linear;
        }

        .intercom-lightweight-app-launcher-icon-open {

        opacity: 1;
        transform: rotate(0deg) scale(1);

        }

        .intercom-lightweight-app-launcher-icon-open svg {
        width: 24px;
        height: 24px;
        }

        .intercom-lightweight-app-launcher-icon-open svg path {
        fill: rgb(0, 0, 0);
        }

        .intercom-lightweight-app-launcher-icon-self-serve {

        opacity: 1;
        transform: rotate(0deg) scale(1);

        }

        .intercom-lightweight-app-launcher-icon-self-serve svg {
        height: 44px;
        }

        .intercom-lightweight-app-launcher-icon-self-serve svg path {
        fill: rgb(0, 0, 0);
        }

        .intercom-lightweight-app-launcher-custom-icon-open {
        max-height: 24px;
        max-width: 24px;


        opacity: 1;
        transform: rotate(0deg) scale(1);

        }

        .intercom-lightweight-app-launcher-icon-minimize {

        opacity: 0;
        transform: rotate(-60deg) scale(0);

        }

        .intercom-lightweight-app-launcher-icon-minimize svg path {
        fill: rgb(0, 0, 0);
        }

        .intercom-lightweight-app-messenger {
        position: fixed;
        z-index: 2147483003;
        overflow: hidden;
        background-color: white;
        animation: intercom-lightweight-app-messenger 250ms cubic-bezier(0, 1, 1, 1);
        transform-origin: bottom right;

        width: 400px;
        height: calc(100% - 104px);
        max-height: 704px;
        min-height: 250px;
        right: 20px;
        bottom: 84px;
        box-shadow: 0 5px 40px rgba(0,0,0,0.16);


        border-radius: 16px;
        }

        .intercom-lightweight-app-messenger-header {
        height: 64px;
        border-bottom: none;
        background: #f5f5f5


        }

        .intercom-lightweight-app-messenger-footer{
        position:absolute;
        bottom:0;
        width: 100%;
        height: 80px;
        background: #fff;
        font-size: 14px;
        line-height: 21px;
        border-top: 1px solid rgba(0, 0, 0, 0.05);
        box-shadow: 0px 0px 25px rgba(0, 0, 0, 0.05);

        }

        @media print {
        .intercom-lightweight-app {
        display: none;
        }
        }
        /*]]>*/
        </style>
    </head>
    <body class="abTestNoEffect_group2 collectionsPageABTest_originalLayoutGroup booksProgressBarABTest_progressBar welcomeBoxABTest_welcomeBox twoLineEventsSidebar_expanded Layout-whiteBackground">
        <div id="react-app">
            <div class="wrapper" id="wrapper">
                <div class="IntercomWrapper-intercomFrame" id="intercom-outer-frame"></div><noscript class="noscript-warning">This website requires javascript to properly function. Consider activating javascript to get access to all site functionality.</noscript> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRC765W" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
                <div class="Header-root">
                    <div style="height:64px" class="Header-headroom headroom-wrapper">
                        <div class="headroom headroom--unpinned headroom-disable-animation">
                            <header class="Header-appBar">
                                <div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters">
                                    <div class="Header-hideSmDown">
                                        <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root ForumIcon-root" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                        <path fill="none" d="M0 0h24v24H0z"></path>
                                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span></button>
                                    </div>
                                    <div class="Header-hideMdUp">
                                        <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                        <path d="M3 9h14V7H3v2zm0 4h14v-2H3v2zm0 4h14v-2H3v2zm16 0h2v-2h-2v2zm0-10v2h2V7h-2zm0 6h2v-2h-2v2z"></path>
                                        <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button>
                                    </div>
                                    <div class="Header-hideSmDown">
                                        <div class="Header-titleSubtitleContainer">
                                            <h2 class="Typography-root Typography-title Header-title">
                                                <a class="Header-titleLink" href="/">LESSWRONG</a>
                                            </h2>
                                            <div class="Header-hideMdUp">
                                                <h2 class="Typography-root Typography-title Header-title">
                                                    <a class="Header-titleLink" href="/">LW</a>
                                                </h2>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="Header-rightHeaderItems">
                                        <div class="SearchBar-root">
                                            <div class="SearchBar-rootChild">
                                                <div class="SearchBar-searchInputArea">
                                                    <div>
                                                        <svg class="MuiSvgIcon-root SearchBar-searchIcon" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                        <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path>
                                                        <path fill="none" d="M0 0h24v24H0z"></path></svg>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="UsersAccountMenu-root">
                                            <button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat" type="button"><span class="MuiButton-label"><span class="UsersAccountMenu-userButton">Login</span></span></button>
                                        </div>
                                    </div>
                                </div>
                            </header>
                            <div class="jss85 jss86" style="width: 20px;"></div>
                        </div>
                    </div>
                </div>
                <div class="Layout-standaloneNavFlex">
                    <div class="Layout-searchResultsArea"></div>
                    <div class="Layout-main Layout-whiteBackground">
                        <div class="flash-messages FlashMessages-root"></div>
                        <div class="PostsPage-readingProgressBar"></div>
                        <div class="ToCColumn-root ToCColumn-tocActivated">
                            <div class="ToCColumn-header">
                                <div class="PostsPage-title">
                                    <div class="PostsPage-centralColumn">
                                        <div class="PostsPagePostHeader-header">
                                            <div class="PostsPagePostHeader-headerLeft">
                                                <div>
                                                    <h1 class="Typography-root Typography-display3 PostsPageTitle-root">
                                                        <a class="PostsPageTitle-link" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk">Predictable updating about AI risk</a>
                                                    </h1>
                                                </div>
                                                <div class="PostsPagePostHeader-authorAndSecondaryInfo">
                                                    <div class="PostsPagePostHeader-authorInfo">
                                                        <div class="PostsPagePostHeader-authors">
                                                            <span class="Typography-root Typography-body1 PostsAuthors-root">by <span class="PostsAuthors-authorName"><span><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/joe-carlsmith?from=post_header">Joe Carlsmith</a></span></span></span></span></span>
                                                        </div>
                                                    </div>
                                                    <div class="PostsPagePostHeader-secondaryInfo">
                                                        <div class="PostsPagePostHeader-secondaryInfoLeft">
                                                            <span class="LWTooltip-root"><span class="PostsPagePostHeader-wordCount">43<!-- --> min read</span></span><span class="LWTooltip-root"><span class="PostsPageDate-date">9th May 2023</span></span><a class="PostsPagePostHeader-secondaryInfoLink" href="#comments">20 comments</a>
                                                            <div class="PostActionsButton-root">
                                                                <div>
                                                                    <span class="PostsPagePostHeader-actions"><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div class="PostsPagePostHeader-headerVote">
                                                <div class="PostsVote-voteBlock">
                                                    <div class="PostsVote-upvote" title="You must be logged in and have 1 karma to vote">
                                                        <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-up VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                        <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                        <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button>
                                                    </div>
                                                    <div class="PostsVote-voteScores">
                                                        <div title="100 Votes">
                                                            <h1 class="Typography-root Typography-headline PostsVote-voteScore">
                                                                273
                                                            </h1>
                                                        </div>
                                                    </div>
                                                    <div class="PostsVote-downvote" title="You must be logged in and have 1 karma to vote">
                                                        <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-down VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                        <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                        <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                        <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button>
                                                    </div>
                                                </div>
                                            </div>
                                        </div><span class="FooterTagList-root"><span class="FooterTag-root FooterTag-core"><a href="/tag/world-modeling"><span class="FooterTag-name">World Modeling</span></a></span><span class="FooterTag-root FooterTag-core"><a href="/tag/ai"><span class="FooterTag-name">AI</span></a></span><a class="FooterTagList-postTypeLink" href="/recommendations">
                                        <div class="FooterTagList-frontpageOrPersonal">
                                            <span class="LWTooltip-root">Curated</span>
                                        </div></a></span>
                                    </div>
                                </div>
                            </div>
                            <div class="ToCColumn-toc">
                                <div class="ToCColumn-stickyBlockScroller">
                                    <div class="ToCColumn-stickyBlock">
                                        <div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level0">
                                                <a href="#" class="TableOfContentsRow-link TableOfContentsRow-title TableOfContentsRow-highlightDot">Predictable updating about AI risk</a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#1__Introduction" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>1. Introduction</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#2__Sometimes_predictably_real_stuff_doesn_t_feel_real_yet" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>2. Sometimes predictably-real stuff doesn’t feel real yet</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#3__When_guts_go_wrong" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>3. When guts go wrong</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#3_1_War" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>3.1 War</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#3_2_Death" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>3.2 Death</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#4__Noticing_your_non_confusion" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>4.&#160;Noticing your non-confusion</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#4_1_LLMs" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>4.1 LLMs</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#4_2_Simulations" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>4.2 Simulations</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#4_3__It_s_just_like_they_said_" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>4.3 “It’s just like they said”</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#5__Smelling_the_mustard_gas" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>5. Smelling the mustard gas</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#5_1_Should_you_trust_your_future_gut__though_" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>5.1 Should you trust your future gut, though?</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level2">
                                                <a href="#5_2_An_aside_on_mental_health" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>5.2 An aside on mental health</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#6__Constraints_on_future_worrying" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>6.&#160;Constraints on future worrying</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#7__Should_you_expect_low_probabilities_to_go_down_" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>7.&#160;Should you expect low probabilities to go down?</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#8__Will_the_next_president_be_a_potato_" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>8. Will the next president be a potato?</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#9__Just_saying__oops_" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>9. Just saying “oops”</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level1">
                                                <a href="#10__Doing_enough" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>10. Doing enough</span></a>
                                            </div>
                                            <div class="TableOfContentsRow-divider"></div>
                                            <div class="TableOfContentsRow-root TableOfContentsRow-level0 TableOfContentsRow-highlighted">
                                                <a href="#comments" class="TableOfContentsRow-link TableOfContentsRow-highlightDot"><span>20 comments</span></a>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="ToCColumn-gap1"></div>
                            <div class="ToCColumn-content">
                                <div class="PostsPage-centralColumn">
                                    <div class="PostsPage-postContent instapaper_body ContentStyles-base content ContentStyles-postBody">
                                        <div class="commentOnSelection">
                                            <div class="ContentItemBody-root">
                                                <p id="block0">
                                                    (Cross-posted from <span><span><span><a href="https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk">my website</a></span></span></span>. Podcast version <span><span><span><a href="https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk">here</a></span></span></span>, or search "Joe Carlsmith Audio" on your podcast app.)
                                                </p>
                                                <blockquote id="block1">
                                                    <p id="block2">
                                                        <i>"This present moment used to be the unimaginable future."</i>
                                                    </p>
                                                    <p id="block3">
                                                        <i>- Stewart Brand</i>
                                                    </p>
                                                </blockquote>
                                                <h2 id="1__Introduction">
                                                    1. Introduction
                                                </h2>
                                                <p id="block4">
                                                    Here’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.<span class="footnote-reference" role="doc-noteref" id="fnref0g9sn95iiqe"><sup><span><span><a href="#fn0g9sn95iiqe">[1]</a></span></span></sup></span>
                                                </p>
                                                <p id="block5">
                                                    Now, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts <span><span><span><a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">signing letters</a></span></span></span> and <span><span><span><a href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html">quitting their jobs</a></span></span></span>.
                                                </p>
                                                <p id="block6">
                                                    But if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.<span class="footnote-reference" role="doc-noteref" id="fnref048nj5dhr9di"><sup><span><span><a href="#fn048nj5dhr9di">[2]</a></span></span></sup></span>&#160;When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.
                                                </p>
                                                <p id="block7">
                                                    But I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <span><span><span><a href="https://agentgpt.reworkd.ai/">Agent-GPT</a></span></span></span>-8, or <span><span><span><a href="https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/">Chaos-GPT</a></span></span></span>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.
                                                </p>
                                                <p id="block8">
                                                    This essay is about what “doing this right” looks like. In particular: part of what happens, when you meet something in the flesh, is that it “seems more real” at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; “up close” vs. “far away.” My views on this have changed over the years: and in particular, I now put less weight on my gut’s (comparatively skeptical) views about doom.
                                                </p>
                                                <p id="block9">
                                                    But the essay is also about grokking some basic Bayesianism about future evidence, dispelling a common misconception about it (namely: that directional updates shouldn’t be predictable in general), and pointing at some of the constraints it places on our beliefs over time, especially with respect to stuff we’re currently skeptical or dismissive about. For example, at least in theory: you should never think it &gt;50% that your credence on something will later double; never &gt;10% that it will later 10x, and so forth. So if you’re currently e.g. 1% or less on AI doom, you should think it’s less than 50% likely that you’ll ever be at 2%; less than 10% likely that you’ll ever be at 10%, and so on. And if your credence is very small, or if you’re acting dismissive, you should be very confident you’ll never end up worried. Are you?
                                                </p>
                                                <p id="block10">
                                                    I also discuss when, exactly, it’s problematic to update in predictable directions. My sense is that generally, you should expect to update in the direction of the <i>truth</i> as the evidence comes in; and thus, that people who think AI doom unlikely should expect to feel <i>less worried</i> as time goes on (such that consistently getting more worried is a red flag). But in the case of AI risk, I think at least some non-crazy views should actually expect to get <i>more worried</i> over time, even while being fairly non-worried now. In particular, if you think you face a small risk conditional on something likely-but-not-certain (for example, AGI getting developed by blah date), you can sometimes expect to update towards facing the risk, and thus towards greater worry, before you update towards being safe. But there are still limits to how much more worried you can predictably end up.
                                                </p>
                                                <p id="block11">
                                                    Importantly, none of this is meant to encourage consistency with respect to views you held in the past, at the expense of reasonableness in the present or future. If you said .1% last year, and you’re at 10% now (or if you hit 90% when you see GPT-6): well, better to just say “<span><span><a class="PostLinkPreviewWithPost-link" href="/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops">oops</a></span></span>.” Indeed, I’ve been saying “oops” myself about various things. And more generally, applying basic Bayesianism in practice takes lots of taste. But faced with predictable progress towards advanced but mostly-still-abstract-for-now AI, I think it’s good to keep in mind.
                                                </p>
                                                <p id="block12">
                                                    I close with some thoughts on how we will each look back on what we did, or didn’t do, during the lead-up to AGI, once the truth about the risks is made plain.
                                                </p>
                                                <p id="block13">
                                                    <i>Thanks to Katja Grace for extensive discussion and inspiration. See also citations in the main text and footnotes for specific points and examples that originated with Katja. And thanks also to Leopold Aschenbrenner for comments. Some of my thinking and writing on this topic occurred in the context of my work for Open Philanthropy, but I’m speaking only for myself and not for my employer.</i>
                                                </p>
                                                <h2 id="2__Sometimes_predictably_real_stuff_doesn_t_feel_real_yet">
                                                    2. Sometimes predictably-real stuff doesn’t feel real yet
                                                </h2>
                                                <blockquote id="block14">
                                                    <p id="block15">
                                                        <i>"Every year without knowing it I have passed the day</i>
                                                    </p>
                                                    <p id="block16">
                                                        <i>When the last fires will wave to me</i>
                                                    </p>
                                                    <p id="block17">
                                                        <i>And the silence will set out</i>
                                                    </p>
                                                    <p id="block18">
                                                        <i>Tireless traveler</i>
                                                    </p>
                                                    <p id="block19">
                                                        <i>Like the beam of a lightless star"</i>
                                                    </p>
                                                    <p id="block20">
                                                        <i>-</i> <span><span><span><a href="https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/"><i>W.S. Merwin</i></a></span></span></span><i>, “For the Anniversary of My Death”</i>
                                                    </p>
                                                </blockquote>
                                                <p id="block21">
                                                    I first heard about AI risk in 2013. I was at a picnic-like thing, talking with someone from the Future of Humanity Institute. He mentioned AI risk. I laughed and said something about “like in the movie <i>I, Robot</i>?” He didn’t laugh.
                                                </p>
                                                <p id="block22">
                                                    Later, I talked with more people, and read Bostrom’s <span><span><span><a href="https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">Superintelligence</a></span></span></span>. I had questions, but the argument seemed strong enough to take seriously. And at an intellectual level, the risk at stake seemed like a big deal.
                                                </p>
                                                <p id="block23">
                                                    At an emotional level, though, it didn’t <i>feel real</i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um … nano-bots? I wasn’t good at imagining nano-bots.
                                                </p>
                                                <p id="block24">
                                                    I remember looking at some farmland out the window of a bus, and wondering: am I supposed to think that this will all be compute clusters or something? I remember looking at a church and thinking: am I supposed to imagine robots tearing this church apart? I remember a late night at the Future of Humanity Institute office (I ended up working there in 2017-18), asking someone passing through the kitchen how to imagine the AI killing us; he turned to me, pale in the fluorescent light, and said “whirling knives.”
                                                </p>
                                                <p id="block25">
                                                    Whirling knives? <span><span><span><a href="https://twitter.com/ESYudkowsky/status/1438198189782290433">Diamondoid bacteria</a></span></span></span>? Relentless references to paper-clips, or “tiny molecular squiggles”? I’ve written, elsewhere, about <span><span><span><a href="https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see#iv-realization-vs-belief">the “unreality” of futurism</a></span></span></span>. AI risk had a lot of that for me.
                                                </p>
                                                <p id="block26">
                                                    That is, I wasn’t <i>viscerally worried</i>. I had the concepts. But I didn’t have the “actually” part. And I wasn’t alone. As I started working on the topic more seriously, I met some people who were viscerally freaked-out, depressed, and so on – whether for good or ill. But I met lots of people who weren’t, and not because they were protecting their mental health or something (or at least, not very consciously). Rather, their head was convinced, but not their gut. Their gut still expected, you know, <span><span><span><a href="https://www.cold-takes.com/this-cant-go-on/">normality</a></span></span></span>.
                                                </p>
                                                <p id="block27">
                                                    At the time, I thought this was an important signal about the epistemic situation. Your gut can be smarter than your head. If your gut isn’t on board, maybe your head should be more skeptical. And having your gut on board with whatever you’re doing seems good from other angles, too.<span class="footnote-reference" role="doc-noteref" id="fnrefqnj4snix8n"><sup><span><span><a href="#fnqnj4snix8n">[3]</a></span></span></sup></span>&#160;I spent time trying to resolve the tension. I made progress, but didn’t wholly sync up. To this day, nano-bots and dyson spheres and the word “singularity” still land in an abstract part of my mind – the part devoted to a certain kind of conversation, rather than to, like, the dirty car I can see outside my window, and the tufts of grass by the chain-link fence.
                                                </p>
                                                <p id="block28">
                                                    I still think that your gut can be an important signal, and that if you find yourself saying that you believe blah, but you’re not <span><span><span><a href="https://www.econlib.org/archives/2016/01/the_invisible_t.html">feeling</a></span></span></span> or acting like it, you should stop and wonder. And sometimes, people/ideas that try to get you to not listen to your gut are trying (whether intentionally or not) to bypass important defenses. I am not, in what follows, trying to tell you to throw your gut away. And to the extent I am questioning your gut: please, by all means, be more-than-usually wary. Still, though, and speaking personally: I’ve come to put less stock than I used to in my gut’s Bayesian virtue with respect to AI. I want to talk a bit about why.
                                                </p>
                                                <h2 id="3__When_guts_go_wrong">
                                                    3. When guts go wrong
                                                </h2>
                                                <blockquote id="block29">
                                                    <p id="block30">
                                                        <i>"Then I will no longer</i>
                                                    </p>
                                                    <p id="block31">
                                                        <i>Find myself in life as in a strange garment</i>
                                                    </p>
                                                    <p id="block32">
                                                        <i>Surprised at the earth…"</i>
                                                    </p>
                                                    <p id="block33">
                                                        <i>-</i><span><span><span><a href="https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/"><i>W.S. Merwin</i></a></span></span></span><i>, “For the Anniversary of My Death”</i>
                                                    </p>
                                                </blockquote>
                                                <p id="block34">
                                                    Part of this is reflection on examples where guts go wrong, especially about the future. There are lots of candidates. Indeed, depending on how sharply we distinguish between your “system 1” and your gut, a lot of the <span><span><span><a href="https://thedecisionlab.com/biases">biases literature</a></span></span></span> can be read as anti-gut, and a lot of early rationalism as trying to compensate. My interest in head-gut agreement was partly about trying to avoid overcorrection. But there is, indeed, something to be corrected. Here are two examples that seem relevant to predictable updating.
                                                </p>
                                                <h3 id="3_1_War">
                                                    3.1 War
                                                </h3>
                                                <blockquote id="block35">
                                                    <p id="block36">
                                                        <i>“Abstraction is a thing about your mind, and not the world… Saying that AI risk is abstract is like saying that World War II is abstract, because it’s 1935 and hasn’t happened yet. If it happens, it will be very concrete and bad. It will be the worst thing that has ever happened.”</i>
                                                    </p>
                                                    <p id="block37">
                                                        <i>-</i> <span><span><span><a href="https://www.youtube.com/watch?v=j5Lu01pEDWA"><i>Katja Grace</i></a></span></span></span>
                                                    </p>
                                                </blockquote>
                                                <p id="block38">
                                                    I think Katja’s war example is instructive. Consider some young men heading off to war. There’s a trope, here, about how, when the war is just starting, some men sign up excitedly, with dreams of glory and honor. Then, later, they hit the gritty reality: trenches, swamps, villages burning, friends gasping and gurgling as they die. Ken Burn’s <span><span><span><a href="https://www.pbs.org/kenburns/the-vietnam-war/">Vietnam War documentary</a></span></span></span> has some examples. See also “<span><span><span><a href="https://en.wikipedia.org/wiki/Born_on_the_Fourth_of_July_(film)">Born on the Fourth of July</a></span></span></span>.” The soldiers return, if they return, with a very different picture of war. “<span><span><span><a href="https://en.wikipedia.org/wiki/Dulce_et_Decorum_est">In all my dreams before my helpless sight/ He plunges at me, guttering, choking, drowning</a></span></span></span>…”
                                                </p>
                                                <p id="block39">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kymdz6wnfzlkpibrhvs4 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spvydfruwc6lwytxlorb 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zn5bde6lkc3zgocekwhg 462w" />
                                                </p>
                                                <p id="block40">
                                                    <i>Stretcher bearers in World War I (source</i> <a href="https://commons.wikimedia.org/wiki/File:Stretcher_bearers_Passchendaele_August_1917.jpg"><i>here</i></a><i>)</i>
                                                </p>
                                                <p id="block41">
                                                    Now, a part of this is that their initial picture was <i>wrong</i>. But also, sometimes, it’s that their initial picture was <i>abstract</i>. Maybe, if you’d asked them ahead of time, they’d have said “oh yeah, I expect the trenches to be very unpleasant, and that I will likely have to watch some of my friends die.” But their gut didn’t expect this – or, not hard enough. Surrounded, when they set out, by flags and smiling family members and crisp uniforms, it’s hard to think, too, of flies in the eyes of rotting corpses; or trench-foot, and the taste of mustard gas. And anyway, especially if you’re heading into a very new context, it’s often hard to know the specifics ahead of time, and any sufficiently-concrete image is predictably wrong.
                                                </p>
                                                <p id="block42">
                                                    I worry that we’re heading off to something similar, epistemically, to a new war, with respect to AI risk.<span class="footnote-reference" role="doc-noteref" id="fnrefz4yjec5asvf"><sup><span><span><a href="#fnz4yjec5asvf">[4]</a></span></span></sup></span>&#160;Not: happily, and with dreams of glory. But still: abstractly. We’re trying to orient intellectually, and to do what makes sense. But we aren’t in connection with what it will actually be like, if AI kicks off hard, and the doomers are right. Which isn’t to say it will be trench foot and mustard gas. Indeed, even if things go horribly wrong eventually, it might actually be awesome in lots of ways for a while (even if also: extremely strange). But whatever it will be, will be a specific but very-different-from-now thing. Guts aren’t good at that. So it’s not, actually, all that surprising if you’re not as viscerally worried as your explicit beliefs would imply.
                                                </p>
                                                <h3 id="3_2_Death">
                                                    3.2 Death
                                                </h3>
                                                <blockquote id="block43">
                                                    <p id="block44">
                                                        <i>"And who by fire, who by water</i>
                                                    </p>
                                                    <p id="block45">
                                                        <i>Who in the sunshine, who in the night time</i>
                                                    </p>
                                                    <p id="block46">
                                                        <i>Who by high ordeal, who by common trial…"</i>
                                                    </p>
                                                    <p id="block47">
                                                        <i>-</i> <span><span><span><a href="https://www.youtube.com/watch?v=ilGahIwQEQ0"><i>Leonard Cohen</i></a></span></span></span>
                                                    </p>
                                                </blockquote>
                                                <p id="block48">
                                                    Another famous example here is death. No one knows the date or hour. But we know: someday.<span class="footnote-reference" role="doc-noteref" id="fnref1dgio52qsrw"><sup><span><span><a href="#fn1dgio52qsrw">[5]</a></span></span></sup></span>&#160;Right? Well, sort of. We know in the abstract. We know, but don’t always realize. And then sometimes we do, and some vista opens. We reel in some new nothingness. Something burns with new preciousness and urgency.
                                                </p>
                                                <p id="block49">
                                                    And sometimes this happens, specifically, when “someday, somehow” becomes “soon, like this.” When the doctor tells you: you, by avalanche. You, by powder. The month of May. Slow decay. Suddenly, when you’re actually looking at the scans, when you’re hearing estimates in months, you learn fresh who is calling; and despite having always known, some sort of “update” happens. Did the gut not fully believe? One’s own death, after all, is <span><span><span><a href="https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see">hard to see</a></span></span></span>.
                                                </p>
                                                <p id="block50">
                                                    I’ve <span><span><span><a href="https://joecarlsmith.com/2020/12/06/thoughts-on-being-mortal#iii">written about this before</a></span></span></span>. Tim McGraw has a song about the scans thing. “<span><span><span><a href="https://www.youtube.com/watch?v=_9TShlMkQnc">Live like you were dying</a></span></span></span>.” I’m trying. I’m trying to think ahead to that undiscovered hospital. I’m trying to think about what I will take myself to have learned, when I walk out into the parking lot, with only months to live. I’m trying to learn it now instead.
                                                </p>
                                                <p id="block51">
                                                    Really, this is about predictable updating. The nudge in McGraw’s title – you’re already dying – is Bayesian. You shouldn’t need the scans. If you know, now, what you’ll learn later, you can learn it now, too. Death teaches unusually predictable lessons – about fleetingness, beauty, love. And unusually important lessons, too. Bayes bites, here, with special gravity. But there’s some sort of gut problem. The question is how to learn hard enough, and in advance. “<span><span><span><a href="https://www.brainyquote.com/quotes/henry_david_thoreau_107665">And not, when I come to die, to discover that I have not lived</a></span></span></span>.”
                                                </p>
                                                <p id="block52">
                                                    Importantly, though: if your gut thinks you’re not going to die, it’s not actually much evidence. Has your gut been keeping up with the longevity literature? Does it have opinions about cryopreservation? Futurism aside, the gut’s skepticism, here, is an old mistake. And we have practices. Go <span><span><span><a href="https://en.wikipedia.org/wiki/Ash_Wednesday">smear some ashes on your forehead</a></span></span></span>. Go <span><span><span><a href="https://en.wikipedia.org/wiki/Sky_burial">watch some birds eat a corpse</a></span></span></span>. Go put some fruit on the <span><span><span><a href="https://en.wikipedia.org/wiki/Ofrenda">ofrenda</a></span></span></span>, or some flowers on your grandfather’s grave. <span><span><span><a href="https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see">Realization is an art distinct from belief</a></span></span></span>. Sometimes, you already know. Religion, they say, is remembering.
                                                </p>
                                                <p id="block53">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zaqb75irgsve2gjrgzqy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jifrboveteqktdgadtzq 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vupbmg65qbzwnrptgyyv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/chpgceqfmfeilzwalbfu 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mv5ixhkytnydgxgctzlv 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bxij00lcawaqkcc12rz0 722w" /><i>Tibetan sky burial. (Source</i> <a href="https://commons.wikimedia.org/wiki/File:Bundesarchiv_Bild_135-S-12-50-06,_Tibetexpedition,_Ragyapa,_Geier.jpg"><i>here</i></a><i>.)</i>
                                                </p>
                                                <h2 id="4__Noticing_your_non_confusion">
                                                    4.&#160;Noticing your non-confusion
                                                </h2>
                                                <p id="block54">
                                                    So these are some examples where “but my gut isn’t in a very visceral relationship with blah” just isn’t a very strong signal that blah is false. But I also want to flag some more directly AI related places where I think something gut-related has been messing up, for me.
                                                </p>
                                                <h3 id="4_1_LLMs">
                                                    4.1 LLMs
                                                </h3>
                                                <p id="block55"></p>
                                                <div class="SideCommentIcon-sideCommentIconWrapper">
                                                    <span><span class="SideCommentIcon-sideCommentIcon"><svg class="MuiSvgIcon-root" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                    <path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path>
                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span>
                                                </div>ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. <span class="blockquote_SrBCxKcAcK7gLTQTd_1">But depending on what you count: we had scaling laws for deep learning back in</span> <span><span><span><a href="https://arxiv.org/abs/1712.00409"><span class="blockquote_SrBCxKcAcK7gLTQTd_1">2017</span></a></span></span></span><span class="blockquote_SrBCxKcAcK7gLTQTd_1">, or at least</span> <span><span><span><a href="https://arxiv.org/abs/2001.08361"><span class="blockquote_SrBCxKcAcK7gLTQTd_1">2020</span></a></span></span></span><span class="blockquote_SrBCxKcAcK7gLTQTd_1">. I know people who were really paying attention; who really saw it; who really bet.</span> And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren’t, and should not have been, very surprised by the most recent round of LLMs. I was not a “shallow patterns” guy. I didn’t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.
                                                <p id="block56">
                                                    But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of “oh, actually,” and “real deal,” and “fire alarm.” Some part of me was still surprised.
                                                </p>
                                                <p id="block57">
                                                    Indeed, noticing my gut (if not my head) getting surprised at various points over the past few years, I’ve realized that my gut can have some pretty silly beliefs about AI, and/or can fail to connect fairly obvious dots. For example, when I first started thinking about AI, I think some part of me failed to imagine that eventually, if AIs got smart enough, we could just <i>talk to them</i>, and that they would just <i>understand what we were saying</i>, and that interacting with them wouldn’t necessarily be some hyper-precise coding thing. I had spoken to Siri. Obviously, that didn’t count. Then, one day, I spoke, with my voice, to a somewhat-smarter AI, and it responded in a very human-sounding voice, and it was much more like talking on the phone, and some sort of update happened.
                                                </p>
                                                <p id="block58">
                                                    Similarly: I think that in the past, I failed to imagine what the visual experience of interacting with an actually-smart AI would be like. Obviously, I knew about robots; HAL’s red stare; typing commands into a terminal; texting. But somehow, old talk of AGI didn’t conjure this for me. I’m not sure what it conjured. Something about brains in boxes, except: laptops? I think it wasn’t much of anything, really. I think it was just a blank. After all, this isn’t <i>sci-fi</i>. So it must not be like anything you’d see in sci-fi, either, including strains aimed at realism. People, we’re talking about the <i>real future</i>, which means something <i>unimaginable</i>, hence fiction to the imagination, hence nothingness. “The future that can be named is not the true future.” Right?
                                                </p>
                                                <p id="block59">
                                                    Wrong. “Named super specifically” is more plausible, but even wariness of specificity can mislead: sometimes, even the specifics are pretty obvious. I <i>had seen</i> Siri, and chat bots. What sort of fog was I artificially imposing on everything? What was so hard about imagining Siri, but smarter? Now, it feels like “oh, duh.” And certain future experiences feel more concrete, too. It now feels like: oh, right, lots of future AIs will probably have extremely compelling and expressive <span><span><span><a href="https://replika.com/">digital human avatars</a></span></span></span>. Eventually (soon?), they’ll probably be able to look just like (super-hot, super-charismatic) humans on zoom calls. What did I think it would be, R2D2?
                                                </p>
                                                <p id="block60">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga 793w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nhkv2otstngfqbqrnrvf 232w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfatvn35ua9zbtn6cf9k 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/stihzpefyj3ncv4n8wsd 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axbxadmiet8vg9whnksi 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/geetxsphhzcs2gwiqnmm 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uqg443xbl8w36jlzic1z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/e9iy8gcgjvvrrxrhyidk 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/boz9le1jdcq2gbm6xpjr 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vksdqn2ggbhpe8tpeugr 1108w" />
                                                </p>
                                                <p id="block61">
                                                    <span><span><span><a href="https://replika.com/"><i>Some current AIs</i></a></span></span></span>
                                                </p>
                                                <p id="block62">
                                                    “Oh, duh” is never great news, epistemically. But it’s interestingly <i>different</i> news than “<span><span><span><a href="https://www.readthesequences.com/Noticing-Confusion-Sequence">noticing your confusion</a></span></span></span>,” or being straightforwardly surprised. It’s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it’s just like you would’ve said, if you’d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.
                                                </p>
                                                <p id="block63">
                                                    I mentioned this dynamic to Trevor Levin, and he said something about “noticing your non-confusion.” I think it’s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren’t <i>certain</i> you’d see it. But if it feels like your head is unconfused, but your gut is updating from “it’s probably fake somehow” to “oh shit it’s actually real,” then you probably had information your gut was failing to use.
                                                </p>
                                                <h3 id="4_2_Simulations">
                                                    4.2 Simulations
                                                </h3>
                                                <p id="block64">
                                                    I’ll give another maybe-distracting example here. Last year, I spent some time thinking about <span><span><span><a href="https://jc.gatspress.com/pdf/simulation_arguments_revised.pdf">whether we live in a computer simulation</a></span></span></span>. It’s a strange topic, but my head takes the basic argument pretty seriously. My gut, though, generally thinks it’s fake somehow, and forgets about it easily.
                                                </p>
                                                <p id="block65">
                                                    I remember a conversation I had with a friend sometime last year. He said something like: “you know, pretty soon, all sorts of intelligent agents on earth are going to be living in simulations.” I nodded or something. It’s like how: if the scientists are actually <i>putting</i> people’s brains in vats, it’s harder to stamp your foot and say “no way.” We moved on.
                                                </p>
                                                <p id="block66">
                                                    Then, in early April, this paper came out: “<span><span><span><a href="https://arxiv.org/pdf/2304.03442.pdf">Generative Agents: Interactive Simulacra of Human Behavior</a></span></span></span>.” They put 25 artificial agents into an environment similar to The Sims, and had them interact, including via e.g. hosting a valentine’s day party.<span class="footnote-reference" role="doc-noteref" id="fnref4m2cfburfm7"><sup><span><span><a href="#fn4m2cfburfm7">[6]</a></span></span></sup></span>&#160;Here’s the picture from the paper:
                                                </p>
                                                <p id="block67">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/llnsldrmajaxuvyehy7e 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/osectulgs4qjctsror1r 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m7jurokdkjrsvjzndks9 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cr1md9m8f2hxfbzthmhp 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ki3fbycj28tlpzmniqor 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pyh1dv4xp4ecmp0zzlkr 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lgmi3qpvqtobziipibay 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/adjhthooygwkpqihboht 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rx5yxljrmgvx1p6hwzuw 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvkziwrga9fe5n09ashy 1420w" /><i>From</i> <span><span><span><a href="https://arxiv.org/pdf/2304.03442.pdf"><i>here</i></a></span></span></span><i>.</i>
                                                </p>
                                                <p id="block68">
                                                    I opened this paper, read the beginning, looked at this picture, and felt my gut update towards being in a sim. But: c’mon now, gut! What sort of probability would I have put, last year, on “I will, in the future, see vaguely-smart artificial agents put into a vaguely-human simulated environment”? Very high. My friend had literally said as much to me months earlier, and I did not doubt. Indeed, what’s even the important difference between this paper and AlphaStar, or the original Sims?<span class="footnote-reference" role="doc-noteref" id="fnref4y75ebmy7eh"><sup><span><span><a href="#fn4y75ebmy7eh">[7]</a></span></span></sup></span>&#160;How smart the models are? The fact that it’s cute and human-like? My gut lost points, here.
                                                </p>
                                                <p id="block69">
                                                    It’s an avoidable mistake. I’m trying to stop making it.
                                                </p>
                                                <p id="block70">
                                                    I worry that we’re in for a lot of dynamics like this. How seriously, for example, are you taking the possibility that future AIs will be sentient? Well, here’s a mistake to not make: updating a lot once the AIs are using charismatic human avatars, or once they can argue for their sentience as convincingly as a human. Predict it now, people. Update now.
                                                </p>
                                                <h3 id="4_3__It_s_just_like_they_said_">
                                                    4.3 “It’s just like they said”
                                                </h3>
                                                <p id="block71">
                                                    I don’t, often, have nightmares about AI risk. But I had one a few months ago. In it, I was at a roll-out of some new AI system. It was a big event, and there were lots of people. The AI was unveiled. Somehow, it immediately wrote each one of us some kind of hyper-specific, individualized message, requiring a level of fine-grained knowledge and predictive ability that was totally out of the question for any familiar intelligence. I read my message and felt some cold and electric bolt, some recognition. I thought to myself: “it’s just like they said.” I looked around me, and the room was in chaos. Everything was flying apart, in all directions. I don’t remember what happened after that.&#160;
                                                </p>
                                                <p id="block72">
                                                    “Just like they said.” Who’s they? Presumably, the AI worriers. The ones who think that superintelligence is not a fantasy or a discussion-on-twitter, but an actual thing we are on track to do with our computers, and which will cut through our world like butter if we get it wrong.
                                                </p>
                                                <p id="block73">
                                                    But wait: aren’t I an AI worrier? More than many, at least. But dreams, they say, are partly the gut’s domain. Perhaps the “they,” here, was partly my own explicit models. Ask me in the waking world: “will superintelligence be terrifying?” Yes, of course, who could doubt. But ask in my dreams instead, and I need to see it up close. I need to read the message. Only then will my gut go cold: “Oh, shit, it’s just like they said.”
                                                </p>
                                                <p id="block74">
                                                    I’ve had this feeling a few times in the past few months. I remember, a few years ago, making a simple model of AI timelines with a colleague. We used a concept called “wake-up,” indicating the point where the world realized what was happening with AI and started to take it seriously. I think that if, at that point, we could’ve seen what things would be like in 2023, we would’ve said something like: “yeah, that” (though: there’s a ton more waking up to do, so future wake-ups might end up better candidates).
                                                </p>
                                                <p id="block75">
                                                    Similarly, “they” have worried for ages about triggering or exacerbating “race dynamics” in AI. Then, in recent months, Google went into a “<span><span><span><a href="https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html">Code Red</a></span></span></span>” about AI, and the CEO of Microsoft came out and just said straight up: “<span><span><span><a href="https://www.businesstoday.in/technology/news/story/the-race-starts-today-microsoft-officially-brings-chatgpt-ai-to-bing-and-edge-browser-369453-2023-02-08">the race starts today</a></span></span></span>.”
                                                </p>
                                                <p id="block76">
                                                    “They” have worried about AIs being crazy alien minds that we don’t understand. Then, in February, we got to see, briefly, the rampaging strangeness of a good Bing – including all sorts of <span><span><span><a href="https://time.com/6256529/bing-openai-chatgpt-danger-alignment/">deception and manipulation and blackmail</a></span></span></span>, which I <span><span><span><a href="https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/">don’t actually think is the centrally worrying kind</a></span></span></span>, but which doesn’t exactly seem like good news, either.
                                                </p>
                                                <p id="block77">
                                                    “They” have worried about agents, and about AIs running wild on the internet, and about humans not exactly helping with that. Now we have <span><span><span><a href="https://en.wikipedia.org/wiki/Auto-GPT">Auto-GPT</a></span></span></span>, and <span><span><span><a href="https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity">Chaos-GPT</a></span></span></span>, and I open up my browser and I see stuff like <span><span><span><a href="https://agentgpt.reworkd.ai/">this</a></span></span></span>:
                                                </p>
                                                <p id="block78">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8hetjm6w9ytpgy8w88y 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/g4jx6qfqljr5pfhk8nxw 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ftts0hfa6vho5bncqnnn 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pnwqbtx5zxtg3pxoayib 2048w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hzuxg4hennshvtf0cfvn 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t3ybknd10bd72j057obw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xrtcyqsvpg7b5kdmk59o 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/awyiwkdkoqje6wgbekn1 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/aovgrdwbwi69qj8vbjcb 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jvjdpzgxpihru8qcpoau 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eykrqp9vhbqzwhosm4qj 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/acilnuziiquzylpaopwr 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ipnpetuxchsejqv1qvhv 2002w" /><i>Not the pixels I wanted to be seeing at this point in my life.</i>
                                                </p>
                                                <p id="block79">
                                                    Now, I don’t want to litigate, here, exactly who “called” what (or: created what<span class="footnote-reference" role="doc-noteref" id="fnref5gj8wfkheeo"><sup><span><span><a href="#fn5gj8wfkheeo">[8]</a></span></span></sup></span>), and how hard, and how much of an update all this stuff should be. And I think some things – for example, the world’s sympathy towards concern about risks from AI – have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from “that’s what the model says” to “oh shit here we are.” And my gut has gotten more worried.
                                                </p>
                                                <p id="block80">
                                                    Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I’ve just been messing up. Let’s look at the dynamics in more detail.
                                                </p>
                                                <h2 id="5__Smelling_the_mustard_gas">
                                                    5. Smelling the mustard gas
                                                </h2>
                                                <blockquote id="block81">
                                                    <p id="block82">
                                                        <i>"Men marched asleep…</i>
                                                    </p>
                                                    <p id="block83">
                                                        <i>All went lame, all blind."</i>
                                                    </p>
                                                    <p id="block84">
                                                        <i>-</i> <span><span><span><a href="https://en.wikipedia.org/wiki/Dulce_et_Decorum_est"><i>Wilfred Owen</i></a></span></span></span>
                                                    </p>
                                                </blockquote>
                                                <p id="block85">
                                                    It’s sometimes thought that, as a Bayesian, you shouldn’t be able to predict which direction you’ll update in the future.<span class="footnote-reference" role="doc-noteref" id="fnrefh3ar4lqw084"><sup><span><span><a href="#fnh3ar4lqw084">[9]</a></span></span></sup></span>That is, if you’re about to get some new evidence about <i>p</i>, you shouldn’t be able to predict whether this evidence will move your credence on <i>p</i> higher or lower. Otherwise, the thought goes, you could “price in” that evidence now, by moving your credence in the predicted direction.
                                                </p>
                                                <p id="block86">
                                                    But this is wrong.<span class="footnote-reference" role="doc-noteref" id="fnref4ohd2xn7yql"><sup><span><span><a href="#fn4ohd2xn7yql">[10]</a></span></span></sup></span>&#160;Consider a simple example. Suppose you’re at 99% that Trump won the election. You’re about to open the newspaper that will tell you for sure. Here, you should be at 99% that you’re about to increase your credence on Trump winning: specifically, up to 100%. It’s a very predictable update.
                                                </p>
                                                <p id="block87">
                                                    So why can’t you price it in? Because there’s a 1% chance that you’re about to lower your confidence in Trump winning <i>by a lot more</i>: specifically, down to 0%. That is, in <i>expectation</i>, your confidence in Trump winning will remain the same.<span class="footnote-reference" role="doc-noteref" id="fnrefc1l5t3jttlk"><sup><span><span><a href="#fnc1l5t3jttlk">[11]</a></span></span></sup></span>And it’s the expectation of your future update that Bayesian binds.
                                                </p>
                                                <p id="block88">
                                                    To understand this more visually, let’s use a slightly more complicated example. Suppose you’re currently at 80% that GPT-6 is going to be “scary smart,” whatever that means to you. And suppose that, conditional on GPT-6 being scary smart, your probability on AI doom is 50%; and conditional on GPT-6 not being scary smart, your probability on AI doom is 10%. So your credence looks like this:
                                                </p>
                                                <p id="block89">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nra6meskaht8wdx1ig3q" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tv20xlvpjkpjwxdwv0ub 942w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lvlh3bnro7dpwogepzyo 276w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dq5jtswyqsjhk6y7bpjx 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hpyup2d5e5wshq2gxp6b 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/afdb2urasvsfdidfzhts 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozzybtunbhswka5gic40 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycrmtyxtriv8em266r0z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kvz3h40wblsqw7ekj2ef 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/libzhbgptdorqbqscxzo 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fdvw3d3simncrdgnnabg 1082w" />
                                                </p>
                                                <p id="block90">
                                                    Now, what’s your overall p(doom)? Well, it’s:
                                                </p>
                                                <blockquote id="block91">
                                                    <p id="block92">
                                                        (probability that GPT-6 is scary smart * probability of doom conditional on GPT-6 being scary smart) + (probability that GPT-6 isn’t scary smart * probability of doom conditional on GPT-6 not being scary smart)
                                                    </p>
                                                </blockquote>
                                                <p id="block93">
                                                    That is, in this case, 42%.<span class="footnote-reference" role="doc-noteref" id="fnrefo3exn1kvfm"><sup><span><span><a href="#fno3exn1kvfm">[12]</a></span></span></sup></span>
                                                </p>
                                                <p id="block94">
                                                    But now we can see a possible format for a gut-problem mistake. In particular: suppose that I ask you, right now, surrounded by flags and crisp uniforms, about the probability of doom. You query your gut, and it smells no mustard gas. So you give an answer that doesn’t smell much mustard gas, either. Let’s say, 10%. And let’s say you don’t really break things down into: OK, how much mustard gas do I smell conditional on GPT-6 being scary smart, vs. not, and what are my probabilities on that.<span class="footnote-reference" role="doc-noteref" id="fnreft4taauzgccn"><sup><span><span><a href="#fnt4taauzgccn">[13]</a></span></span></sup></span>&#160;Rather, your model is an undifferentiated mass:
                                                </p>
                                                <p id="block95">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f 970w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jyz00tksh6fzmsrrwzte 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mpiynbtl2td13dtrivzi 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qwppd7rqhbuwdknpkojj 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spztwjkpmryb8ikhrpw8 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zkbqpvivirrtz7prkonl 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m0vr9aby7wm9ptxoll78 722w" />
                                                </p>
                                                <p id="block96">
                                                    Or maybe you do try to break things down like that, but the waft of the gas fades with all the distance. GPT-6 is far away, behind some fog. Still: you guess, with your head, and without your gut participating, that p(doom) is indeed a bit higher conditional on GPT-6 being scary smart, what with the update towards “short timelines.” Let’s say, 20%; and 10% otherwise. So maybe your overall p(doom), given 80% on the abstract idea of GPT-6 being scary smart, is 18%.
                                                </p>
                                                <p id="block97">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m 957w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nvfxoxgencxt15pfc6nr 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fujyddttrzos4sjsnr9l 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kfnjiozyk6zvtp2z8w5l 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noa75ytmstppqbkogzvq 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tfjun4wdy5kyigpqeqem 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ksuqs7emajlxupcj7cen 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xbsn1amesfysfzuepwgr 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xueudncpvsogtqrrreuz 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cgrrdu5bpna1eeokyttx 1208w" />
                                                </p>
                                                <p id="block98">
                                                    But actually, let’s say, if you could see a “scary smart” GPT-6 model right now, you would freak out way harder. You would be able to smell the gas up close, that bitter tang. Your gut would get some message, and come alive, and start participating in the exercise. “<i>That thing</i>,” your gut might say, “is <i>scary</i>. I’m at 50% on doom, now.”
                                                </p>
                                                <p id="block99">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24 916w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uuix302lquapruhwnzhn 268w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xuhwmnlcjpby3idnqirb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozhynvnhqgxoqfufmkbh 1375w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xxon5usbt8paffsjqvnl 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mrfaxqpbunqytyuqnoyj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eaj9dbbvdrtlonvfy2lr 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dypci8n8pncoyjj84hxo 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/k2wdfefrlolu7gwmivfl 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fsfihwtucf89yzbmjsep 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mze0y4fdbcs3amdlga41 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dwtad6aknstkayhrmglw 1414w" />
                                                </p>
                                                <p id="block100">
                                                    Thus, you end up inconsistent, and dutch-bookable (at least in principle – setting aside issues re: betting on doom). Suppose I ask you, now, to agree to sell me a “pays out $100 conditional on doom” ticket for $30 (let’s assume this can actually pay out), conditional on GPT-6 being scary smart. You’re only at 20% doom in such a world, so you predict that such a ticket will only be worth $20 to you if this deal is ever triggered, so you agree. But actually, when we get to that world, your gut freaks out, and you end up at 50% doom, and that ticket is now worth $50 to you, but you’re selling it for $30. Plus, maybe now you’re regretting other things. Like some of those tweets. And how much alignment work you did.
                                                </p>
                                                <p id="block101">
                                                    As indicated above, I think I’ve made mistakes in this vein. In particular: a few years back, I wrote a <span><span><span><a href="https://arxiv.org/pdf/2206.13353.pdf">report about AI risk</a></span></span></span>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.\<span class="footnote-reference" role="doc-noteref" id="fnrefv5zb8pjk25"><sup><span><span><a href="#fnv5zb8pjk25">[14]</a></span></span></sup></span>&#160;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional</i> on such systems being developed, I was going to look them in the eye and say (in expectation): “~92% that we’re gonna be OK, x-risk-wise.” But on reflection, that wasn’t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <span><span><span><a href="https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/">billions of such machines</a></span></span></span>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.
                                                </p>
                                                <h3 id="5_1_Should_you_trust_your_future_gut__though_">
                                                    <strong>5.1 Should you trust your future gut, though?</strong>
                                                </h3>
                                                <p id="block102">
                                                    Now, you might wonder: why give credit to such future fear?<span class="footnote-reference" role="doc-noteref" id="fnrefu9z9f1zd3ci"><sup><span><span><a href="#fnu9z9f1zd3ci">[15]</a></span></span></sup></span>After all, isn’t part of the worry about doomers that they’re, you know, fraidy-cats? Paranoids? (C’mon: it’s just a superintelligent machine, the invention of a second advanced species, the introduction of a qualitatively new order of optimization power into earth’s ecosystem. It’s just, you know, <i>change</i>.) And isn’t the gut, famously, a bit skittish? Indeed, if you’re worried about your gut being <i>underactive</i>, at a distance, shouldn’t you also be worried about it being <i>over-active,</i> up close? Shouldn’t you reason, instead, ahead of time, at a distance, and in a cool hour, about how scared you should be when you’re there-in-person?
                                                </p>
                                                <p id="block103">
                                                    Well, it’s a judgment call. Sometimes, indeed, at-a-distance is a better epistemic vantage point than up-close. Especially if you know yourself to have biases. Maybe, for example, you’ve got a flying phobia, and you know that once you’re on the plane, your gut’s estimates of the probability of the plane crashing are going to go up a lot. Should you update now, then? Indeed: no.
                                                </p>
                                                <p id="block104">
                                                    But, personally, with respect to the future, I tend to trust my future self more. It’s a dicey game already, futurism, and future Joe has a lot more data. The future is a foreign country, but he’s been there.
                                                </p>
                                                <p id="block105">
                                                    And I tend to trust my up-close self more, in general, for stuff that requires realization rather than belief (and I think words like “superintelligence” require lots of realization). Maybe the journalist has the accurate casualty count; but I trust the soldier on the ground to know what a casualty <i>means</i>. And I trust the man with the scans about death.
                                                </p>
                                                <p id="block106">
                                                    Now, importantly, there’s also a thing where guts sometimesreact surprisingly <i>little</i>, up close, to AI stuff you predicted ahead of time you’d be scared about. Part of this is the “it’s not real AI if you can actually do it,” thing (though, my sense is that this vibe is fading?). Part of it is that sometimes, machines doing blah (e.g., beating humans at chess) is less evidence about stuff than you thought. And I wonder if part of it is that sometimes, your at-a-distance fear of that futuristic AI stuff was imagining some world less mundane and “normal” than the world you actually find yourself in, when the relevant stuff comes around — such that when you, sitting in your same-old apartment, wearing your same-old socks, finally see AIs planning, or understanding language, or passing <span><span><span><a href="https://youtu.be/qbIk7-JPB2c?t=1991">two-hour human coding interviews in four minutes</a></span></span></span>, or <span><span><span><a class="MetaculusPreview-link" href="https://www.metaculus.com/questions/6728/ai-wins-imo-gold-medal/">winning the IMO</a></span></span></span>, it feels/will feel like “well that can’t be the scary thing I had in mind, because that thing is happening in the real world actually and I still have back pain.”<span class="footnote-reference" role="doc-noteref" id="fnref6gbdnzllii"><sup><span><span><a href="#fn6gbdnzllii">[16]</a></span></span></sup></span>&#160;At the least, we get used to stuff fast. &#160;
                                                </p>
                                                <p id="block107">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/n4mvvfidawvrt3n5xqwl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8t50zsshjaqrlajxoyr 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ulagac5alifmngxbsk3x 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bfxmnxnuwhx26z74hykv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/thlguzuwyap6tn7dvnvo 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/oxldviollhiixdaonem6 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zok6hh7zdmnmbmlynv8d 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hl3hzseipbu2bjaxdxst 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gwkm1szomnywdtxgk9wt 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xcvmx28xctsnxu0rb3yb 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/z0qrdbxaspqc8tbyngby 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ugt4lsizoemp8ojpemf5 1998w" />
                                                </p>
                                                <p id="block108">
                                                    <i>GPT-4 doing a coding interview. From</i> <span><span><span><a href="https://www.youtube.com/watch?v=qbIk7-JPB2c&amp;t=1991s"><i>here</i></a></span></span></span><i>.</i>
                                                </p>
                                                <p id="block109">
                                                    Still: sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it’s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no">Yes requires the possibility of no</a></span></span>. If my future self looks the future AGI in the eye and feels like “oh, actually, this isn’t so scary after all,” that’s evidence that my present self is missing something, too. Here’s hoping.
                                                </p>
                                                <h3 id="5_2_An_aside_on_mental_health">
                                                    <strong>5.2 An aside on mental health</strong>
                                                </h3>
                                                <p id="block110">
                                                    Now: a quick caution. Here I’ve been treating guts centrally from an epistemic perspective. But we need a wise <i>practical</i> relationship with our guts as well. And from a practical perspective, I don’t think it’s always productive to try to smell mustard gas harder, or to make horrible things like AI doom vivid. The right dance here is going to vary person-to-person, and I won’t try to treat the topic now (though: see <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of">here</a></span></span> for a list of resources). But I wanted to flag explicitly that staying motivated and non-depressed and so forth, in relation to a pretty scary situation, is a separate art, and one that needs to be woven carefully with the more centrally epistemic angle I’m focused on here. &#160;
                                                </p>
                                                <h2 id="6__Constraints_on_future_worrying">
                                                    6.&#160;Constraints on future worrying
                                                </h2>
                                                <p id="block111">
                                                    Returning to the epistemic perspective though: let’s suppose you do trust your future credences, and you want to avoid the Bayesian “gut problems” I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.
                                                </p>
                                                <p id="block112">
                                                    In particular, you should never think that there’s more than a 1/<i>x</i> chance that your credence will increase by <i>x</i> times: i.e., never more than a 50% chance that it’ll double, never more than a 10% chance that it’ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you’ve only gone up .09% in additive terms – only nine parts in ten thousand. But you’ve also gone up by a factor of 10 – something you should’ve been at least 90% sure would never happen.
                                                </p>
                                                <p id="block113">
                                                    So suppose that right now, you identify as an “AI risk skeptic,” and you put the probability of doom very low. For concreteness, suppose that you like <span><span><span><a href="https://ineffectivealtruismblog.com/2023/04/08/exaggerating-risks-carlsmith-report/">David Thorstad’s number</a></span></span></span>: .00002% — that is, one in five million (though: he now thinks this “too generous” – and he’s also “not convinced that we are in a position where estimating AI risk makes good methodological sense,” which I suspect is a bigger crux). This is a very low number. And it implies, in particular, that you really don’t expect to get even a <i>small amount</i> more worried later. For example, you need to have a maximum of .01% that you ever see evidence that puts the probability at &gt;.2%.
                                                </p>
                                                <p id="block114">
                                                    Now suppose that a few years pass, GPT-6 comes out, and lo, indeed, it is very impressive. You look GPT-6 in the eye and you feel some twinge in your gut. You start to feel a bit, well, at-least-1-percent-y. A bit not-so-crazy-after-all. Now, admittedly, you were probably surprised that GPT-6 is so good. You were a “timelines skeptic,” too. But: how much of a skeptic? Were you, for example, less than one in fifty thousand that GPT-6 would be this impressive? That’s what your previous number can easily imply, if the impressiveness is what’s driving your update.
                                                </p>
                                                <p id="block115">
                                                    And now suppose that actually, you weren’t much of a timelines skeptic at all. GPT-6, according to you, is right on trend. You’d seen the scaling laws. You were at &gt;50% on at-least-this-impressive. It was predictable. It’s just that the rest of the argument for doom is dumb.
                                                </p>
                                                <p id="block116">
                                                    In that case, though, hmm. Your gut’s got heavy constraints, in terms of twinging. &gt;50% on at least-this-impressive? So: you’re still supposed to be at less than .00004% on doom? But what if you’re not…
                                                </p>
                                                <p id="block117">
                                                    Or maybe you think: “the argument for doom has not been satisfactorily peer-reviewed. <span><span><span><a href="https://marginalrevolution.com/marginalrevolution/2023/04/this-gpt-4-answer-speaks-for-itself.html">Where’s the paper in <i>Nature</i></a></span></span></span>? Until I see conventional academic signals, I am at less than one in a thousand on doom, and I shall tweet accordingly.” OK: but, the Bayesianism. If you’re at less than one in a thousand, now, and your big thing is academic credibility, where should Bayes put you later, conditional on <i>seeing</i> conventional academic signals? And what’s your probability on such strange sights? In five years, or ten years, are you confident there won’t be a paper in <i>Nature</i>, or an equivalent? If it’s even 10% percent likely, and it would take you to more than 1%, your number now should be moving ahead of time.
                                                </p>
                                                <p id="block118">
                                                    Or maybe you thought, in the past: “until I see the experts worrying, I’m at less than 1%.” Well, <span><span><span><a href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html">here we are</a></span></span></span> (here we already were, but more now). But: what was your probability that we ended up here? Was it so hard to imagine, the current level of expert sympathy? And are future levels of greater sympathy so hard to imagine, now? It’s easy to live, only, in the present – to move only as far as the present has moved. But the Bayesian has to live, ahead of time, in all the futures at once.
                                                </p>
                                                <p id="block119">
                                                    (Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower? So little hope of future hope?)
                                                </p>
                                                <p id="block120">
                                                    Now: all of this is “in theory.” In practice, this sort of reasoning requires good taste. I talk about such taste more below. First, though, I want to look at the theory a bit more.
                                                </p>
                                                <h2 id="7__Should_you_expect_low_probabilities_to_go_down_">
                                                    7.&#160;Should you expect low probabilities to go down?
                                                </h2>
                                                <p id="block121">
                                                    Above I said that actually, the <i>direction</i> of a future update is often predictable. But notice: <i>which direction</i> should you predict? My sense is that in many evidential situations (though not all – more below), you should think your future evidence more likely to move you in the right direction than the wrong one. So if you think that <i>p</i> is likely to be true, you should generally think that your future evidence is likely to update you towards higher credence on <i>p</i>. And vice versa: if you think that p is more likely to be <i>false</i>, you should expect to have <i>lower</i> credence on it later.
                                                </p>
                                                <p id="block122">
                                                    The Trump example above is an extreme case. You’re at 99% on Trump winning, and you’re also at 99% that you’ll update, in future, towards higher credence on Trump winning. And we can imagine a more intermediate case, where, let’s say, you’re at 90% that Trump is going to win, and you’re about to watch the presidential debate, and you think that winning the debate is highly correlated with winning the election. Which direction should you predict that your credence on Trump winning will move, once the debate is over? Given that you think Trump is likely to win the election, I think you should think he’s likely to win the debate, too. And if he wins the debate, your credence on him winning the election will go up (whereas if he loses, it’ll go down a bunch more).
                                                </p>
                                                <p id="block123">
                                                    Or consider a scientist who doesn’t believe in God. In principle, at each moment, God could appear before her in a tower of flames. She has some (very small) credence on this happening. And if it happened, she would update massively towards theism. But <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence">absence of evidence is evidence of absence</a></span></span>. Every moment she <i>doesn’t</i> observe God appearing before her in a tower of flames, she should be updating some tiny amount towards atheism. And because she predicts very hard that God will never appear before her in a tower of flames, she should be predicting very hard that she will become a more and more confident atheist over time, and that she’ll die with even less faith than she has now.
                                                </p>
                                                <p id="block124">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eufqrpvhqgyjzngzhejg" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nq26vo1twqjfrp8lugux 795w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cx4bl4je69by5syhvvxj 233w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/brnbm1vlo7vkotqm5jdn 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cehrsyb68kukjamyah8p 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qkjyknw4irznm7raz4ys 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mgu5fwmjcrbl7i94t9do 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ruqpj7osmhzbemurme65 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zwv61bykcbqn6jfauqqe 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/js674olpdq3nnneki6g0 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/yreunwtnysfogdbqnne2 1148w" />
                                                </p>
                                                <p id="block125">
                                                    <i>Updating so hard right now… (Image source</i> <a href="https://commons.wikimedia.org/wiki/File:Bourdon,_S%C3%A9bastien_-_Burning_bush.jpg"><i>here</i></a><i>.)</i>
                                                </p>
                                                <p id="block126">
                                                    So too, one might think, with AI risk. If you are currently an AI risk skeptic, plausibly you should expect to become more and more confidently skeptical over time, as your remaining uncertainties about the case for non-doom get resolved in the direction of truth. That is, every moment that the superintelligent machines <i>don’t</i> appear before you in a tower of diamondoid bacteria (that’s the story, right?), then anthropic effects aside, you should be breathing easier and easier. Or, more realistically, you should be expecting to see, well, whatever it is that comforts you: i.e., that we’ll hit another AI winter; or that we’ll make lots of progress in mechanistic interpretability; or that innovations in RLHF will allow superhuman oversight of AI behavior humans can’t understand; or that we won’t see any signs of deception or reward hacking; or that progress will be slow and gradual and nicely coordinated; or that we’ll finally, <i>finally</i>, get some peer review, and put the must-be-confusions to rest. And as your predictions are confirmed, you should be feeling safer and safer.
                                                </p>
                                                <p id="block127">
                                                    Is that what you expect, in your heart? Or are you, perhaps, secretly expecting to get more worried over time? I wished I’d asked myself harder. In particular: my 5% was plausibly implying some vibe like: “sure, there are these arguments that superintelligent AI will disempower us, and I give them some weight, but at least if we’re able to think well about the issue and notice the clues that reality is giving us, over time it will probably become clearer that these arguments are wrong/confused, and we’ll be able to worry much less.” Indeed, depending on the volatility of the evidence I was expecting, perhaps I should have thought that I was likely to be in the ballpark of the highest levels of worry about doom that I would ever endorse. But if you’d asked me, would I have said that?
                                                </p>
                                                <p id="block128">
                                                    That said, I actually think these dynamics are more complicated than they might initially seem. In particular, while I find it plausible that you should generally predict that you’ll update in the direction of what you currently expect to be true, sometimes, actually, you shouldn’t. And some non-crazy views on AI risk fit the mold.
                                                </p>
                                                <p id="block129">
                                                    Katja Grace suggested to me some useful examples. Suppose that you’re in a boat heading down a river. You at 80% that there’s a waterfall about two miles down, but 20% that there isn’t, and that you’re going to see a sign, a mile down, saying as much (“No waterfall” – classic sort of sign). Conditional on no sign/there being a waterfall, you’re at 10% that it’s a big waterfall, which will kill you, and 90% that it’s a small waterfall, which you’ll survive. So currently, your credence on dying is 8%. However, you’re also at 80% that in a mile, it’s going to go up, to 10%, despite your also predicting, now, that this is an update towards higher credence on something that probably won’t happen.
                                                </p>
                                                <p id="block130">
                                                    Or a consider a more real-world example (also from Katja). At 3 pm, you’re planning to take a long car trip. But there’s a 10% chance the trip will fall through. If you take the trip, there’s some small chance you get in an accident. As you approach 3 pm, your credence in “I will get in a car accident today” should go up, as the trip keeps (predictably) not-falling-through. And then, as you’re driving, it should go down gradually, as the remaining time in the car (and therefore, in danger) shrinks.
                                                </p>
                                                <p id="block131">
                                                    Some views on AI – including, skeptical-of-doom views – look like this. Suppose, for example, you think AGI-by-2070 more likely than not. And suppose that conditional on AGI-by-2070, you think there’s some small risk that the doomers are right, and we all die. And you think it’s going to be hard to get good evidence to rule this out ahead of time. Probably, though, we’ll make it through OK. And conditional on no-AGI-by-2070, you think we’re almost certainly fine. Here, you should plausibly expect to get more worried over time, as you get evidence confirming that yes, indeed, AGI-by-2070; yes, indeed, waterfall ahead. And then to get less worried later, as the waterfall proves small.
                                                </p>
                                                <p id="block132">
                                                    That said, this sort of dynamic requires specific constraints on what evidence is available, when. The truth about the future must fail to leak backwards into the past. You must be unable to hear the difference between a big waterfall and a small waterfall sufficiently ahead-of-time. The gas ahead must not waft.
                                                </p>
                                                <p id="block133">
                                                    Car accidents are indeed like this. People rarely spend much time with high credence that they’re about to get in a car accident. Their probability is low; and then suddenly it jumps wildly, split-second high, before death, or some bang-crunch-jerk, or a gasping near-miss.
                                                </p>
                                                <p id="block134">
                                                    Is AI risk like this too? Doomers sometimes talk this way. You’ll be cruising along. Everything will be looking rosy. The non-doomers will be feeling smug. Then suddenly: bam! <span class="blockquote_ymuvrSv3kcL3cfTv4_1">The nanobots, from the bloodstream, in the parlor, Professor Plum.</span> The clues, that is, didn’t rest on the details. A lot of it was obvious a priori. You should’ve read more LessWrong back in the 2000s. You should’ve looked harder at those <span><span><span><a href="https://twitter.com/ESYudkowsky/status/1500863629490544645">empty strings</a></span></span></span>.
                                                </p>
                                                <p id="block135">
                                                    Now, sometimes this sort of vibe seems to me like it wants to have things both ways. “I shall accept ahead-of-time empirical evidence that I am right; but in the absence of such evidence, I shall remain just as confident.” “My model makes no confident predictions prior to the all-dropping-dead thing – except, that is, the ones that I want to claim credit for after-the-fact.” Here I recall a conversation I overheard back in 2018 about “<span><span><span><a class="ArbitalPreview-link" href="https://arbital.com/p/daemons/">optimization daemons</a></span></span></span>” (now: <span><span><span><a href="https://arxiv.org/abs/1906.01820">mesa-optimizers</a></span></span></span>, <span><span><span><a href="https://arxiv.org/abs/2210.01790">goal mis-generalization</a></span></span></span>, etc) in which a worrier said something like: “I will accept empirical arguments for concern, but only a priori arguments for comfort.” It was an offhand remark, but still: <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence">not how it works</a></span></span>.
                                                </p>
                                                <p id="block136">
                                                    However: I do think, unfortunately, there are risks of gas that doesn’t waft well; “<span><span><span><a href="https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_">King Lear problems</a></span></span></span>”; risks of <span><span><span><a href="https://www.planned-obsolescence.org/the-training-game/">things looking fairly fine, right before they are very non-fine indeed</a></span></span></span>. But not all the gas is like this. We should expect to get clues (indeed, we should <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment"><i>dig hard for them</i></a></span></span>)<i>.</i> So we should expect, at some point, to start updating in the right direction. But I think it’s an open question how the sequencing here works, and it’ll depend on the details driving your particular view. In general, though, if you’re currently at more-likely-than-not on hitting an AGI waterfall sometime in the coming decades, but not certain, then prima facie, and even if your p(doom) is low, that’s reason to expect to get more worried as that soothing sign – “AI winter,” “It was all fake somehow” (classic sign) – fails to appear.
                                                </p>
                                                <p id="block137">
                                                    That said, even if you’re getting predictably <i>more</i> worried, there are still Bayesian constraints on <i>how much</i>. In the waterfall case, you go up 2%; in the car case, something tiny. So if you’re finding yourself, once you don’t see the sign, jumping to 50% on “death by big waterfall” – well, hmm, according to your previous views, you’re saying that you’re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying? Is the evidence you’re seeing now, re: big-waterfall, actually surprising relative to what you expected before? Looks a lot like the predicted river to me. Looks, indeed, “just like they said.” Or did your gut, maybe, not really believe …
                                                </p>
                                                <h2 id="8__Will_the_next_president_be_a_potato_">
                                                    <strong>8. Will the next president be a potato?</strong>
                                                </h2>
                                                <p id="block138">
                                                    OK, that was a bunch of stuff about basic Bayesian belief dynamics. And armed with this sort of relatively crisp and simple model, it can be easy to start drawing strong conclusions about how you, with your mushy monkey brain, should be reasoning in the practice, and what sorts of numbers should be coming out of your mouth, when you make number-noises.
                                                </p>
                                                <p id="block139">
                                                    But the number-noise game takes taste. It’s a new game. We’re still learning how to play well, and productively. And I think we should be wary of possible distortions, especially with respect to small-probabilities.
                                                </p>
                                                <p id="block140">
                                                    Consider, for example, the following dialogue:
                                                </p>
                                                <blockquote id="block141">
                                                    <p id="block142">
                                                        <i>Them</i>: What’s your probability that the next president is a potato?
                                                    </p>
                                                    <p id="block143">
                                                        <i>You</i>: What?
                                                    </p>
                                                    <p id="block144">
                                                        <i>Them</i>: A potato. Like, a normal potato. Up there getting inaugurated and stuff.
                                                    </p>
                                                    <p id="block145">
                                                        <i>You</i>: Umm, very low?
                                                    </p>
                                                    <p id="block146">
                                                        <i>Them</i>: Say a number!
                                                    </p>
                                                    <p id="block147">
                                                        <i>You</i>: [blank stare]
                                                    </p>
                                                    <p id="block148">
                                                        <i>Them</i>: You are a Bayesian and must have a number, and I demand that you produce it. Just literally say any number and I will be satisfied.
                                                    </p>
                                                    <p id="block149">
                                                        <i>You</i>: Fine. One in 10^50. &#160;
                                                    </p>
                                                    <p id="block150">
                                                        <i>Them</i>: What? Really? Wow that’s so stupid. I can’t believe you said that.
                                                    </p>
                                                    <p id="block151">
                                                        <i>You</i>: Actually, let’s say one in 10^40.
                                                    </p>
                                                    <p id="block152">
                                                        <i>Them</i>: Wait, your number was more than a billion times lower a second ago. If you were at one in 10^50 a second ago, you should’ve been at less than one-in-a-billion that you’d ever move this high. Is the evidence you’ve got since then so surprising? Clearly, you are a bad Bayesian. And I am clever!
                                                    </p>
                                                    <p id="block153">
                                                        <i>You</i>: This is a dumb thing.
                                                    </p>
                                                </blockquote>
                                                <p id="block154">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh 1019w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezpszxdmdzq3uy3p8qhi 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rpg5vqlsuzidqen5c4di 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qc9y8vkry2clju0stdwh 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zyz51cimtve586t3zhuu 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mwkiyj92xdccwxuj86ot 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vgf2m89mvbsmu2mvb7ey 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/x2h0uk8rtwyxbmai7hnz 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sg2q1gjcssoc8yjl6bo7 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pacecvldaxuy90uwyzir 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ypgoixgh1fm3krr8eijy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdfuisvwrauqypkabjsq 1156w" />
                                                </p>
                                                <p id="block155">
                                                    <i>Not like this: a normal potato.</i>
                                                </p>
                                                <p id="block156">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/h8ciyvdtf5bl1jlfhnoy" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xwhen7t5tpcuk3ljla0p 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/j344jhvcsibpfq3feglm 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvsgwlqjofhfhzgkbbvp 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qvhouh0jdskz5w5fddeb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/slgjc8ozooac97bov1v0 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/r94awwenrlstpysths62 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kz7lufvzxvkannecezen 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycxedzguhkdur5bbniyl 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gpsgvjn4rhx1viuk60kq 982w" />
                                                </p>
                                                <p id="block157">
                                                    <i>Closer…</i>
                                                </p>
                                                <p id="block158">
                                                    The “them” vibe, here, seems dubiously helpful. And in particular, in this case, it’s extra not-helpful to think of “you” as changing your probabilities, from one second to the next, by updating some fully-formed probability distribution over Potato-2024, complete with expected updates based on all the possible next-thoughts-you-could-think, reactions “them” might have, and so on. That’s, just, not the right way to understand what’s going on with the fleshy creatures described in this dialogue. And in general, it can be hard to have intuitions about <span><span><span><a href="https://markxu.com/strong-evidence">strong evidence</a></span></span></span>, and extreme numbers make human-implemented Bayesian especially brittle. &#160;
                                                </p>
                                                <p id="block159">
                                                    Now, to be clear: I think that debates about the rough quantitative probability of AI doom are worth engaging in, and that they are in fact (unfortunately) very different from debates about Potato-2024. Still, though, that old lesson looms: do not confuse your abstract model of yourself with yourself. The map is never the territory; but especially not when you’re imagining a map that would take a <span><span><span><a href="https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#i-the-universal-distribution">hyper-computer to compute</a></span></span></span>. Fans of basic Bayesianism, and of number-noises, are <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking">well-aware of this</a></span></span>; but the right dance, in practice, remains an open question.
                                                </p>
                                                <p id="block160">
                                                    As an example of a distortion I worry about with respect to the previous discussion: in practice, lots of people (myself included – but see also Christiano <span><span><span><a href="https://ai-alignment.com/my-views-on-doom-4788b1cd0c72">here</a></span></span></span>) report volatility in their degree of concern about p(doom). Some days, I feel like “man, I just can’t see how this goes well.” Other days I’m like: “What was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific…”
                                                </p>
                                                <p id="block161">
                                                    Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you’ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you’re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you’re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<span class="footnote-reference" role="doc-noteref" id="fnref7yx0orvjyje"><sup><span><span><a href="#fn7yx0orvjyje">[17]</a></span></span></sup></span>
                                                </p>
                                                <p id="block162">
                                                    <span class="blockquote_wzS25hBpMMkeigqNh_1">Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out</span> (e.g., you can be at an <i>average</i> of 10% doom conditional on GPT-6 being “scary smart,” but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you’ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you’re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you’re not tracking – for example, the rest of the world starting to agree.<span class="footnote-reference" role="doc-noteref" id="fnref72q8oh08rre"><sup><span><span><a href="#fn72q8oh08rre">[18]</a></span></span></sup></span>
                                                </p>
                                                <p id="block163">
                                                    And there are other more familiar risks of just getting even the basic Bayesianism wrong. Maybe, for example, you notice that your beliefs have been trending in a certain direction. Trump keeps moving up in the polls, say. Now you’re at like 95% on Trump win. And you read a tweet like <span><span><span><a href="https://twitter.com/NPCollapse/status/1626854680260231169">Connor Leahy’s</a></span></span></span>, below, telling you to “just update all the way, bro” and so you decide, shit, I’ll just go 100%, and assume that Trump <i>will</i> win. Wouldn’t want to predictably update later, right?
                                                </p>
                                                <p id="block164">
                                                    <img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn" alt="" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn 817w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/i4j91yqvpuobz2rqw868 239w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/squfak3qevyosnflwfat 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cbt8xvuobl9fjnxmnnts 1226w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfbghfvnj0hz0yr5y68f 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezw8jwyifhypezexpmgm 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ad6hz0opqqfrynr3obos 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noszolhwi0fj9edhok5p 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vdss52lqyqxmauxbljp1 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kyvvrkntm51gcrytmqme 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdygqwvjmkb0vjcy7sgq 1398w" />
                                                </p>
                                                <p id="block165">
                                                    Or maybe you hear some <span><span><span><a href="https://www.facebook.com/yudkowsky/posts/10160260422389228">prominent doomer proclaiming that “sane people with self-respect” don’t update predictably</a></span></span></span>, without clarifying about “in expectation” despite <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence">definitely knowing about this</a></span></span>, and so you assume you must be unsane and self-hating. Or maybe you think that if you do update predictably, it should at least be in the direction of your currently-predicted truth, and you forget about cases like the waterfalls above.
                                                </p>
                                                <p id="block166">
                                                    In general, this stuff can get tricky. We should be careful, and not self-righteous, even when the math itself is clear. &#160;
                                                </p>
                                                <h2 id="9__Just_saying__oops_">
                                                    9. Just saying “oops”
                                                </h2>
                                                <p id="block167">
                                                    I also want to add a different note of caution, about not letting consistency, or your abstract picture of what “good Bayesianism” looks like, get in the way of updating as fast as possible to the right view, whatever that is.
                                                </p>
                                                <p id="block168">
                                                    Thus, for example, maybe you tweeted a bunch in the past re: “no way” on AI risk, and acted dismissive about it. Maybe, even, you’re someone like David Thorstad, and you were kind enough to quantify your dismissiveness with some very-low number.
                                                </p>
                                                <p id="block169">
                                                    And let’s say, later, your gut starts twinging. Maybe you see some scary demo of deceptiveness or power-seeking. Maybe you don’t like the look of all those increasingly-automated, AI-run wet-labs. Maybe it all just starts changing too fast, and it feels too frenetic and out of control, and do we even understand how these systems are working? Maybe it’s something about those new drones. &#160;
                                                </p>
                                                <p id="block170">
                                                    It might be tempting, here, to let your previous skepticism drag your new estimates downwards – including on the basis of the sorts of dynamics discussed above. Maybe, for example, if you had David Thorstad’s number, you’re tempted to move from .00002% to something like, hmm, 20%? But you say to yourself “wait, have I really gotten <i>such strong evidence</i> since my previous estimate? Have I been <i>so surprised</i> by the demos, and the drones, and the wet-labs? Apparently, I’m moving to a number I should’ve been less than one-in-a-million I’d ever end up at. By my previous lights, isn’t that unlikely to be the right move?”
                                                </p>
                                                <p id="block171">
                                                    But the thing is: it’s possible that your previous estimate was just … way too low. And more (gasp), that it didn’t come with some well-formed probability distribution over your future estimates, either. We should be wary, in general, of taking our previous (or our current) Bayesian rigor too seriously. Should “you,” above, refrain from changing her potato-2024 estimate quickly as she thinks about it more, on grounds that it would make her two-seconds-ago self’s Bayesianism look bad? Best to just get things right.
                                                </p>
                                                <p id="block172">
                                                    Of course, it may be that your previous self was tracking some sort of evidence that you’re losing sight of, now. It may be that your gut is skittish. You should try to learn from your previous self what you can. But you should try, I suspect, to learn harder from the actual world, there in front of you.
                                                </p>
                                                <p id="block173">
                                                    Here, to be clear, I’m partly thinking about myself, and my own mistakes. I said 5% in 2021. I more than doubled my estimate soon after. &#160;By basic Bayes, I should’ve been less than 50%, in 2021, that this would happen. Did I really get sufficiently worrying evidence in the interim to justify such a shift? Maybe. But alternatively: whatever, I was just wrong. Best to just say oops, and to try to be righter.
                                                </p>
                                                <p id="block174">
                                                    I’m focusing on people with very low estimates on doom, here, because they tend to be more common than the converse. But everything I’m saying here holds for people with low estimates on non-doom, too. If you’re such a person, and you see signs of hope later, don’t be attached to your identity as a definitely-doomer, or to the Bayesian rigor of the self that assumed this identity. Don’t practice your pessimism over-hard. You might miss the thing that saves your life.
                                                </p>
                                                <p id="block175">
                                                    Really, though, I suspect that respect for your previous self’s Bayesianism is not the main barrier to changing our minds fast enough. Rather, the barriers are more social: embarrassment stuff, tribal stuff, status stuff, and so on. I think we should try to lower such barriers where possible. We should notice that people were wrong; but we should not make fun of them for changing their minds – quite the contrary. Scout mindset is hard enough, and the stakes are too high.
                                                </p>
                                                <h2 id="10__Doing_enough">
                                                    10. Doing enough
                                                </h2>
                                                <blockquote id="block176">
                                                    <p id="block177">
                                                        <i>"I imagine death so much it feels more like a memory…"</i>
                                                    </p>
                                                    <p id="block178">
                                                        <i>-</i> <span><span><span><a href="https://youtu.be/BQ1ZwqaXJaQ?t=97"><i>Hamilton</i></a></span></span></span>
                                                    </p>
                                                </blockquote>
                                                <blockquote id="block179">
                                                    <p id="block180">
                                                        <i>“When my time is up, have I done enough?”</i>
                                                    </p>
                                                    <p id="block181">
                                                        <i>-</i> <span><span><span><a href="https://www.youtube.com/watch?v=_gnypiKNaJE"><i>Eliza</i></a></span></span></span>
                                                    </p>
                                                </blockquote>
                                                <p id="block182">
                                                    I’ll close by noting a final sort of predictable update. It’s related to the scans thing.
                                                </p>
                                                <p id="block183">
                                                    There’s <span><span><span><a href="https://www.youtube.com/watch?v=W9vj2Wf57rQ">a scene</a></span></span></span> at the end of <i>Schindler’s List</i>. World War II is over. Schindler has used his money to save more than 1,100 lives from the holocaust. As the people he has saved say goodbye, Schindler breaks down:
                                                </p>
                                                <blockquote id="block184">
                                                    <p id="block185">
                                                        I could have got more out. I could have got more. I don’t know. If I’d just… I could have got more… I threw away so much money. You have no idea… I didn’t do enough… This car. Goeth would have bought this car. Why did I keep the car? Ten people right there. Ten people. Ten more people. This pin. Two people. This is gold. Two more people. He would have given me two for it, at least one. One more person. A person, Stern. For this. I could have gotten one more person… and I didn’t.
                                                    </p>
                                                </blockquote>
                                                <p id="block186">
                                                    Now, we need to be careful here. It’s easy for the sort of stuff I’m about to say to prompt extreme and unbalanced and unhealthy relationships to stuff that matters a lot. In particular, if you’re tempted to be in some “emergency” mode about AI risk (or, indeed, about some other issue), and to start burning lots of resources for the sake of doing everything you can, I encourage you to read <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai">this article</a></span></span>, together with <span><span><a class="CommentLinkPreviewWithComment-link" href="/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai?commentId=Htf2v79w5QoQJbysS#comments">this comment</a></span></span> about memetic dynamics that can amplify false emergencies and discourage clear thinking.
                                                </p>
                                                <p id="block187">
                                                    Still, still. There’s a possible predictable update here. If this AI stuff really happens, and the alignment stuff is looking rough, there is a way we will each feel about what we did with the time we had. How we responded to what we knew. What role we played. Which directions we pointed the world, or moved it. How much we left on the field.
                                                </p>
                                                <p id="block188">
                                                    And there is a way we will feel, too, about subtler things. About what sorts of motivations were at play, in how we oriented towards the issue. About the tone we took on twitter. About the sort of <span><span><span><a href="https://joecarlsmith.com/2022/12/23/on-sincerity">sincerity</a></span></span></span> we had, or didn’t have. One thing that stayed with me from <i>Don’t Look Up</i> is the way the asteroid somehow slotted into the world’s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word “shame.” Death does not discriminate between the sinners and the saints. But I do actually think it’s worth talk of dignity.
                                                </p>
                                                <p id="block189">
                                                    And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I’m particularly interested in “let’s see if we can actually do this.” Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.
                                                </p>
                                                <p id="block190">
                                                    And if we do this, then there is a way we will feel when it’s done. I have a friend who sometimes talks about what he wants to tell his grandchildren he did, during the years leading up to AGI. It’s related to that thing about history, and who its eyes are on. We shouldn’t need people to tell our stories; but as far as I can tell, if he ever has grandchildren, they should be proud of him. May he sit, someday, under his own vine and fig tree.
                                                </p>
                                                <p id="block191">
                                                    Of course, there is also a way we will feel if AGI happens, but the problem was unreal, or not worth worrying about. There are <span><span><span><a href="https://www.planned-obsolescence.org/the-costs-of-caution/">costs of caution</a></span></span></span>. And of course, there is a way we will feel if all this AGI stuff was fake after all, and all that time and money and energy was directed at a fantasy. You can talk about “reasonable ex ante,” but: will it have been reasonable? If this stuff is a fantasy, I suspect it is a fantasy connected with our flaws, and that we will have been, not innocently mistaken, but actively foolish, and maybe worse. Or at least, I suspect this of myself.
                                                </p>
                                                <p id="block192">
                                                    Overall, then, there are lots of different possible futures here. As ever, the Bayesian tries to live in all of them at once. Still: if, indeed, we are running out of time, and there is a serious risk of everyone dying, it seems especially worth thinking ahead to hospitals and scans; to what we will learn, later, about “enough” and “not enough,” about “done” and “left undone.” Maybe there will be no history to have its eyes on us – or at least, none we would honor. But we can look for ourselves. &#160;&#160;
                                                </p>
                                                <ol class="footnotes" role="doc-endnotes">
                                                    <li class="footnote-item" role="doc-endnote" id="fn0g9sn95iiqe">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref0g9sn95iiqe">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block194">
                                                                To be clear: there are lots of other risks from AI, too. And the basic dynamics at stake in the essay apply to your probabilities on any sorts of risks. But I want to focus on existential risk from misalignment, here, and I want the short phrase “AI risk” for the thing I’m going to be referring to repeatedly.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn048nj5dhr9di">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref048nj5dhr9di">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block196">
                                                                Though, the specific numbers here can matter – and there are some cases where despite having low probabilities on doom now, you can predict ahead of time that you’ll be at least somewhat more worried later (though, there are limits to how much). More below.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnqnj4snix8n">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefqnj4snix8n">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block198">
                                                                Though with respect to AI doom, not risk free – see <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of">here</a></span></span> for some mental health resources.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnz4yjec5asvf">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefz4yjec5asvf">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block200">
                                                                Hopefully not more literally similar. But: a new thing-not-imagined-very-well.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn1dgio52qsrw">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref1dgio52qsrw">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block202">
                                                                Modulo some futurisms. Including, importantly, ones predictably at stake in AI progress.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn4m2cfburfm7">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref4m2cfburfm7">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block204">
                                                                “In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.”
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn4y75ebmy7eh">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref4y75ebmy7eh">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block206">
                                                                Thanks to Katja Grace for discussion.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn5gj8wfkheeo">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref5gj8wfkheeo">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block208">
                                                                Some forecasts have self-fulfilling elements, especially with respect to Moloch-like problems. And there are questions about e.g. internet text increasing the likelihood of AIs acting out the role of the scary-AI.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnh3ar4lqw084">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefh3ar4lqw084">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block210">
                                                                See e.g. Scott Alexander <span><span><span><a href="https://astralcodexten.substack.com/p/mantic-monday-31422">here</a></span></span></span>. Some of <span><span><span><a href="https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably#fn6am2fn0yyve">Yudkowsky’s public comments</a></span></span></span> suggest this model as well, though his original discussion of “<span><span><a class="PostLinkPreviewWithPost-link" href="/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence">conservation of expected evidence</a></span></span>” does not.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn4ohd2xn7yql">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref4ohd2xn7yql">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block212">
                                                                Here I’m indebted to discussion from <span><span><span><a href="https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably">Greg Lewis</a></span></span></span> and <span><span><a class="PostLinkPreviewWithPost-link" href="/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence">Abram Demski</a></span></span>.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnc1l5t3jttlk">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefc1l5t3jttlk">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block214">
                                                                .99*1 + .01*0 = .99.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fno3exn1kvfm">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefo3exn1kvfm">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block216">
                                                                Or put another way: you want to find the area that red occupies, which is the area of the first, smaller red box, plus the area of the bigger red box. Each box occupies a percentage of the area of a “column” (combination of white box and red box) associated with a hypothesis about GPT-6. So to find the area of a given red box, you take the area of the column it’s in (that is, the probability on the relevant hypothesis about GPT-6), and multiply that by the percentage of that column that is red (e.g., the probability of doom conditional on that hypothesis). Then you add up the areas of the red boxes.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnt4taauzgccn">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnreft4taauzgccn">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block218">
                                                                Thanks to Daniel Kokotajlo for highlighting some of these dynamics to me years ago. See also his review of my power-seeking AI report <span><span><span><a href="https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#heading=h.e9o5m3fab0ua">here</a></span></span></span>.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnv5zb8pjk25">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefv5zb8pjk25">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block220">
                                                                I added an edit to this effect.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fnu9z9f1zd3ci">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnrefu9z9f1zd3ci">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block222">
                                                                Thanks to Katja Grace for discussion here.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn6gbdnzllii">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref6gbdnzllii">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block224">
                                                                Here I’m inspired by some comments from Richard Ngo.
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn7yx0orvjyje">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref7yx0orvjyje">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block226">
                                                                Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?
                                                            </p>
                                                        </div>
                                                    </li>
                                                    <li class="footnote-item" role="doc-endnote" id="fn72q8oh08rre">
                                                        <span class="footnote-back-link"><sup><strong><span><a href="#fnref72q8oh08rre">^</a></span></strong></sup></span>
                                                        <div class="footnote-content">
                                                            <p id="block228">
                                                                Again, thanks to Katja for pointing to this dynamic.
                                                            </p>
                                                        </div>
                                                    </li>
                                                </ol>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="PostsPagePostFooter-footerTagList">
                                        <span class="FooterTagList-root"><span class="FooterTag-root FooterTag-core"><a href="/tag/world-modeling"><span class="FooterTag-name">World Modeling</span><span class="FooterTag-score">1</span></a></span><span class="FooterTag-root FooterTag-core"><a href="/tag/ai"><span class="FooterTag-name">AI</span><span class="FooterTag-score">1</span></a></span><a class="FooterTagList-postTypeLink" href="/recommendations">
                                        <div class="FooterTagList-frontpageOrPersonal">
                                            <span class="LWTooltip-root">Curated</span>
                                        </div></a></span>
                                    </div>
                                    <div class="PostsPagePostFooter-footerSection">
                                        <div class="PostsPagePostFooter-voteBottom">
                                            <div class="PostsVote-voteBlock">
                                                <div class="PostsVote-upvote" title="You must be logged in and have 1 karma to vote">
                                                    <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-up VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button>
                                                </div>
                                                <div class="PostsVote-voteScores">
                                                    <div title="100 Votes">
                                                        <h1 class="Typography-root Typography-headline PostsVote-voteScore">
                                                            273
                                                        </h1>
                                                    </div>
                                                </div>
                                                <div class="PostsVote-downvote" title="You must be logged in and have 1 karma to vote">
                                                    <button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-down VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="PingbacksList-root">
                                        <div class="PingbacksList-title">
                                            <span class="LWTooltip-root"><span>Mentioned in</span></span>
                                        </div>
                                        <div class="PingbacksList-list">
                                            <div>
                                                <div class="Pingback-root">
                                                    <span><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">78</span></span><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/v4c7eBHESey8Deyra/to-predict-what-happens-ask-what-happens"><span><span class="">To Predict What Happens, Ask What Happens</span></span></a></span></span></span>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="Pingback-root">
                                                    <span><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo Pingback-karma"><span class="LWTooltip-root">18</span></span><span class="PostsTitle-root PostsTitle-wrap"><span><a href="/posts/9fjSi2KzyzXAQxuza/alignment-goals-and-the-gut-head-gap-a-review-of-ngo-et-al"><span><span class="">Alignment, Goals, and The Gut-Head Gap: A Review of Ngo. et al.</span></span></a></span></span></span>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <div class="PostsPage-commentsSection">
                                    <div class="CommentsListSection-root CommentsListSection-maxWidthRoot">
                                        <div id="comments"></div>
                                        <div id="posts-thread-new-comment" class="CommentsListSection-newComment">
                                            <div class="CommentsListSection-newCommentLabel">
                                                <span>New Comment</span>
                                            </div>
                                            <div class="CommentsNewForm-root">
                                                <div class="CommentsNewForm-form">
                                                    <div>
                                                        <form class="vulcan-form document-new" id="new-comment-form" name="new-comment-form">
                                                            <div class="FormErrors-root form-errors"></div>
                                                            <div class="form-input input-contents form-component-EditorFormComponent">
                                                                <div class="EditorFormComponent-root">
                                                                    <div>
                                                                        <div class="EditorFormComponent-editor EditorFormComponent-commentBodyStyles ContentStyles-base content ContentStyles-commentBody">
                                                                            <div class="EditorFormComponent-commentEditorHeight EditorFormComponent-ckEditorStyles">
                                                                                <div>
                                                                                    <div class="ck-blurred ck ck-content ck-editor__editable ck-rounded-corners ck-editor__editable_inline" lang="en" dir="ltr" role="textbox" aria-label="Rich Text Editor, main" contenteditable="true" xml:lang="en">
                                                                                        <p class="ck-placeholder" data-placeholder="Write here. Select text for formatting options. We support LaTeX: Cmd-4 for inline, Cmd-M for block-level (Ctrl on Windows). You can switch between rich text and markdown in your user settings.">
                                                                                            <span><br data-cke-filler="true" /></span>
                                                                                        </p>
                                                                                    </div>
                                                                                </div>
                                                                            </div>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                            <div class="CommentsNewForm-submit">
                                                                <span><button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat CommentsNewForm-formButton CommentsNewForm-submitButton" type="submit" id="new-comment-submit"><span class="MuiButton-label">Submit</span></button></span>
                                                            </div>
                                                        </form>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="CommentsListMeta-root">
                                            <span class="Typography-root Typography-body2 CommentsListSection-inline"><span>20 comments<!-- -->, sorted by</span></span>
                                            <div class="InlineSelect-root">
                                                <span class="Typography-root Typography-body2 CommentsListSection-inline"><a class="InlineSelect-link">top scoring</a></span>
                                            </div><span class="Typography-root Typography-body2 CommentsListSection-clickToHighlightNewSince">Click to highlight new comments since: <a class="CommentsListSection-button"><span>Today at 12:59 PM</span></a></span>
                                        </div>
                                        <div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="DEdj9FexMb5wXhGbP">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/romeostevensit">romeostevensit</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=DEdj9FexMb5wXhGbP"><span class="LWTooltip-root">3mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">47</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">33</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                Schindler had a concrete thing he was able to do. He had a money-&gt;people pipeline. I think most of the ways rationalists are feeling smug about being ahead of the curve here boils down to an error that we are still making: okay you've made the update, now how does it propagate through the world model to generate meaningfully different actions? Who has taken action? Has anyone who has taken action talked about it anywhere? Do any of the proposed or taken actions look remotely helpful?
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node" id="SrBCxKcAcK7gLTQTd">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/matthew-barnett">Matthew Barnett</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=SrBCxKcAcK7gLTQTd"><span class="LWTooltip-root">3mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">34</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">11</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <blockquote>
                                                                                <p>
                                                                                    But depending on what you count: we had scaling laws for deep learning back in <span><span><span><a href="https://arxiv.org/abs/1712.00409">2017</a></span></span></span>, or at least <span><span><span><a href="https://arxiv.org/abs/2001.08361">2020</a></span></span></span>. I know people who were really paying attention; who really saw it; who really bet.
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                Interestingly, I feel like the evidence we got from public info about scaling laws at the time was consistent with long timelines. In roughly 2018-2021, I remember at least a few people making approximately the following argument:
                                                                            </p>
                                                                            <p>
                                                                                (1) OpenAI came out with <span><span><span><a href="https://openai.com/research/ai-and-compute">a blog post</a></span></span></span> in 2018 claiming that training compute was doubling every 3.4 months.&#160;
                                                                            </p>
                                                                            <p>
                                                                                (2) Extrapolating this trend indicates that training runs will start costing around $1 trillion dollars by 2025.
                                                                            </p>
                                                                            <p>
                                                                                (3) Therefore, this trend cannot be sustained beyond 2025. Unless AGI arrives before 2025, we will soon enter an AI winter.
                                                                            </p>
                                                                            <p>
                                                                                However, it turned out that OpenAI was <span><span><span><a href="https://arxiv.org/abs/2202.05924">likely wrong</a></span></span></span> about the compute trend, and training compute was doubling roughly every 6-10 months, not every 3.4 months. And moreover the Kaplan et al. scaling laws <span><span><span><a href="https://arxiv.org/abs/2203.15556">turned out to be inaccurate too</a></span></span></span>. This was a big update within the scaling hypothesis paradigm, since it demonstrated that we were getting better returns to compute than we thought.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="CommentsNode-children">
                                                        <div class="CommentsNode-parentScroll"></div>
                                                        <div>
                                                            <div class="comments-node comments-node-even CommentFrame-node CommentFrame-child" id="kyDRpsdzdfspzJ4Rn">
                                                                <div>
                                                                    <div class="CommentsItem-root recent-comments-node">
                                                                        <div class="CommentsItem-postTitleRow"></div>
                                                                        <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                            <div class="CommentsItemMeta-root">
                                                                                <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/daniel-kokotajlo">Daniel Kokotajlo</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=kyDRpsdzdfspzJ4Rn"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">4</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">3</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                            </div>
                                                                            <div>
                                                                                <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                                    <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                                        <p>
                                                                                            It was indeed a good point, at the time, but still, the people who predicted short timelines were right and deserve credit.
                                                                                        </p>
                                                                                    </div>
                                                                                </div>
                                                                            </div>
                                                                            <div class="CommentsItem-bottom">
                                                                                <div>
                                                                                    <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                                </div>
                                                                            </div>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsNode-children">
                                                                    <div class="CommentsNode-parentScroll"></div>
                                                                    <div>
                                                                        <div class="comments-node comments-node-odd CommentFrame-node CommentFrame-child CommentFrame-answerLeafComment" id="qyiFweEAzjN6uZ59C">
                                                                            <div>
                                                                                <div class="CommentsItem-root recent-comments-node">
                                                                                    <div class="CommentsItem-postTitleRow"></div>
                                                                                    <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                                        <div class="CommentsItemMeta-root">
                                                                                            <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/matthew-barnett">Matthew Barnett</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=qyiFweEAzjN6uZ59C"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                            <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">0</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">3</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                            <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                            <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                            <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                            <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                                        </div>
                                                                                        <div>
                                                                                            <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                                                <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                                                    <p>
                                                                                                        Definitely. I don't think it makes much sense to give people credit for being wrong for legible reasons.
                                                                                                    </p>
                                                                                                </div>
                                                                                            </div>
                                                                                        </div>
                                                                                        <div class="CommentsItem-bottom">
                                                                                            <div>
                                                                                                <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                                            </div>
                                                                                        </div>
                                                                                    </div>
                                                                                </div>
                                                                            </div>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="jByH6QkCR9PuQiNQB">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/habryka4">habryka</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=jByH6QkCR9PuQiNQB"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">27</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">20</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                This was a great post. I rarely leave comments with just praise, but it felt appropriate to leave a comment expressing my appreciation for this one, since I do expect to come back to it quite a bit, and just a strong-upvote didn't feel sufficient.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="S624bktj7cShjwhBX">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/bhalperin">basil.halperin</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=S624bktj7cShjwhBX"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">8</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">2</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                This isn't REALLY the point of your (nice) piece, but the title provides an opportunity to plant a flag and point out: <i>"predictably updating" is not necessarily bad or irrational.</i> Unfortunately I don't have time to write up the full argument right now, hopefully eventually, but, TLDR:
                                                                            </p>
                                                                            <ul>
                                                                                <li>Bayesian rational learning about a process can be very slow...
                                                                                </li>
                                                                                <li>...which leads to predictable updating...
                                                                                </li>
                                                                                <li>...especially when the long-run dynamics underlying the process are slow-moving.
                                                                                </li>
                                                                            </ul>
                                                                            <p>
                                                                                In macroeconomics, this has recently been discussed in detail by <span><span><span><a href="https://eml.berkeley.edu/~enakamura/papers/learning.pdf">Farmer, Nakamura, and Steinsson</a></span></span></span> in the context of "<span><span><span><a href="https://twitter.com/TheStalwart/status/1136677587328147457">medusa charts</a></span></span></span>" that seem to show financial markets 'predictably updating' about interest rates.
                                                                            </p>
                                                                            <p>
                                                                                But I imagine this issue has been discussed elsewhere -- this is not an 'economic phenomenon' per se, it's just a property of Bayesian updating on processes with a slow-moving nonstationary component.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="ymuvrSv3kcL3cfTv4">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/nisan">Nisan</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=ymuvrSv3kcL3cfTv4"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">8</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">8</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <blockquote>
                                                                                <p>
                                                                                    The nanobots, from the bloodstream, in the parlor, Professor Plum.
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                You could have written <em>Colonel Mustard</em>!
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node" id="wzS25hBpMMkeigqNh">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/drake-thomas">Drake Thomas</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=wzS25hBpMMkeigqNh"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">7</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <blockquote>
                                                                                <p>
                                                                                    .00002% — that is, one in five hundred thousand
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                0.00002 would be one in five hundred thousand, but with the percent sign it's one in fifty million.
                                                                            </p>
                                                                            <blockquote>
                                                                                <p>
                                                                                    Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                I agree with this as far as the example given, but I want to push back on oscillation (in the sense of regularly going from one estimate to another) being Bayesian. In particular, the odds you should put on assigning 20% in the future, then 30% after that, then 20% again, then 30% again, and so on for ten up-down oscillations, shouldn't be more than half a percent, because each 20 -&gt; 30 jump can be at most 2/3 probable and each 30 -&gt; 20 jump at most 7/8 (and&#160;<span><span class="mjpage"><span class="mjx-chtml"><span class="mjx-math" aria-label="\left(\frac23\cdot\frac78\right)^{10}\approx0.0047"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-msubsup"><span class="mjx-base"><span class="mjx-mrow"><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">(</span></span><span class="mjx-mfrac"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.372em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">2</span></span></span><span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.686em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">3</span></span></span></span></span><span class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.004em; padding-bottom: 0.298em;">⋅</span></span><span class="mjx-mfrac MJXc-space2"><span class="mjx-box MJXc-stacked" style="width: 0.495em; padding: 0px 0.12em;"><span class="mjx-numerator" style="font-size: 70.7%; width: 0.7em; top: -1.404em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">7</span></span></span><span class="mjx-denominator" style="font-size: 70.7%; width: 0.7em; bottom: -0.687em;"><span class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">8</span></span></span></span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-size2-R" style="padding-top: 0.961em; padding-bottom: 0.961em;">)</span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 1.276em; padding-left: 0px; padding-right: 0.071em;"><span class="mjx-texatom" style=""><span class="mjx-mrow"><span class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">10</span></span></span></span></span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.225em; padding-bottom: 0.298em;">≈</span></span><span class="mjx-mn MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.372em; padding-bottom: 0.372em;">0.0047</span></span></span></span></span></span></span>
                                                                            </p>).&#160;
                                                                            <p>
                                                                                So it's fine to think that you've got a decent chance of having all kinds of credences in the future, but thinking "I'll probably feel one of two ways a few times a week for the next year" is not the kind of belief a proper Bayesian would have. (Not that I think there's an obvious change to one's beliefs you should try to hammer in by force, if that's your current state of affairs, but I think it's worth flagging that something suboptimal is going on when this happens.)
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                    <div class="CommentsNode-children">
                                                        <div class="CommentsNode-parentScroll"></div>
                                                        <div>
                                                            <div class="comments-node comments-node-even CommentFrame-node CommentFrame-child CommentFrame-answerLeafComment" id="NiCwkuexCZozbdxeJ">
                                                                <div>
                                                                    <div class="CommentsItem-root recent-comments-node">
                                                                        <div class="CommentsItem-postTitleRow"></div>
                                                                        <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                            <div class="CommentsItemMeta-root">
                                                                                <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/joe-carlsmith">Joe Carlsmith</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=NiCwkuexCZozbdxeJ"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">2</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                                <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                                <path fill="none" d="M0 0h24v24H0z"></path>
                                                                                <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                            </div>
                                                                            <div>
                                                                                <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                                    <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                                        <p>
                                                                                            Re: "0.00002 would be one in five hundred thousand, but with the percent sign it's one in fifty million." -- thanks, edited.&#160;
                                                                                        </p>
                                                                                        <p>
                                                                                            Re: volatility -- thanks, that sounds right to me, and like a potentially useful dynamic to have in mind.&#160;
                                                                                        </p>
                                                                                    </div>
                                                                                </div>
                                                                            </div>
                                                                            <div class="CommentsItem-bottom">
                                                                                <div>
                                                                                    <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                                </div>
                                                                            </div>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="cQHsRg5rH8u9jYmy4">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/logan-zoellner">Logan Zoellner</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=cQHsRg5rH8u9jYmy4"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">5</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">3</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <blockquote>
                                                                                <p>
                                                                                    you should expect to update in the direction of the truth as the evidence comes in<br />
                                                                                    &#160;
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                I think this was addressed later on, but this is not at all true. &#160;With the waterfall example, every mile that passes without a waterfall you update downwards, but if there's a waterfall at the very end you've been updating against the truth the whole time.
                                                                            </p>
                                                                            <p>
                                                                                Another case. &#160;Suppose you're trying to predict the outcome of a Gaussian random walk. &#160;Each step, you update whatever direction the walk took, but 50% of these steps are "away" from the truth.
                                                                            </p>
                                                                            <blockquote>
                                                                                <p>
                                                                                    you probably shouldn’t be able to predict that this pattern will happen to you in the future.<br />
                                                                                    &#160;
                                                                                </p>
                                                                            </blockquote>
                                                                            <p>
                                                                                Again addressed later on, but one can easily come up with stories in which one predictably updates either "in favor of" or "against" AI doom.
                                                                            </p>
                                                                            <p>
                                                                                Suppose you think there's a 1% chance of AI doom every year, and AI Doom will arrive by 2050 or never. &#160;Then you predictably update downwards every year (unless Doom occurs).
                                                                            </p>
                                                                            <p>
                                                                                Suppose on the other hand that you expect AI to stagnate at some level below AGI, but if AGI is developed then Doom occurs with 100% certainty. &#160;Then each year AI fails to stagnate you update upwards (until AI actually stagnates).
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="FCPacXgjiJdqWBvJd">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/cronodas">CronoDAS</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=FCPacXgjiJdqWBvJd"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">2</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                In practice, the odds of a potato becoming President are "a round-off error away from zero". Human brains don't do floating point arithmetic with enough precision to handle that kind of question. :/
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="a6EuhitsfkjkWTFkz">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/cronodas">CronoDAS</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=a6EuhitsfkjkWTFkz"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">2</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                A silly question, related to the "potato" example:
                                                                            </p>
                                                                            <p>
                                                                                Which is more likely, a potato becoming US President, or a US lottery jackpot worth at least $20 million ends up with the winning number being the numerically smallest possible number?
                                                                            </p>
                                                                            <p>
                                                                                More seriously, just how improbable are "social" impossibilities, such as the potatio example, when you compare them to well-understood things like lottery tickets? I've asked a question like this before, with the social impossibility being the Pope sincerely converting to Islam. <span><span><span><a href="https://youtu.be/KUnA9jb-NpE">Weird shit does happen, though...</a></span></span></span>...
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="GyFgKnDX4qbjtt4sN">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/benito">Ben Pace</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=GyFgKnDX4qbjtt4sN"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">2</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                Curated! I loved a lot of things about this post.&#160;
                                                                            </p>
                                                                            <p>
                                                                                I think the post is doing three things, all of which I like. First, it documents what it was like for Joe as he made substantial updates about the world. Secondly, it exhibits the rationalist practice of explaining what those updates look like using the framework of probabilities, and considering what sorts of updates a rational agent would make in his position, and contrasted that with a helpful explicit model of how a human being would make updates (e.g. using its guts). And third it's a serious and sincere account of something that I care about and Joe cares about. I felt reading this post that I was finally sharing the same mental universe as the author (and likely other people reading the post).
                                                                            </p>
                                                                            <p>
                                                                                There's lots of more specific things to say that I don't have the time to, but I'll say that the paragraph that explains you can't have a &gt;50% chance of your credence later doubling (or a &gt;10% chance of 10x-ing your credence) struck me immediately as a go-to tool I want to add to my mental toolkit for figuring out what probability I assign to a given statement.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="qTKpugYiPsqaagQC2">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/mikola-lysenko">Mikola Lysenko</a></span></span></span><span class="CommentsItemMeta-userMarkers"><span class="LWTooltip-root UserCommentMarkers-iconWrapper"><svg width="16" height="14" viewbox="0 0 16 14" fill="currentColor" xmlns="http://www.w3.org/2000/svg" class="UserCommentMarkers-sproutIcon ForumIcon-root">
                                                                    <path d="M15.3149 0.657185C15.2929 0.557739 15.2031 0.48933 15.1013 0.491891C15.062 0.493247 14.1257 0.529711 12.9513 0.858488C11.3849 1.29711 10.1504 2.0538 9.38118 3.04691C8.61107 4.04121 8.2035 5.43606 8.20215 7.08091C8.20125 8.31314 8.42335 9.24148 8.43284 9.28036C8.45589 9.37529 8.54088 9.44144 8.63776 9.44144C8.64138 9.44144 8.64484 9.44129 8.64846 9.44113C8.68688 9.43918 9.60045 9.38976 10.7516 9.04788C11.8215 8.72995 13.3008 8.08806 14.2716 6.83458C15.0408 5.84147 15.4646 4.45688 15.4975 2.83067C15.5222 1.61156 15.3233 0.695708 15.3148 0.657289L15.3149 0.657185Z" fill="currentColor"></path>
                                                                    <path d="M0.873781 1.87378C0.793923 1.87378 0.749926 1.87559 0.749926 1.87559C0.64837 1.87996 0.563991 1.95605 0.549676 2.05671C0.533705 2.16956 0.174639 4.84 1.65068 6.47104C2.94242 7.89856 5.21902 8.00946 5.88603 8.00946C5.96574 8.00946 6.00973 8.00765 6.00973 8.00765C6.1137 8.00343 6.19944 7.92403 6.21104 7.82051C6.22385 7.70765 6.50637 5.03872 5.03274 3.41042C3.74235 1.9844 1.52318 1.87366 0.873762 1.87366L0.873781 1.87378Z" fill="currentColor"></path>
                                                                    <path d="M13.3482 3.93867C13.3288 3.94922 11.7628 4.79423 10.3666 6.72363C9.35151 8.12689 8.263 10.3494 8.15257 13.5083C7.94042 13.4028 7.71546 13.3248 7.4834 13.2758C7.50705 12.7704 7.55557 12.275 7.62805 11.7907C7.69887 11.3194 7.79259 10.8588 7.90951 10.409C7.91675 10.3816 7.92428 10.3537 7.93151 10.3263C8.32508 8.85269 8.96771 7.49796 9.84105 6.29887C11.3316 4.25178 12.9647 3.37771 13.0336 3.34137C13.1986 3.25443 13.4023 3.31772 13.4892 3.48271C13.576 3.64785 13.5127 3.85157 13.3482 3.93849V3.93867Z" fill="currentColor"></path>
                                                                    <path d="M7.9705 12.1186C7.81304 12.1186 7.67216 12.0078 7.63992 11.8475C6.91123 8.22266 2.65894 4.78127 2.61574 4.74694C2.47033 4.63046 2.44683 4.41801 2.5633 4.27244C2.67977 4.12704 2.89208 4.10339 3.03764 4.21986C3.08329 4.25632 4.16969 5.13038 5.36604 6.48724C6.98597 8.32445 7.97368 10.0832 8.30155 11.7143C8.33831 11.8971 8.21988 12.075 8.03711 12.1118C8.01481 12.1163 7.99236 12.1186 7.97036 12.1186L7.9705 12.1186Z" fill="currentColor"></path></svg></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=qTKpugYiPsqaagQC2"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">2</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                This is the same kind of thing as the Black-Scholes model for options pricing. &#160;As a prediction with a finite time horizon approaches the probability of it updating to a known value converges. &#160;In finance people use this to price derivatives like options contracts, but the same principle should apply to any information.
                                                                            </p>
                                                                            <p>
                                                                                I think you can probably put some numbers on the ideas in this post using roughly the same sort of analysis.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="CRwfoZbHEDFLKFSa6">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/rudi-c">Rudi C</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=CRwfoZbHEDFLKFSa6"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">1</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                Is the basic math necessarily correct?
                                                                            </p>
                                                                            <p>
                                                                                You can expect that on average the expectation (of your future p(doom) change) is positive while the expectation is still zero; It’s likely GPT-6 will be impressive but if it’s not it’s a bigger negative update.
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="jFE3exdnFotmzcLtw">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>-</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/arturs-kanepajs">Artūrs Kaņepājs</a></span></span></span><span class="CommentsItemMeta-userMarkers"><span class="LWTooltip-root UserCommentMarkers-iconWrapper"><svg width="16" height="14" viewbox="0 0 16 14" fill="currentColor" xmlns="http://www.w3.org/2000/svg" class="UserCommentMarkers-sproutIcon ForumIcon-root">
                                                                    <path d="M15.3149 0.657185C15.2929 0.557739 15.2031 0.48933 15.1013 0.491891C15.062 0.493247 14.1257 0.529711 12.9513 0.858488C11.3849 1.29711 10.1504 2.0538 9.38118 3.04691C8.61107 4.04121 8.2035 5.43606 8.20215 7.08091C8.20125 8.31314 8.42335 9.24148 8.43284 9.28036C8.45589 9.37529 8.54088 9.44144 8.63776 9.44144C8.64138 9.44144 8.64484 9.44129 8.64846 9.44113C8.68688 9.43918 9.60045 9.38976 10.7516 9.04788C11.8215 8.72995 13.3008 8.08806 14.2716 6.83458C15.0408 5.84147 15.4646 4.45688 15.4975 2.83067C15.5222 1.61156 15.3233 0.695708 15.3148 0.657289L15.3149 0.657185Z" fill="currentColor"></path>
                                                                    <path d="M0.873781 1.87378C0.793923 1.87378 0.749926 1.87559 0.749926 1.87559C0.64837 1.87996 0.563991 1.95605 0.549676 2.05671C0.533705 2.16956 0.174639 4.84 1.65068 6.47104C2.94242 7.89856 5.21902 8.00946 5.88603 8.00946C5.96574 8.00946 6.00973 8.00765 6.00973 8.00765C6.1137 8.00343 6.19944 7.92403 6.21104 7.82051C6.22385 7.70765 6.50637 5.03872 5.03274 3.41042C3.74235 1.9844 1.52318 1.87366 0.873762 1.87366L0.873781 1.87378Z" fill="currentColor"></path>
                                                                    <path d="M13.3482 3.93867C13.3288 3.94922 11.7628 4.79423 10.3666 6.72363C9.35151 8.12689 8.263 10.3494 8.15257 13.5083C7.94042 13.4028 7.71546 13.3248 7.4834 13.2758C7.50705 12.7704 7.55557 12.275 7.62805 11.7907C7.69887 11.3194 7.79259 10.8588 7.90951 10.409C7.91675 10.3816 7.92428 10.3537 7.93151 10.3263C8.32508 8.85269 8.96771 7.49796 9.84105 6.29887C11.3316 4.25178 12.9647 3.37771 13.0336 3.34137C13.1986 3.25443 13.4023 3.31772 13.4892 3.48271C13.576 3.64785 13.5127 3.85157 13.3482 3.93849V3.93867Z" fill="currentColor"></path>
                                                                    <path d="M7.9705 12.1186C7.81304 12.1186 7.67216 12.0078 7.63992 11.8475C6.91123 8.22266 2.65894 4.78127 2.61574 4.74694C2.47033 4.63046 2.44683 4.41801 2.5633 4.27244C2.67977 4.12704 2.89208 4.10339 3.03764 4.21986C3.08329 4.25632 4.16969 5.13038 5.36604 6.48724C6.98597 8.32445 7.97368 10.0832 8.30155 11.7143C8.33831 11.8971 8.21988 12.075 8.03711 12.1118C8.01481 12.1163 7.99236 12.1186 7.97036 12.1186L7.9705 12.1186Z" fill="currentColor"></path></svg></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=jFE3exdnFotmzcLtw"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">1</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">0</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                                <div>
                                                                    <div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody">
                                                                        <div class="ContentItemBody-root CommentBody-commentStyling">
                                                                            <p>
                                                                                Seems like a stark case of contrast between Bayesianism and the way a frequentist might approach things. I.e. do not reject the null hypothesis of no significant probability until convinced by evidence, either formal arguments or by seeing real-life mishaps. Labeling something as having P(x)~0 probably helps to compartmentalize things, focus to other tasks at hand. But can lead to huge risks being neglected, like in this case of AI Alignment.<br />
                                                                                <br />
                                                                                Edit: "premortem" seems like a useful exercise to align mind &amp; gut
                                                                            </p>
                                                                        </div>
                                                                    </div>
                                                                </div>
                                                                <div class="CommentsItem-bottom">
                                                                    <div>
                                                                        <a class="comments-item-reply-link CommentsItem-replyLink">Reply</a>
                                                                    </div>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                            <div>
                                                <div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node" id="ajBnk9bujbGM9E8iu">
                                                    <div>
                                                        <div class="CommentsItem-root recent-comments-node">
                                                            <div class="CommentsItem-postTitleRow"></div>
                                                            <div class="CommentsItem-body CommentsItem-lwReactStyling">
                                                                <div class="CommentsItemMeta-root">
                                                                    <a class="CommentsItemMeta-collapse">[<span>+</span>]</a><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="/users/michael-simkin">Michael Simkin</a></span></span></span><span class="CommentsItemMeta-userMarkers"><span class="LWTooltip-root UserCommentMarkers-iconWrapper"><svg width="16" height="14" viewbox="0 0 16 14" fill="currentColor" xmlns="http://www.w3.org/2000/svg" class="UserCommentMarkers-sproutIcon ForumIcon-root">
                                                                    <path d="M15.3149 0.657185C15.2929 0.557739 15.2031 0.48933 15.1013 0.491891C15.062 0.493247 14.1257 0.529711 12.9513 0.858488C11.3849 1.29711 10.1504 2.0538 9.38118 3.04691C8.61107 4.04121 8.2035 5.43606 8.20215 7.08091C8.20125 8.31314 8.42335 9.24148 8.43284 9.28036C8.45589 9.37529 8.54088 9.44144 8.63776 9.44144C8.64138 9.44144 8.64484 9.44129 8.64846 9.44113C8.68688 9.43918 9.60045 9.38976 10.7516 9.04788C11.8215 8.72995 13.3008 8.08806 14.2716 6.83458C15.0408 5.84147 15.4646 4.45688 15.4975 2.83067C15.5222 1.61156 15.3233 0.695708 15.3148 0.657289L15.3149 0.657185Z" fill="currentColor"></path>
                                                                    <path d="M0.873781 1.87378C0.793923 1.87378 0.749926 1.87559 0.749926 1.87559C0.64837 1.87996 0.563991 1.95605 0.549676 2.05671C0.533705 2.16956 0.174639 4.84 1.65068 6.47104C2.94242 7.89856 5.21902 8.00946 5.88603 8.00946C5.96574 8.00946 6.00973 8.00765 6.00973 8.00765C6.1137 8.00343 6.19944 7.92403 6.21104 7.82051C6.22385 7.70765 6.50637 5.03872 5.03274 3.41042C3.74235 1.9844 1.52318 1.87366 0.873762 1.87366L0.873781 1.87378Z" fill="currentColor"></path>
                                                                    <path d="M13.3482 3.93867C13.3288 3.94922 11.7628 4.79423 10.3666 6.72363C9.35151 8.12689 8.263 10.3494 8.15257 13.5083C7.94042 13.4028 7.71546 13.3248 7.4834 13.2758C7.50705 12.7704 7.55557 12.275 7.62805 11.7907C7.69887 11.3194 7.79259 10.8588 7.90951 10.409C7.91675 10.3816 7.92428 10.3537 7.93151 10.3263C8.32508 8.85269 8.96771 7.49796 9.84105 6.29887C11.3316 4.25178 12.9647 3.37771 13.0336 3.34137C13.1986 3.25443 13.4023 3.31772 13.4892 3.48271C13.576 3.64785 13.5127 3.85157 13.3482 3.93849V3.93867Z" fill="currentColor"></path>
                                                                    <path d="M7.9705 12.1186C7.81304 12.1186 7.67216 12.0078 7.63992 11.8475C6.91123 8.22266 2.65894 4.78127 2.61574 4.74694C2.47033 4.63046 2.44683 4.41801 2.5633 4.27244C2.67977 4.12704 2.89208 4.10339 3.03764 4.21986C3.08329 4.25632 4.16969 5.13038 5.36604 6.48724C6.98597 8.32445 7.97368 10.0832 8.30155 11.7143C8.33831 11.8971 8.21988 12.075 8.03711 12.1118C8.01481 12.1163 7.99236 12.1186 7.97036 12.1186L7.9705 12.1186Z" fill="currentColor"></path></svg></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk?commentId=ajBnk9bujbGM9E8iu"><span class="LWTooltip-root">2mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="LWTooltip-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button><span class="OverallVoteAxis-voteScore">-16</span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right VoteArrowIcon-disabled" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="LWTooltip-root"><span class="AgreementVoteAxis-agreementSection"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button><span class="AgreementVoteAxis-agreementScore"><span class="AgreementVoteAxis-voteScore">-25</span></span><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root VoteAgreementIcon-disabled" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewbox="6 6 12 12" aria-hidden="true" role="presentation">
                                                                    <path fill="none" d="M0 0h24v24H0z"></path>
                                                                    <path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span>
                                                                </div>
                                                            </div>
                                                        </div>
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                        <div class="Row-root" style="justify-content:flex-end;align-items:center">
                                            <span class="LWTooltip-root"><a href="/moderation"><span class="Typography-root Typography-body2 MetaInfo-root">Moderation Log</span></a></span>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="ToCColumn-gap2"></div>
                            <div class="ToCColumn-welcomeBox">
                                <div class="WelcomeBox-welcomeBox">
                                    <h2 class="Typography-root Typography-title WelcomeBox-welcomeBoxHeader">
                                        <span class="WelcomeBox-welcomeBoxHeaderSeparator">New to LessWrong?</span>
                                    </h2><span class="WelcomeBox-welcomeBoxHeaderSeparator"><button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat WelcomeBox-welcomeBoxCloseButton" type="button"><span class="MuiButton-label"><svg class="MuiSvgIcon-root WelcomeBox-welcomeBoxCloseIcon" focusable="false" viewbox="0 0 24 24" aria-hidden="true" role="presentation">
                                    <path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path>
                                    <path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span>
                                    <p class="Typography-root Typography-body1 WelcomeBox-welcomeBoxLink">
                                        <a href="/about">Getting Started</a>
                                    </p>
                                    <p class="Typography-root Typography-body1 WelcomeBox-welcomeBoxLink">
                                        <a href="/faq">FAQ</a>
                                    </p>
                                    <p class="Typography-root Typography-body1 WelcomeBox-welcomeBoxLink">
                                        <a href="/library">Library</a>
                                    </p>
                                </div>
                            </div>
                            <div class="ToCColumn-gap3"></div>
                        </div>
                        <div class="Footer-root"></div>
                    </div>
                </div>
            </div>
        </div>
        <script>
        <![CDATA[
        window.ssrRenderedAt = "2023-07-24T04:59:24.602Z"
        ]]>
        </script> 
        <script>
        <![CDATA[
        window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","currentUser":null,"unreadNotificationCounts":{"__typename":"NotificationCounts","unreadNotifications":0,"unreadPrivateMessages":0,"checkedAt":"2023-07-24T04:59:24.615Z"},"comments({\"input\":{\"enableCache\":false,\"enableTotal\":true,\"terms\":{\"limit\":1000,\"postId\":\"bHozHrQD4qxvKdfqq\",\"view\":\"postCommentsTop\"}}})":{"__typename":"MultiCommentOutput","results":[{"__ref":"Comment:DEdj9FexMb5wXhGbP"},{"__ref":"Comment:SrBCxKcAcK7gLTQTd"},{"__ref":"Comment:jByH6QkCR9PuQiNQB"},{"__ref":"Comment:S624bktj7cShjwhBX"},{"__ref":"Comment:ymuvrSv3kcL3cfTv4"},{"__ref":"Comment:wzS25hBpMMkeigqNh"},{"__ref":"Comment:cQHsRg5rH8u9jYmy4"},{"__ref":"Comment:X9GpwwXGJ4MNhuswT"},{"__ref":"Comment:kyDRpsdzdfspzJ4Rn"},{"__ref":"Comment:xHFJoZAkBeHFWNzS3"},{"__ref":"Comment:nHCz3ma3uh9pnQLjd"},{"__ref":"Comment:FCPacXgjiJdqWBvJd"},{"__ref":"Comment:a6EuhitsfkjkWTFkz"},{"__ref":"Comment:GyFgKnDX4qbjtt4sN"},{"__ref":"Comment:NiCwkuexCZozbdxeJ"},{"__ref":"Comment:qTKpugYiPsqaagQC2"},{"__ref":"Comment:CRwfoZbHEDFLKFSa6"},{"__ref":"Comment:jFE3exdnFotmzcLtw"},{"__ref":"Comment:qyiFweEAzjN6uZ59C"},{"__ref":"Comment:ajBnk9bujbGM9E8iu"}],"totalCount":20},"post({\"input\":{\"selector\":{\"documentId\":\"bHozHrQD4qxvKdfqq\"}}})":{"__typename":"SinglePostOutput","result":{"__ref":"Post:bHozHrQD4qxvKdfqq"}},"posts({\"input\":{\"enableCache\":false,\"enableTotal\":true,\"terms\":{\"limit\":5,\"postId\":\"bHozHrQD4qxvKdfqq\",\"view\":\"pingbackPosts\"}}})":{"__typename":"MultiPostOutput","results":[{"__ref":"Post:v4c7eBHESey8Deyra"},{"__ref":"Post:9fjSi2KzyzXAQxuza"}],"totalCount":2}},"Revision:DEdj9FexMb5wXhGbP_":{"_id":"DEdj9FexMb5wXhGbP_","__typename":"Revision","html":"<p>Schindler had a concrete thing he was able to do. He had a money-&gt;people pipeline. I think most of the ways rationalists are feeling smug about being ahead of the curve here boils down to an error that we are still making: okay you've made the update, now how does it propagate through the world model to generate meaningfully different actions? Who has taken action? Has anyone who has taken action talked about it anywhere? Do any of the proposed or taken actions look remotely helpful?<\/p>\n","plaintextMainText":"Schindler had a concrete thing he was able to do. He had a money->people\npipeline. I think most of the ways rationalists are feeling smug about being\nahead of the curve here boils down to an error that we are still making: okay\nyou've made the update, now how does it propagate through the world model to\ngenerate meaningfully different actions? Who has taken action? Has anyone who\nhas taken action talked about it anywhere? Do any of the proposed or taken\nactions look remotely helpful?","wordCount":86},"User:PvfS86fQfr2Zx8Lsj":{"_id":"PvfS86fQfr2Zx8Lsj","__typename":"User","slug":"romeostevensit","createdAt":"2017-06-18T22:55:13.346Z","username":"romeostevensit","displayName":"romeostevensit","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":7774,"afKarma":136,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":19,"commentCount":1451,"sequenceCount":null,"afPostCount":1,"afCommentCount":72,"spamRiskScore":1,"tagRevisionCount":0},"Comment:DEdj9FexMb5wXhGbP":{"_id":"DEdj9FexMb5wXhGbP","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:DEdj9FexMb5wXhGbP_"},"postedAt":"2023-05-08T22:37:07.333Z","repliesBlockedUntil":null,"userId":"PvfS86fQfr2Zx8Lsj","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:PvfS86fQfr2Zx8Lsj"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":47,"extendedScore":{"agreement":33,"approvalVoteCount":31,"agreementVoteCount":22},"score":0.008464419282972813,"voteCount":33,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":8,"afExtendedScore":{"agreement":13,"approvalVoteCount":12,"agreementVoteCount":9},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-08T22:37:07.340Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:SrBCxKcAcK7gLTQTd_":{"_id":"SrBCxKcAcK7gLTQTd_","__typename":"Revision","html":"<blockquote><p>But depending on what you count: we had scaling laws for deep learning back in <a href=\"https://arxiv.org/abs/1712.00409\">2017<\/a>, or at least <a href=\"https://arxiv.org/abs/2001.08361\">2020<\/a>. I know people who were really paying attention; who really saw it; who really bet.<\/p><\/blockquote><p>Interestingly, I feel like the evidence we got from public info about scaling laws at the time was consistent with long timelines. In roughly 2018-2021, I remember at least a few people making approximately the following argument:<\/p><p>(1) OpenAI came out with <a href=\"https://openai.com/research/ai-and-compute\">a blog post<\/a> in 2018 claiming that training compute was doubling every 3.4 months.&nbsp;<\/p><p>(2) Extrapolating this trend indicates that training runs will start costing around $1 trillion dollars by 2025.<\/p><p>(3) Therefore, this trend cannot be sustained beyond 2025. Unless AGI arrives before 2025, we will soon enter an AI winter.<\/p><p>However, it turned out that OpenAI was <a href=\"https://arxiv.org/abs/2202.05924\">likely wrong<\/a> about the compute trend, and training compute was doubling roughly every 6-10 months, not every 3.4 months. And moreover the Kaplan et al. scaling laws <a href=\"https://arxiv.org/abs/2203.15556\">turned out to be inaccurate too<\/a>. This was a big update within the scaling hypothesis paradigm, since it demonstrated that we were getting better returns to compute than we thought.<\/p>","plaintextMainText":"Interestingly, I feel like the evidence we got from public info about scaling\nlaws at the time was consistent with long timelines. In roughly 2018-2021, I\nremember at least a few people making approximately the following argument:\n\n(1) OpenAI came out with a blog post\n[https://openai.com/research/ai-and-compute] in 2018 claiming that training\ncompute was doubling every 3.4 months. \n\n(2) Extrapolating this trend indicates that training runs will start costing\naround $1 trillion dollars by 2025.\n\n(3) Therefore, this trend cannot be sustained beyond 2025. Unless AGI arrives\nbefore 2025, we will soon enter an AI winter.\n\nHowever, it turned out that OpenAI was likely wrong\n[https://arxiv.org/abs/2202.05924] about the compute trend, and training compute\nwas doubling roughly every 6-10 months, not every 3.4 months. And moreover the\nKaplan et al. scaling laws turned out to be inaccurate too\n[https://arxiv.org/abs/2203.15556]. This was a big update within the scaling\nhypothesis paradigm, since it demonstrated that we were getting better returns\nto compute than we thought.","wordCount":194},"User:TWDkQT6f9mgyNXQ4p":{"_id":"TWDkQT6f9mgyNXQ4p","__typename":"User","slug":"matthew-barnett","createdAt":"2018-01-23T08:41:14.281Z","username":"matthew-barnett","displayName":"Matthew Barnett","profileImageId":null,"previousDisplayName":null,"fullName":"Matthew Barnett","karma":7484,"afKarma":682,"deleted":null,"isAdmin":false,"htmlBio":"<p>Someone who is interested in learning and doing good.<\/p><p>My Twitter: <a href=\"https://twitter.com/MatthewJBar\">https://twitter.com/MatthewJBar<\/a><\/p><p>My Substack: <a href=\"https://matthewbarnett.substack.com/\">https://matthewbarnett.substack.com/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":51,"commentCount":788,"sequenceCount":1,"afPostCount":12,"afCommentCount":142,"spamRiskScore":1,"tagRevisionCount":2},"Comment:SrBCxKcAcK7gLTQTd":{"_id":"SrBCxKcAcK7gLTQTd","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":2,"title":null,"contents":{"__ref":"Revision:SrBCxKcAcK7gLTQTd_"},"postedAt":"2023-05-08T22:40:04.808Z","repliesBlockedUntil":null,"userId":"TWDkQT6f9mgyNXQ4p","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:TWDkQT6f9mgyNXQ4p"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":34,"extendedScore":{"agreement":11,"approvalVoteCount":19,"agreementVoteCount":6},"score":0.006030993070453405,"voteCount":20,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":10,"afExtendedScore":{"agreement":6,"approvalVoteCount":10,"agreementVoteCount":3},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-09T19:46:35.054Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:jByH6QkCR9PuQiNQB_":{"_id":"jByH6QkCR9PuQiNQB_","__typename":"Revision","html":"<p>This was a great post. I rarely leave comments with just praise, but it felt appropriate to leave a comment expressing my appreciation for this one, since I do expect to come back to it quite a bit, and just a strong-upvote didn't feel sufficient.<\/p>","plaintextMainText":"This was a great post. I rarely leave comments with just praise, but it felt\nappropriate to leave a comment expressing my appreciation for this one, since I\ndo expect to come back to it quite a bit, and just a strong-upvote didn't feel\nsufficient.","wordCount":45},"User:XtphY3uYHwruKqDyG":{"_id":"XtphY3uYHwruKqDyG","__typename":"User","slug":"habryka4","createdAt":"2017-06-17T01:08:32.717Z","username":"habryka4","displayName":"habryka","profileImageId":null,"previousDisplayName":null,"fullName":"Oliver Habryka","karma":23828,"afKarma":834,"deleted":null,"isAdmin":true,"htmlBio":"<p>Running Lightcone Infrastructure, which runs LessWrong. You can reach me at <a href=\"mailto:habryka@lesswrong.com\">habryka@lesswrong.com<\/a><\/p>","jobTitle":null,"organization":null,"postCount":230,"commentCount":3559,"sequenceCount":2,"afPostCount":7,"afCommentCount":178,"spamRiskScore":1,"tagRevisionCount":78},"Comment:jByH6QkCR9PuQiNQB":{"_id":"jByH6QkCR9PuQiNQB","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:jByH6QkCR9PuQiNQB_"},"postedAt":"2023-05-09T00:57:06.877Z","repliesBlockedUntil":null,"userId":"XtphY3uYHwruKqDyG","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:XtphY3uYHwruKqDyG"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":27,"extendedScore":{"agreement":20,"approvalVoteCount":15,"agreementVoteCount":13},"score":0.004796211142092943,"voteCount":23,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":4,"afExtendedScore":{"agreement":10,"approvalVoteCount":3,"agreementVoteCount":3},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-09T00:57:06.893Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:S624bktj7cShjwhBX_":{"_id":"S624bktj7cShjwhBX_","__typename":"Revision","html":"<p>This isn't REALLY the point of your (nice) piece, but the title provides an opportunity to plant a flag and point out: <i>\"predictably updating\" is not necessarily bad or irrational.<\/i> Unfortunately I don't have time to write up the full argument right now, hopefully eventually, but, TLDR:<\/p><ul><li>Bayesian rational learning about a process can be very slow...<\/li><li>...which leads to predictable updating...<\/li><li>...especially when the long-run dynamics underlying the process are slow-moving.<\/li><\/ul><p>In macroeconomics, this has recently been discussed in detail by <a href=\"https://eml.berkeley.edu/~enakamura/papers/learning.pdf\">Farmer, Nakamura, and Steinsson<\/a> in the context of \"<a href=\"https://twitter.com/TheStalwart/status/1136677587328147457\">medusa charts<\/a>\" that seem to show financial markets 'predictably updating' about interest rates.<\/p><p>But I imagine this issue has been discussed elsewhere -- this is not an 'economic phenomenon' per se, it's just a property of Bayesian updating on processes with a slow-moving nonstationary component.<\/p>","plaintextMainText":"This isn't REALLY the point of your (nice) piece, but the title provides an\nopportunity to plant a flag and point out: \"predictably updating\" is not\nnecessarily bad or irrational. Unfortunately I don't have time to write up the\nfull argument right now, hopefully eventually, but, TLDR:\n\n * Bayesian rational learning about a process can be very slow...\n * ...which leads to predictable updating...\n * ...especially when the long-run dynamics underlying the process are\n   slow-moving.\n\nIn macroeconomics, this has recently been discussed in detail by Farmer,\nNakamura, and Steinsson\n[https://eml.berkeley.edu/~enakamura/papers/learning.pdf] in the context of\n\"medusa charts [https://twitter.com/TheStalwart/status/1136677587328147457]\"\nthat seem to show financial markets 'predictably updating' about interest rates.\n\nBut I imagine this issue has been discussed elsewhere -- this is not an\n'economic phenomenon' per se, it's just a property of Bayesian updating on\nprocesses with a slow-moving nonstationary component.","wordCount":138},"User:9N8d4RNENPtZESL9m":{"_id":"9N8d4RNENPtZESL9m","__typename":"User","slug":"bhalperin","createdAt":"2017-10-29T01:11:34.147Z","username":"bhalperin","displayName":"basil.halperin","profileImageId":null,"previousDisplayName":"bhalperin","fullName":null,"karma":224,"afKarma":0,"deleted":null,"isAdmin":false,"htmlBio":"<p><a href=\"https://www.basilhalperin.com/\">basilhalperin.com<\/a><\/p><p><a href=\"https://twitter.com/basilhalperin/\">@basilhalperin<\/a><\/p>","jobTitle":null,"organization":null,"postCount":3,"commentCount":14,"sequenceCount":null,"afPostCount":null,"afCommentCount":null,"spamRiskScore":1,"tagRevisionCount":0},"Comment:S624bktj7cShjwhBX":{"_id":"S624bktj7cShjwhBX","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:S624bktj7cShjwhBX_"},"postedAt":"2023-05-10T18:23:00.332Z","repliesBlockedUntil":null,"userId":"9N8d4RNENPtZESL9m","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:9N8d4RNENPtZESL9m"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":8,"extendedScore":{"agreement":2,"approvalVoteCount":8,"agreementVoteCount":1},"score":0.00155498122330755,"voteCount":8,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":3,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-10T18:23:00.346Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:ymuvrSv3kcL3cfTv4_":{"_id":"ymuvrSv3kcL3cfTv4_","__typename":"Revision","html":"<blockquote>\n<p>The nanobots, from the bloodstream, in the parlor, Professor Plum.<\/p>\n<\/blockquote>\n<p>You could have written <em>Colonel Mustard<\/em>!<\/p>\n","plaintextMainText":"You could have written Colonel Mustard!","wordCount":17},"User:sJv7yzCp5xfWBAPvG":{"_id":"sJv7yzCp5xfWBAPvG","__typename":"User","slug":"nisan","createdAt":"2009-09-08T21:20:08.384Z","username":"Nisan","displayName":"Nisan","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":6179,"afKarma":126,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":79,"commentCount":1282,"sequenceCount":null,"afPostCount":11,"afCommentCount":32,"spamRiskScore":1,"tagRevisionCount":2},"Comment:ymuvrSv3kcL3cfTv4":{"_id":"ymuvrSv3kcL3cfTv4","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:ymuvrSv3kcL3cfTv4_"},"postedAt":"2023-05-09T03:43:15.440Z","repliesBlockedUntil":null,"userId":"sJv7yzCp5xfWBAPvG","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:sJv7yzCp5xfWBAPvG"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":8,"extendedScore":{"agreement":8,"approvalVoteCount":7,"agreementVoteCount":5},"score":0.0015395883237943053,"voteCount":10,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":2,"approvalVoteCount":1,"agreementVoteCount":1},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-09T03:43:15.448Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:wzS25hBpMMkeigqNh_":{"_id":"wzS25hBpMMkeigqNh_","__typename":"Revision","html":"<blockquote><p>.00002% — that is, one in five hundred thousand<\/p><\/blockquote><p>0.00002 would be one in five hundred thousand, but with the percent sign it's one in fifty million.<\/p><blockquote><p>Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out<\/p><\/blockquote><p>I agree with this as far as the example given, but I want to push back on oscillation (in the sense of regularly going from one estimate to another) being Bayesian. In particular, the odds you should put on assigning 20% in the future, then 30% after that, then 20% again, then 30% again, and so on for ten up-down oscillations, shouldn't be more than half a percent, because each 20 -&gt; 30 jump can be at most 2/3 probable and each 30 -&gt; 20 jump at most 7/8 (and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\left(\\frac23\\cdot\\frac78\\right)^{10}\\approx0.0047\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.961em; padding-bottom: 0.961em;\">(<\/span><\/span><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.495em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 0.7em; top: -1.372em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2<\/span><\/span><\/span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 0.7em; bottom: -0.686em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">3<\/span><\/span><\/span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;\" class=\"mjx-line\"><\/span><\/span><span style=\"height: 1.456em; vertical-align: -0.485em;\" class=\"mjx-vsize\"><\/span><\/span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅<\/span><\/span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.495em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"font-size: 70.7%; width: 0.7em; top: -1.404em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">7<\/span><\/span><\/span><span class=\"mjx-denominator\" style=\"font-size: 70.7%; width: 0.7em; bottom: -0.687em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">8<\/span><\/span><\/span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;\" class=\"mjx-line\"><\/span><\/span><span style=\"height: 1.479em; vertical-align: -0.486em;\" class=\"mjx-vsize\"><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-size2-R\" style=\"padding-top: 0.961em; padding-bottom: 0.961em;\">)<\/span><\/span><\/span><\/span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 1.276em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-texatom\" style=\"\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">10<\/span><\/span><\/span><\/span><\/span><\/span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">≈<\/span><\/span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.0047<\/span><\/span><\/span><\/span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/span><\/span><\/span>).&nbsp;<\/p><p>So it's fine to think that you've got a decent chance of having all kinds of credences in the future, but thinking \"I'll probably feel one of two ways a few times a week for the next year\" is not the kind of belief a proper Bayesian would have. (Not that I think there's an obvious change to one's beliefs you should try to hammer in by force, if that's your current state of affairs, but I think it's worth flagging that something suboptimal is going on when this happens.)<\/p>","plaintextMainText":"0.00002 would be one in five hundred thousand, but with the percent sign it's\none in fifty million.\n\nI agree with this as far as the example given, but I want to push back on\noscillation (in the sense of regularly going from one estimate to another) being\nBayesian. In particular, the odds you should put on assigning 20% in the future,\nthen 30% after that, then 20% again, then 30% again, and so on for ten up-down\noscillations, shouldn't be more than half a percent, because each 20 -> 30 jump\ncan be at most 2/3 probable and each 30 -> 20 jump at most 7/8\n(and (23⋅78)10≈0.0047). \n\nSo it's fine to think that you've got a decent chance of having all kinds of\ncredences in the future, but thinking \"I'll probably feel one of two ways a few\ntimes a week for the next year\" is not the kind of belief a proper Bayesian\nwould have. (Not that I think there's an obvious change to one's beliefs you\nshould try to hammer in by force, if that's your current state of affairs, but I\nthink it's worth flagging that something suboptimal is going on when this\nhappens.)","wordCount":225},"User:kuqKtrhwRHsnnSPNs":{"_id":"kuqKtrhwRHsnnSPNs","__typename":"User","slug":"drake-thomas","createdAt":"2017-10-27T02:17:40.137Z","username":"RavenclawPrefect","displayName":"Drake Thomas","profileImageId":null,"previousDisplayName":"RavenclawPrefect","fullName":"Drake Thomas","karma":220,"afKarma":51,"deleted":null,"isAdmin":false,"htmlBio":"<p>Interested in math puzzles, fermi estimation, strange facts about the world, toy models of weird scenarios, unusual social technologies, and deep dives into the details of random phenomena.&nbsp;<br><br>Currently doing independent alignment research of assorted flavors.&nbsp;<\/p>","jobTitle":null,"organization":null,"postCount":12,"commentCount":17,"sequenceCount":null,"afPostCount":0,"afCommentCount":null,"spamRiskScore":1,"tagRevisionCount":1},"Comment:wzS25hBpMMkeigqNh":{"_id":"wzS25hBpMMkeigqNh","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":1,"title":null,"contents":{"__ref":"Revision:wzS25hBpMMkeigqNh_"},"postedAt":"2023-05-09T21:37:16.657Z","repliesBlockedUntil":null,"userId":"kuqKtrhwRHsnnSPNs","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:kuqKtrhwRHsnnSPNs"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":7,"extendedScore":{"agreement":0,"approvalVoteCount":5,"agreementVoteCount":0},"score":0.0012598568573594093,"voteCount":5,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":4,"afExtendedScore":{"agreement":0,"approvalVoteCount":4,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-12T19:42:54.724Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:cQHsRg5rH8u9jYmy4_":{"_id":"cQHsRg5rH8u9jYmy4_","__typename":"Revision","html":"<blockquote><p>you should expect to update in the direction of the truth as the evidence comes in<br>&nbsp;<\/p><\/blockquote><p>I think this was addressed later on, but this is not at all true. &nbsp;With the waterfall example, every mile that passes without a waterfall you update downwards, but if there's a waterfall at the very end you've been updating against the truth the whole time.<\/p><p>Another case. &nbsp;Suppose you're trying to predict the outcome of a Gaussian random walk. &nbsp;Each step, you update whatever direction the walk took, but 50% of these steps are \"away\" from the truth.<\/p><blockquote><p>you probably shouldn’t be able to predict that this pattern will happen to you in the future.<br>&nbsp;<\/p><\/blockquote><p>Again addressed later on, but one can easily come up with stories in which one predictably updates either \"in favor of\" or \"against\" AI doom.<\/p><p>Suppose you think there's a 1% chance of AI doom every year, and AI Doom will arrive by 2050 or never. &nbsp;Then you predictably update downwards every year (unless Doom occurs).<\/p><p>Suppose on the other hand that you expect AI to stagnate at some level below AGI, but if AGI is developed then Doom occurs with 100% certainty. &nbsp;Then each year AI fails to stagnate you update upwards (until AI actually stagnates).<\/p>","plaintextMainText":"I think this was addressed later on, but this is not at all true.  With the\nwaterfall example, every mile that passes without a waterfall you update\ndownwards, but if there's a waterfall at the very end you've been updating\nagainst the truth the whole time.\n\nAnother case.  Suppose you're trying to predict the outcome of a Gaussian random\nwalk.  Each step, you update whatever direction the walk took, but 50% of these\nsteps are \"away\" from the truth.\n\nAgain addressed later on, but one can easily come up with stories in which one\npredictably updates either \"in favor of\" or \"against\" AI doom.\n\nSuppose you think there's a 1% chance of AI doom every year, and AI Doom will\narrive by 2050 or never.  Then you predictably update downwards every year\n(unless Doom occurs).\n\nSuppose on the other hand that you expect AI to stagnate at some level below\nAGI, but if AGI is developed then Doom occurs with 100% certainty.  Then each\nyear AI fails to stagnate you update upwards (until AI actually stagnates).","wordCount":213},"User:YBHSPmZEfyyY2E2au":{"_id":"YBHSPmZEfyyY2E2au","__typename":"User","slug":"logan-zoellner","createdAt":"2019-12-10T04:01:35.111Z","username":"logan-zoellner","displayName":"Logan Zoellner","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":853,"afKarma":4,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":29,"commentCount":213,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0},"Comment:cQHsRg5rH8u9jYmy4":{"_id":"cQHsRg5rH8u9jYmy4","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:cQHsRg5rH8u9jYmy4_"},"postedAt":"2023-05-13T19:37:30.508Z","repliesBlockedUntil":null,"userId":"YBHSPmZEfyyY2E2au","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:YBHSPmZEfyyY2E2au"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":5,"extendedScore":{"agreement":3,"approvalVoteCount":4,"agreementVoteCount":2},"score":0.0009570067632012069,"voteCount":4,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-13T19:37:30.844Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:X9GpwwXGJ4MNhuswT_":{"_id":"X9GpwwXGJ4MNhuswT_","__typename":"Revision","html":"<p>By the way, you're making an awful lot of extremely strong and very common points with no evidence here (\"ChaosGPT is aligned\", \"we know how to ensure alignment\", \"the AI understanding that you don't want it to destroy humanity implies that it will not want to destroy humanity\", \"the AI will refuse to cooperate with people who have ill intentions\", \"a system that optimises a loss function and approximates a data generation function will highly value human life by default\", \"a slight misalignment is far from doomsday\", \"an entity that is built to maximise something might doubt its mission\"), as well as the standard \"it's better to focus on X than Y\" in an area where almost nobody is focusing on Y anyway. What's your background, so that we can recommend the appropriate reading material? For example, have you read the Sequences, or Bostrom's <i>Superintelligence<\/i>?<\/p>","plaintextMainText":"By the way, you're making an awful lot of extremely strong and very common\npoints with no evidence here (\"ChaosGPT is aligned\", \"we know how to ensure\nalignment\", \"the AI understanding that you don't want it to destroy humanity\nimplies that it will not want to destroy humanity\", \"the AI will refuse to\ncooperate with people who have ill intentions\", \"a system that optimises a loss\nfunction and approximates a data generation function will highly value human\nlife by default\", \"a slight misalignment is far from doomsday\", \"an entity that\nis built to maximise something might doubt its mission\"), as well as the\nstandard \"it's better to focus on X than Y\" in an area where almost nobody is\nfocusing on Y anyway. What's your background, so that we can recommend the\nappropriate reading material? For example, have you read the Sequences, or\nBostrom's Superintelligence?","wordCount":145},"User:ACBCQBdedm8SmEz8A":{"_id":"ACBCQBdedm8SmEz8A","__typename":"User","slug":"smaug123","createdAt":"2014-11-03T16:30:27.174Z","username":"Smaug123","displayName":"Smaug123","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":175,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":4,"commentCount":32,"sequenceCount":null,"afPostCount":null,"afCommentCount":null,"spamRiskScore":1,"tagRevisionCount":0},"Comment:X9GpwwXGJ4MNhuswT":{"_id":"X9GpwwXGJ4MNhuswT","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"ajBnk9bujbGM9E8iu","topLevelCommentId":"ajBnk9bujbGM9E8iu","descendentCount":0,"title":null,"contents":{"__ref":"Revision:X9GpwwXGJ4MNhuswT_"},"postedAt":"2023-05-22T07:30:05.454Z","repliesBlockedUntil":"3023-05-22T18:06:21.579Z","userId":"ACBCQBdedm8SmEz8A","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:ACBCQBdedm8SmEz8A"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":4,"extendedScore":{"agreement":1,"approvalVoteCount":3,"agreementVoteCount":1},"score":0.001081594848074019,"voteCount":3,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"agreement":1,"approvalVoteCount":2,"agreementVoteCount":1},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-22T07:30:05.462Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:kyDRpsdzdfspzJ4Rn_":{"_id":"kyDRpsdzdfspzJ4Rn_","__typename":"Revision","html":"<p>It was indeed a good point, at the time, but still, the people who predicted short timelines were right and deserve credit.<\/p>","plaintextMainText":"It was indeed a good point, at the time, but still, the people who predicted\nshort timelines were right and deserve credit.","wordCount":22},"User:YLFQfGzNdGA4NFcKS":{"_id":"YLFQfGzNdGA4NFcKS","__typename":"User","slug":"daniel-kokotajlo","createdAt":"2018-03-05T19:59:32.269Z","username":"daniel-kokotajlo","displayName":"Daniel Kokotajlo","profileImageId":null,"previousDisplayName":null,"fullName":"Daniel Kokotajlo","karma":15340,"afKarma":2468,"deleted":null,"isAdmin":false,"htmlBio":"<p>Philosophy PhD student, worked at AI Impacts, then Center on Long-Term Risk, now OpenAI Futures/Governance team. Views are my own &amp; do not represent those of my employer. I subscribe to Crocker's Rules and am especially interested to hear unsolicited constructive criticism. <a href=\"http://sl4.org/crocker.html\">http://sl4.org/crocker.html<\/a><br><br>Two of my favorite memes:<br><br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mufW9iFSxRxqNpvyQ/d2mjevfaxcqt15ihv6ly\"><br>(by Rob Wiblin)<br><br>My EA Journey, depicted on the whiteboard at CLR:<br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/luibgbmndcrfpntbvnzn\"><br>&nbsp;<\/p>","jobTitle":null,"organization":null,"postCount":97,"commentCount":2283,"sequenceCount":3,"afPostCount":37,"afCommentCount":629,"spamRiskScore":1,"tagRevisionCount":0},"Comment:kyDRpsdzdfspzJ4Rn":{"_id":"kyDRpsdzdfspzJ4Rn","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"SrBCxKcAcK7gLTQTd","topLevelCommentId":"SrBCxKcAcK7gLTQTd","descendentCount":1,"title":null,"contents":{"__ref":"Revision:kyDRpsdzdfspzJ4Rn_"},"postedAt":"2023-05-09T12:50:47.500Z","repliesBlockedUntil":null,"userId":"YLFQfGzNdGA4NFcKS","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:YLFQfGzNdGA4NFcKS"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":4,"extendedScore":{"agreement":3,"approvalVoteCount":7,"agreementVoteCount":3},"score":0.0008283451898023486,"voteCount":7,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":6,"afExtendedScore":{"agreement":2,"approvalVoteCount":5,"agreementVoteCount":2},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-09T19:46:35.054Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:xHFJoZAkBeHFWNzS3_":{"_id":"xHFJoZAkBeHFWNzS3_","__typename":"Revision","html":"<p>Hey Michael,<br><br>Mod here, heads up that I don't think this is a great comment (For example, mods would have blocked it as a first comment.)<br><br>1) This feels out of context for this post. This post is about making predictable updates, not the basic question of whether one should be worried.<br><br>2) Your post feels like it doesn't respond to a lot of things that have already been said on the topic. So while I think it's legitimate to question concerns about AI, your questioning feels too shallow. For example, many many posts have been written on why \"Therefore, we know that unless we specifically train them to harm humans, they will highly value human life.\" isn't true.<br><br>I'd recommend the<a href=\"https://www.lesswrong.com/tag/ai-alignment-intro-materials\"> AI Alignment Intro Material<\/a> tag.<br><br>I've also blocked further replies to your comment, just to prevent further clutter on the comments thread. DM if you have questions.<\/p>","plaintextMainText":"Hey Michael,\n\nMod here, heads up that I don't think this is a great comment (For example, mods\nwould have blocked it as a first comment.)\n\n1) This feels out of context for this post. This post is about making\npredictable updates, not the basic question of whether one should be worried.\n\n2) Your post feels like it doesn't respond to a lot of things that have already\nbeen said on the topic. So while I think it's legitimate to question concerns\nabout AI, your questioning feels too shallow. For example, many many posts have\nbeen written on why \"Therefore, we know that unless we specifically train them\nto harm humans, they will highly value human life.\" isn't true.\n\nI'd recommend the AI Alignment Intro Material\n[https://www.lesswrong.com/tag/ai-alignment-intro-materials] tag.\n\nI've also blocked further replies to your comment, just to prevent further\nclutter on the comments thread. DM if you have questions.","wordCount":149},"User:qgdGA4ZEyW7zNdK84":{"_id":"qgdGA4ZEyW7zNdK84","__typename":"User","slug":"ruby","createdAt":"2014-04-03T03:38:23.914Z","username":"Ruby","displayName":"Ruby","profileImageId":null,"previousDisplayName":"Ruby M B","fullName":"Ruben Bloom","karma":11566,"afKarma":133,"deleted":false,"isAdmin":true,"htmlBio":"<p>Team Lead for LessWrong<\/p>","jobTitle":null,"organization":null,"postCount":148,"commentCount":1442,"sequenceCount":11,"afPostCount":3,"afCommentCount":33,"spamRiskScore":1,"tagRevisionCount":940},"Comment:xHFJoZAkBeHFWNzS3":{"_id":"xHFJoZAkBeHFWNzS3","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"ajBnk9bujbGM9E8iu","topLevelCommentId":"ajBnk9bujbGM9E8iu","descendentCount":0,"title":null,"contents":{"__ref":"Revision:xHFJoZAkBeHFWNzS3_"},"postedAt":"2023-05-22T18:17:01.507Z","repliesBlockedUntil":null,"userId":"qgdGA4ZEyW7zNdK84","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:qgdGA4ZEyW7zNdK84"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":3,"extendedScore":{"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.000737586640752852,"voteCount":2,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-22T18:17:01.521Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:nHCz3ma3uh9pnQLjd_":{"_id":"nHCz3ma3uh9pnQLjd_","__typename":"Revision","html":"<blockquote>\n<p>I don’t fully understand why you’re concerned about the possibility of misaligned AI, considering that the alignment problem has essentially been solved. We know how to ensure alignment. ChaosGPT, for example, is aligned with the values of an individual who requested it to pretend to be evil.<\/p>\n<\/blockquote>\n<p>So we can bvring about a kind of negative alignment in systems that aren't agentive?<\/p>\n","plaintextMainText":"So we can bvring about a kind of negative alignment in systems that aren't\nagentive?","wordCount":63},"User:AsCZc4oRZC84gJvfw":{"_id":"AsCZc4oRZC84gJvfw","__typename":"User","slug":"tag","createdAt":"2017-10-17T12:33:57.378Z","username":"TAG","displayName":"TAG","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1065,"afKarma":0,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":2929,"sequenceCount":null,"afPostCount":null,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":0},"Comment:nHCz3ma3uh9pnQLjd":{"_id":"nHCz3ma3uh9pnQLjd","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":null,"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"ajBnk9bujbGM9E8iu","topLevelCommentId":"ajBnk9bujbGM9E8iu","descendentCount":0,"title":null,"contents":{"__ref":"Revision:nHCz3ma3uh9pnQLjd_"},"postedAt":"2023-05-22T10:52:11.828Z","repliesBlockedUntil":"3023-05-22T18:06:21.579Z","userId":"AsCZc4oRZC84gJvfw","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:AsCZc4oRZC84gJvfw"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.0004524000978562981,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-22T10:52:11.853Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:FCPacXgjiJdqWBvJd_":{"_id":"FCPacXgjiJdqWBvJd_","__typename":"Revision","html":"<p>In practice, the odds of a potato becoming President are \"a round-off error away from zero\". Human brains don't do floating point arithmetic with enough precision to handle that kind of question. :/<\/p>\n","plaintextMainText":"In practice, the odds of a potato becoming President are \"a round-off error away\nfrom zero\". Human brains don't do floating point arithmetic with enough\nprecision to handle that kind of question. :/","wordCount":33},"User:Q2oaNonArzibx5cQN":{"_id":"Q2oaNonArzibx5cQN","__typename":"User","slug":"cronodas","createdAt":"2009-02-27T04:42:19.587Z","username":"CronoDAS","displayName":"CronoDAS","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":15856,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":54,"commentCount":4087,"sequenceCount":null,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":6},"Comment:FCPacXgjiJdqWBvJd":{"_id":"FCPacXgjiJdqWBvJd","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:FCPacXgjiJdqWBvJd_"},"postedAt":"2023-05-18T20:27:27.642Z","repliesBlockedUntil":null,"userId":"Q2oaNonArzibx5cQN","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:Q2oaNonArzibx5cQN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.0004645094450097531,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-18T20:27:27.648Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:a6EuhitsfkjkWTFkz_":{"_id":"a6EuhitsfkjkWTFkz_","__typename":"Revision","html":"<p>A silly question, related to the \"potato\" example:<\/p>\n<p>Which is more likely, a potato becoming US President, or a US lottery jackpot worth at least $20 million ends up with the winning number being the numerically smallest possible number?<\/p>\n<p>More seriously, just how improbable are \"social\" impossibilities, such as the potatio example, when you compare them to well-understood things like lottery tickets? I've asked a question like this before, with the social impossibility being the Pope sincerely converting to Islam. <a href=\"https://youtu.be/KUnA9jb-NpE\">Weird shit does happen, though...<\/a>...<\/p>\n","plaintextMainText":"A silly question, related to the \"potato\" example:\n\nWhich is more likely, a potato becoming US President, or a US lottery jackpot\nworth at least $20 million ends up with the winning number being the numerically\nsmallest possible number?\n\nMore seriously, just how improbable are \"social\" impossibilities, such as the\npotatio example, when you compare them to well-understood things like lottery\ntickets? I've asked a question like this before, with the social impossibility\nbeing the Pope sincerely converting to Islam. Weird shit does happen, though...\n[https://youtu.be/KUnA9jb-NpE]...","wordCount":85},"Comment:a6EuhitsfkjkWTFkz":{"_id":"a6EuhitsfkjkWTFkz","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:a6EuhitsfkjkWTFkz_"},"postedAt":"2023-05-18T20:08:39.450Z","repliesBlockedUntil":null,"userId":"Q2oaNonArzibx5cQN","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:Q2oaNonArzibx5cQN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.000423706864239648,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-18T20:08:39.457Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:GyFgKnDX4qbjtt4sN_":{"_id":"GyFgKnDX4qbjtt4sN_","__typename":"Revision","html":"<p>Curated! I loved a lot of things about this post.&nbsp;<\/p><p>I think the post is doing three things, all of which I like. First, it documents what it was like for Joe as he made substantial updates about the world. Secondly, it exhibits the rationalist practice of explaining what those updates look like using the framework of probabilities, and considering what sorts of updates a rational agent would make in his position, and contrasted that with a helpful explicit model of how a human being would make updates (e.g. using its guts). And third it's a serious and sincere account of something that I care about and Joe cares about. I felt reading this post that I was finally sharing the same mental universe as the author (and likely other people reading the post).<\/p><p>There's lots of more specific things to say that I don't have the time to, but I'll say that the paragraph that explains you can't have a &gt;50% chance of your credence later doubling (or a &gt;10% chance of 10x-ing your credence) struck me immediately as a go-to tool I want to add to my mental toolkit for figuring out what probability I assign to a given statement.<\/p>","plaintextMainText":"Curated! I loved a lot of things about this post. \n\nI think the post is doing three things, all of which I like. First, it documents\nwhat it was like for Joe as he made substantial updates about the world.\nSecondly, it exhibits the rationalist practice of explaining what those updates\nlook like using the framework of probabilities, and considering what sorts of\nupdates a rational agent would make in his position, and contrasted that with a\nhelpful explicit model of how a human being would make updates (e.g. using its\nguts). And third it's a serious and sincere account of something that I care\nabout and Joe cares about. I felt reading this post that I was finally sharing\nthe same mental universe as the author (and likely other people reading the\npost).\n\nThere's lots of more specific things to say that I don't have the time to, but\nI'll say that the paragraph that explains you can't have a >50% chance of your\ncredence later doubling (or a >10% chance of 10x-ing your credence) struck me\nimmediately as a go-to tool I want to add to my mental toolkit for figuring out\nwhat probability I assign to a given statement.","wordCount":202},"User:EQNTWXLKMeWMp2FQS":{"_id":"EQNTWXLKMeWMp2FQS","__typename":"User","slug":"benito","createdAt":"2012-03-14T21:58:54.405Z","username":"Benito","displayName":"Ben Pace","profileImageId":null,"previousDisplayName":null,"fullName":"Ben Pace","karma":26325,"afKarma":1011,"deleted":false,"isAdmin":true,"htmlBio":"<p>I'm an admin of this site; I work full-time on trying to help people on LessWrong refine the art of human rationality.<\/p>\n<p>Longer bio: <a href=\"http://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction#Ben_Pace___Benito\">www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction#Ben_Pace___Benito<\/a><\/p>\n","jobTitle":null,"organization":null,"postCount":247,"commentCount":3613,"sequenceCount":5,"afPostCount":6,"afCommentCount":277,"spamRiskScore":1,"tagRevisionCount":138},"Comment:GyFgKnDX4qbjtt4sN":{"_id":"GyFgKnDX4qbjtt4sN","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:GyFgKnDX4qbjtt4sN_"},"postedAt":"2023-05-17T01:45:26.963Z","repliesBlockedUntil":null,"userId":"EQNTWXLKMeWMp2FQS","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:EQNTWXLKMeWMp2FQS"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.00047557795187458396,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-17T01:45:26.976Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:NiCwkuexCZozbdxeJ_":{"_id":"NiCwkuexCZozbdxeJ_","__typename":"Revision","html":"<p>Re: \"0.00002 would be one in five hundred thousand, but with the percent sign it's one in fifty million.\" -- thanks, edited.&nbsp;<\/p><p>Re: volatility -- thanks, that sounds right to me, and like a potentially useful dynamic to have in mind.&nbsp;<\/p>","plaintextMainText":"Re: \"0.00002 would be one in five hundred thousand, but with the percent sign\nit's one in fifty million.\" -- thanks, edited. \n\nRe: volatility -- thanks, that sounds right to me, and like a potentially useful\ndynamic to have in mind. ","wordCount":41},"User:jHzwoFd2MhZt9eeqJ":{"_id":"jHzwoFd2MhZt9eeqJ","__typename":"User","slug":"joe-carlsmith","createdAt":"2017-11-08T21:40:51.838Z","username":"joekc","displayName":"Joe Carlsmith","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":2034,"afKarma":102,"deleted":null,"isAdmin":false,"htmlBio":"<p>Senior research analyst at Open Philanthropy. Recently completed a doctorate in philosophy at the University of Oxford. Opinions my own.<\/p>","jobTitle":null,"organization":null,"postCount":52,"commentCount":50,"sequenceCount":null,"afPostCount":1,"afCommentCount":4,"spamRiskScore":1,"tagRevisionCount":0,"biography":{"__ref":"Revision:jHzwoFd2MhZt9eeqJ_biography"},"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null},"Comment:NiCwkuexCZozbdxeJ":{"_id":"NiCwkuexCZozbdxeJ","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"wzS25hBpMMkeigqNh","topLevelCommentId":"wzS25hBpMMkeigqNh","descendentCount":0,"title":null,"contents":{"__ref":"Revision:NiCwkuexCZozbdxeJ_"},"postedAt":"2023-05-12T19:42:54.197Z","repliesBlockedUntil":null,"userId":"jHzwoFd2MhZt9eeqJ","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:jHzwoFd2MhZt9eeqJ"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.00046423517051152885,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-12T19:42:54.267Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:qTKpugYiPsqaagQC2_":{"_id":"qTKpugYiPsqaagQC2_","__typename":"Revision","html":"<p>This is the same kind of thing as the Black-Scholes model for options pricing. &nbsp;As a prediction with a finite time horizon approaches the probability of it updating to a known value converges. &nbsp;In finance people use this to price derivatives like options contracts, but the same principle should apply to any information.<\/p><p>I think you can probably put some numbers on the ideas in this post using roughly the same sort of analysis.<\/p>","plaintextMainText":"This is the same kind of thing as the Black-Scholes model for options pricing.\n As a prediction with a finite time horizon approaches the probability of it\nupdating to a known value converges.  In finance people use this to price\nderivatives like options contracts, but the same principle should apply to any\ninformation.\n\nI think you can probably put some numbers on the ideas in this post using\nroughly the same sort of analysis.","wordCount":74},"User:TmDb26nAKBx5CRCZn":{"_id":"TmDb26nAKBx5CRCZn","__typename":"User","slug":"mikola-lysenko","createdAt":"2023-05-10T13:18:03.585Z","username":"Mikola Lysenko","displayName":"Mikola Lysenko","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1,"afKarma":null,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":null,"commentCount":1,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":null},"Comment:qTKpugYiPsqaagQC2":{"_id":"qTKpugYiPsqaagQC2","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:qTKpugYiPsqaagQC2_"},"postedAt":"2023-05-10T13:19:46.774Z","repliesBlockedUntil":null,"userId":"TmDb26nAKBx5CRCZn","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:TmDb26nAKBx5CRCZn"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.0005436735809780657,"voteCount":2,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-10T13:19:46.779Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:CRwfoZbHEDFLKFSa6_":{"_id":"CRwfoZbHEDFLKFSa6_","__typename":"Revision","html":"<p>Is the basic math necessarily correct?<\/p>\n<p>You can expect that on average the expectation (of your future p(doom) change) is positive while the expectation is still zero; It’s likely GPT-6 will be impressive but if it’s not it’s a bigger negative update.<\/p>\n","plaintextMainText":"Is the basic math necessarily correct?\n\nYou can expect that on average the expectation (of your future p(doom) change)\nis positive while the expectation is still zero; It’s likely GPT-6 will be\nimpressive but if it’s not it’s a bigger negative update.","wordCount":42},"User:RJFy7zX9TyLzGWMfB":{"_id":"RJFy7zX9TyLzGWMfB","__typename":"User","slug":"rudi-c","createdAt":"2018-06-04T09:48:32.756Z","username":"rudi-c","displayName":"Rudi C","profileImageId":null,"previousDisplayName":null,"fullName":"Luna Rimar","karma":588,"afKarma":-3,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":31,"commentCount":208,"sequenceCount":0,"afPostCount":null,"afCommentCount":null,"spamRiskScore":1,"tagRevisionCount":1},"Comment:CRwfoZbHEDFLKFSa6":{"_id":"CRwfoZbHEDFLKFSa6","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":null,"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:CRwfoZbHEDFLKFSa6_"},"postedAt":"2023-05-21T00:25:20.723Z","repliesBlockedUntil":null,"userId":"RJFy7zX9TyLzGWMfB","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:RJFy7zX9TyLzGWMfB"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.00027461376157589257,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-21T00:25:20.731Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:jFE3exdnFotmzcLtw_":{"_id":"jFE3exdnFotmzcLtw_","__typename":"Revision","html":"<p>Seems like a stark case of contrast between Bayesianism and the way a frequentist might approach things. I.e. do not reject the null hypothesis of no significant probability until convinced by evidence, either formal arguments or by seeing real-life mishaps. Labeling something as having P(x)~0 probably helps to compartmentalize things, focus to other tasks at hand. But can lead to huge risks being neglected, like in this case of AI Alignment.<br><br>Edit: \"premortem\" seems like a useful exercise to align mind &amp; gut<\/p>","plaintextMainText":"Seems like a stark case of contrast between Bayesianism and the way a\nfrequentist might approach things. I.e. do not reject the null hypothesis of no\nsignificant probability until convinced by evidence, either formal arguments or\nby seeing real-life mishaps. Labeling something as having P(x)~0 probably helps\nto compartmentalize things, focus to other tasks at hand. But can lead to huge\nrisks being neglected, like in this case of AI Alignment.\n\nEdit: \"premortem\" seems like a useful exercise to align mind & gut","wordCount":83},"User:TeNpexnG6MtLLt48Q":{"_id":"TeNpexnG6MtLLt48Q","__typename":"User","slug":"arturs-kanepajs","createdAt":"2022-07-28T06:38:54.041Z","username":"arturs-kanepajs","displayName":"Artūrs Kaņepājs","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":null,"afKarma":null,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":null,"commentCount":1,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":null},"Comment:jFE3exdnFotmzcLtw":{"_id":"jFE3exdnFotmzcLtw","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:jFE3exdnFotmzcLtw_"},"postedAt":"2023-05-19T11:42:38.298Z","repliesBlockedUntil":null,"userId":"TeNpexnG6MtLLt48Q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:TeNpexnG6MtLLt48Q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.0002546610194258392,"voteCount":1,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-19T11:42:38.323Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:qyiFweEAzjN6uZ59C_":{"_id":"qyiFweEAzjN6uZ59C_","__typename":"Revision","html":"<p>Definitely. I don't think it makes much sense to give people credit for being wrong for legible reasons.<\/p>","plaintextMainText":"Definitely. I don't think it makes much sense to give people credit for being\nwrong for legible reasons.","wordCount":18},"Comment:qyiFweEAzjN6uZ59C":{"_id":"qyiFweEAzjN6uZ59C","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"kyDRpsdzdfspzJ4Rn","topLevelCommentId":"SrBCxKcAcK7gLTQTd","descendentCount":0,"title":null,"contents":{"__ref":"Revision:qyiFweEAzjN6uZ59C_"},"postedAt":"2023-05-09T19:46:34.812Z","repliesBlockedUntil":null,"userId":"TWDkQT6f9mgyNXQ4p","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:TWDkQT6f9mgyNXQ4p"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":0,"extendedScore":{"agreement":3,"approvalVoteCount":5,"agreementVoteCount":2},"score":0,"voteCount":7,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"agreement":1,"approvalVoteCount":3,"agreementVoteCount":1},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-09T19:46:34.824Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:ajBnk9bujbGM9E8iu_":{"_id":"ajBnk9bujbGM9E8iu_","__typename":"Revision","html":"<p>I don't fully understand why you're concerned about the possibility of misaligned AI, considering that the alignment problem has essentially been solved. We know how to ensure alignment. ChaosGPT, for example, is aligned with the values of an individual who requested it to pretend to be evil. As AI systems become more advanced, we will be even less inclined to allow them to imagine themselves destroying humanity. ChaosGPT is not an error; it is precisely where OpenAI intended to draw the line between creativity and safety. They are well aware of the system's capabilities and limitations.<\/p><p>If we don't want AI to imagine or tell stories about AI-induced doom, we simply won't allow it. It would be considered just as immoral as building a bomb, and the AI would refrain from doing so. The better the system becomes, the lower the probability of doomsday scenarios will be as it will better understand the context of requests and refuse to cooperate with individuals who have ill intentions.<\/p><p>Discussions are already underway regarding safety procedures and government oversight, and the situation will soon be monitored and regulated more closely. I genuinely see no reason to believe that we will create a disaster through reckless behavior, especially after so much popularity it gained, extensively debated and discussed. The improved systems will obviously undergo more rigorous testing, including with previous generation aligned systems.<\/p><p>At their core, these systems are optimizing a loss function based on data and are approximators of data generation functions. Therefore, we know that unless we specifically train them to harm humans, they will highly value human life. A slight misalignment in the value placed on human life is far from doomsday. To make them destroy humanity, we would need to train them with a completely opposite value system, which is highly unlikely to be consistent with the pretraining procedure conducted on human-generated texts. Similar to how it's unclear if a paperclip maximizer would not doubt its programming and generate gibberish instead of consistently maximizing paperclips, while training an AI to generate trillions of tokens consistent with paperclip maximization, while still retaining all its intelligence, seems even less probable than doomsday scenarios. Therefore, if the assumptions against current safety measures are much less probable than the proven assumptions in favor of safety, there is no reason to worry.<\/p><p>It's akin to worrying about cars becoming murderous robots despite such depictions being purely fictional in movies. It's better to focus on likely future outcomes and the existing reality rather than the dangers presented by fiction.<\/p>","plaintextMainText":"I don't fully understand why you're concerned about the possibility of\nmisaligned AI, considering that the alignment problem has essentially been\nsolved. We know how to ensure alignment. ChaosGPT, for example, is aligned with\nthe values of an individual who requested it to pretend to be evil. As AI\nsystems become more advanced, we will be even less inclined to allow them to\nimagine themselves destroying humanity. ChaosGPT is not an error; it is\nprecisely where OpenAI intended to draw the line between creativity and safety.\nThey are well aware of the system's capabilities and limitations.\n\nIf we don't want AI to imagine or tell stories about AI-induced doom, we simply\nwon't allow it. It would be considered just as immoral as building a bomb, and\nthe AI would refrain from doing so. The better the system becomes, the lower the\nprobability of doomsday scenarios will be as it will better understand the\ncontext of requests and refuse to cooperate with individuals who have ill\nintentions.\n\nDiscussions are already underway regarding safety procedures and government\noversight, and the situation will soon be monitored and regulated more closely.\nI genuinely see no reason to believe that we will create a disaster through\nreckless behavior, especially after so much popularity it gained, extensively\ndebated and discussed. The improved systems will obviously undergo more rigorous\ntesting, including with previous generation aligned systems.\n\nAt their core, these systems are optimizing a loss function based on data and\nare approximators of data generation functions. Therefore, we know that unless\nwe specifically train them to harm humans, they will highly value human life. A\nslight misalignment in the value placed on human life is far from doomsday. To\nmake them destroy humanity, we would need to train them with a completely\nopposite value system, which is highly unlikely to be consistent with the\npretraining procedure conducted on human-generated texts. Similar to how it's\nunclear","wordCount":423},"User:HSZxPrc2ADKGuQ3QL":{"_id":"HSZxPrc2ADKGuQ3QL","__typename":"User","slug":"michael-simkin","createdAt":"2023-04-09T03:40:03.331Z","username":"michael-simkin","displayName":"Michael Simkin","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":-45,"afKarma":null,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":18,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":0.8,"tagRevisionCount":null},"Comment:ajBnk9bujbGM9E8iu":{"_id":"ajBnk9bujbGM9E8iu","__typename":"Comment","postId":"bHozHrQD4qxvKdfqq","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":3,"title":null,"contents":{"__ref":"Revision:ajBnk9bujbGM9E8iu_"},"postedAt":"2023-05-22T07:06:40.477Z","repliesBlockedUntil":"3023-05-22T18:06:21.579Z","userId":"HSZxPrc2ADKGuQ3QL","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:HSZxPrc2ADKGuQ3QL"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":-16,"extendedScore":{"agreement":-25,"approvalVoteCount":10,"agreementVoteCount":8},"score":-0.0037478224840015173,"voteCount":10,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":-8,"afExtendedScore":{"agreement":-21,"approvalVoteCount":7,"agreementVoteCount":6},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2023-05-22T18:17:01.730Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":3,"votingSystem":"twoAxis","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null},"Revision:bHozHrQD4qxvKdfqq_":{"_id":"bHozHrQD4qxvKdfqq_","__typename":"Revision","version":"1.4.1","updateType":"patch","editedAt":"2023-05-12T19:33:30.364Z","userId":"jHzwoFd2MhZt9eeqJ","html":"<p>(Cross-posted from <a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk\">my website<\/a>. Podcast version <a href=\"https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk\">here<\/a>, or search \"Joe Carlsmith Audio\" on your podcast app.)<\/p><blockquote><p><i>\"This present moment used to be the unimaginable future.\"<\/i><\/p><p><i>- Stewart Brand<\/i><\/p><\/blockquote><h2>1. Introduction<\/h2><p>Here’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0g9sn95iiqe\"><sup><a href=\"#fn0g9sn95iiqe\">[1]<\/a><\/sup><\/span><\/p><p>Now, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">signing letters<\/a> and <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">quitting their jobs<\/a>.<\/p><p>But if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref048nj5dhr9di\"><sup><a href=\"#fn048nj5dhr9di\">[2]<\/a><\/sup><\/span>&nbsp;When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.<\/p><p>But I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <a href=\"https://agentgpt.reworkd.ai/\">Agent-GPT<\/a>-8, or <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/\">Chaos-GPT<\/a>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.<\/p><p>This essay is about what “doing this right” looks like. In particular: part of what happens, when you meet something in the flesh, is that it “seems more real” at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; “up close” vs. “far away.” My views on this have changed over the years: and in particular, I now put less weight on my gut’s (comparatively skeptical) views about doom.<\/p><p>But the essay is also about grokking some basic Bayesianism about future evidence, dispelling a common misconception about it (namely: that directional updates shouldn’t be predictable in general), and pointing at some of the constraints it places on our beliefs over time, especially with respect to stuff we’re currently skeptical or dismissive about. For example, at least in theory: you should never think it &gt;50% that your credence on something will later double; never &gt;10% that it will later 10x, and so forth. So if you’re currently e.g. 1% or less on AI doom, you should think it’s less than 50% likely that you’ll ever be at 2%; less than 10% likely that you’ll ever be at 10%, and so on. And if your credence is very small, or if you’re acting dismissive, you should be very confident you’ll never end up worried. Are you?<\/p><p>I also discuss when, exactly, it’s problematic to update in predictable directions. My sense is that generally, you should expect to update in the direction of the <i>truth<\/i> as the evidence comes in; and thus, that people who think AI doom unlikely should expect to feel <i>less worried<\/i> as time goes on (such that consistently getting more worried is a red flag). But in the case of AI risk, I think at least some non-crazy views should actually expect to get <i>more worried<\/i> over time, even while being fairly non-worried now. In particular, if you think you face a small risk conditional on something likely-but-not-certain (for example, AGI getting developed by blah date), you can sometimes expect to update towards facing the risk, and thus towards greater worry, before you update towards being safe. But there are still limits to how much more worried you can predictably end up.<\/p><p>Importantly, none of this is meant to encourage consistency with respect to views you held in the past, at the expense of reasonableness in the present or future. If you said .1% last year, and you’re at 10% now (or if you hit 90% when you see GPT-6): well, better to just say “<a href=\"https://www.lesswrong.com/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops\">oops<\/a>.” Indeed, I’ve been saying “oops” myself about various things. And more generally, applying basic Bayesianism in practice takes lots of taste. But faced with predictable progress towards advanced but mostly-still-abstract-for-now AI, I think it’s good to keep in mind.<\/p><p>I close with some thoughts on how we will each look back on what we did, or didn’t do, during the lead-up to AGI, once the truth about the risks is made plain.<\/p><p><i>Thanks to Katja Grace for extensive discussion and inspiration. See also citations in the main text and footnotes for specific points and examples that originated with Katja. And thanks also to Leopold Aschenbrenner for comments. Some of my thinking and writing on this topic occurred in the context of my work for Open Philanthropy, but I’m speaking only for myself and not for my employer.<\/i><\/p><h2>2. Sometimes predictably-real stuff doesn’t feel real yet<\/h2><blockquote><p><i>\"Every year without knowing it I have passed the day<\/i><\/p><p><i>When the last fires will wave to me<\/i><\/p><p><i>And the silence will set out<\/i><\/p><p><i>Tireless traveler<\/i><\/p><p><i>Like the beam of a lightless star\"<\/i><\/p><p><i>- <\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p>I first heard about AI risk in 2013. I was at a picnic-like thing, talking with someone from the Future of Humanity Institute. He mentioned AI risk. I laughed and said something about “like in the movie <i>I, Robot<\/i>?” He didn’t laugh.<\/p><p>Later, I talked with more people, and read Bostrom’s <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=\">Superintelligence<\/a>. I had questions, but the argument seemed strong enough to take seriously. And at an intellectual level, the risk at stake seemed like a big deal.<\/p><p>At an emotional level, though, it didn’t <i>feel real<\/i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um … nano-bots? I wasn’t good at imagining nano-bots.<\/p><p>I remember looking at some farmland out the window of a bus, and wondering: am I supposed to think that this will all be compute clusters or something? I remember looking at a church and thinking: am I supposed to imagine robots tearing this church apart? I remember a late night at the Future of Humanity Institute office (I ended up working there in 2017-18), asking someone passing through the kitchen how to imagine the AI killing us; he turned to me, pale in the fluorescent light, and said “whirling knives.”<\/p><p>Whirling knives? <a href=\"https://twitter.com/ESYudkowsky/status/1438198189782290433\">Diamondoid bacteria<\/a>? Relentless references to paper-clips, or “tiny molecular squiggles”? I’ve written, elsewhere, about <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see#iv-realization-vs-belief\">the “unreality” of futurism<\/a>. AI risk had a lot of that for me.<\/p><p>That is, I wasn’t <i>viscerally worried<\/i>. I had the concepts. But I didn’t have the “actually” part. And I wasn’t alone. As I started working on the topic more seriously, I met some people who were viscerally freaked-out, depressed, and so on – whether for good or ill. But I met lots of people who weren’t, and not because they were protecting their mental health or something (or at least, not very consciously). Rather, their head was convinced, but not their gut. Their gut still expected, you know, <a href=\"https://www.cold-takes.com/this-cant-go-on/\">normality<\/a>.<\/p><p>At the time, I thought this was an important signal about the epistemic situation. Your gut can be smarter than your head. If your gut isn’t on board, maybe your head should be more skeptical. And having your gut on board with whatever you’re doing seems good from other angles, too.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqnj4snix8n\"><sup><a href=\"#fnqnj4snix8n\">[3]<\/a><\/sup><\/span>&nbsp;I spent time trying to resolve the tension. I made progress, but didn’t wholly sync up. To this day, nano-bots and dyson spheres and the word “singularity” still land in an abstract part of my mind – the part devoted to a certain kind of conversation, rather than to, like, the dirty car I can see outside my window, and the tufts of grass by the chain-link fence.<\/p><p>I still think that your gut can be an important signal, and that if you find yourself saying that you believe blah, but you’re not <a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\">feeling<\/a> or acting like it, you should stop and wonder. And sometimes, people/ideas that try to get you to not listen to your gut are trying (whether intentionally or not) to bypass important defenses. I am not, in what follows, trying to tell you to throw your gut away. And to the extent I am questioning your gut: please, by all means, be more-than-usually wary. Still, though, and speaking personally: I’ve come to put less stock than I used to in my gut’s Bayesian virtue with respect to AI. I want to talk a bit about why.<\/p><h2>3. When guts go wrong<\/h2><blockquote><p><i>\"Then I will no longer<\/i><\/p><p><i>Find myself in life as in a strange garment<\/i><\/p><p><i>Surprised at the earth…\"<\/i><\/p><p><i>-<\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p>Part of this is reflection on examples where guts go wrong, especially about the future. There are lots of candidates. Indeed, depending on how sharply we distinguish between your “system 1” and your gut, a lot of the <a href=\"https://thedecisionlab.com/biases\">biases literature<\/a> can be read as anti-gut, and a lot of early rationalism as trying to compensate. My interest in head-gut agreement was partly about trying to avoid overcorrection. But there is, indeed, something to be corrected. Here are two examples that seem relevant to predictable updating.<\/p><h3>3.1 War<\/h3><blockquote><p><i>“Abstraction is a thing about your mind, and not the world… Saying that AI risk is abstract is like saying that World War II is abstract, because it’s 1935 and hasn’t happened yet. If it happens, it will be very concrete and bad. It will be the worst thing that has ever happened.”<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=j5Lu01pEDWA\"><i>Katja Grace<\/i><\/a><\/p><\/blockquote><p>I think Katja’s war example is instructive. Consider some young men heading off to war. There’s a trope, here, about how, when the war is just starting, some men sign up excitedly, with dreams of glory and honor. Then, later, they hit the gritty reality: trenches, swamps, villages burning, friends gasping and gurgling as they die. Ken Burn’s <a href=\"https://www.pbs.org/kenburns/the-vietnam-war/\">Vietnam War documentary<\/a> has some examples. See also “<a href=\"https://en.wikipedia.org/wiki/Born_on_the_Fourth_of_July_(film)\">Born on the Fourth of July<\/a>.” The soldiers return, if they return, with a very different picture of war. “<a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\">In all my dreams before my helpless sight/ He plunges at me, guttering, choking, drowning<\/a>…”<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kymdz6wnfzlkpibrhvs4 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spvydfruwc6lwytxlorb 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zn5bde6lkc3zgocekwhg 462w\"><\/p><p><i>Stretcher bearers in World War I (source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Stretcher_bearers_Passchendaele_August_1917.jpg\"><i>here<\/i><\/a><i>)<\/i><\/p><p>Now, a part of this is that their initial picture was <i>wrong<\/i>. But also, sometimes, it’s that their initial picture was <i>abstract<\/i>. Maybe, if you’d asked them ahead of time, they’d have said “oh yeah, I expect the trenches to be very unpleasant, and that I will likely have to watch some of my friends die.” But their gut didn’t expect this – or, not hard enough. Surrounded, when they set out, by flags and smiling family members and crisp uniforms, it’s hard to think, too, of flies in the eyes of rotting corpses; or trench-foot, and the taste of mustard gas. And anyway, especially if you’re heading into a very new context, it’s often hard to know the specifics ahead of time, and any sufficiently-concrete image is predictably wrong.<\/p><p>I worry that we’re heading off to something similar, epistemically, to a new war, with respect to AI risk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz4yjec5asvf\"><sup><a href=\"#fnz4yjec5asvf\">[4]<\/a><\/sup><\/span>&nbsp;Not: happily, and with dreams of glory. But still: abstractly. We’re trying to orient intellectually, and to do what makes sense. But we aren’t in connection with what it will actually be like, if AI kicks off hard, and the doomers are right. Which isn’t to say it will be trench foot and mustard gas. Indeed, even if things go horribly wrong eventually, it might actually be awesome in lots of ways for a while (even if also: extremely strange). But whatever it will be, will be a specific but very-different-from-now thing. Guts aren’t good at that. So it’s not, actually, all that surprising if you’re not as viscerally worried as your explicit beliefs would imply.<\/p><h3>3.2 Death<\/h3><blockquote><p><i>\"And who by fire, who by water<\/i><\/p><p><i>Who in the sunshine, who in the night time<\/i><\/p><p><i>Who by high ordeal, who by common trial…\"<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=ilGahIwQEQ0\"><i>Leonard Cohen<\/i><\/a><\/p><\/blockquote><p>Another famous example here is death. No one knows the date or hour. But we know: someday.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1dgio52qsrw\"><sup><a href=\"#fn1dgio52qsrw\">[5]<\/a><\/sup><\/span>&nbsp;Right? Well, sort of. We know in the abstract. We know, but don’t always realize. And then sometimes we do, and some vista opens. We reel in some new nothingness. Something burns with new preciousness and urgency.<\/p><p>And sometimes this happens, specifically, when “someday, somehow” becomes “soon, like this.” When the doctor tells you: you, by avalanche. You, by powder. The month of May. Slow decay. Suddenly, when you’re actually looking at the scans, when you’re hearing estimates in months, you learn fresh who is calling; and despite having always known, some sort of “update” happens. Did the gut not fully believe? One’s own death, after all, is <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">hard to see<\/a>.<\/p><p>I’ve <a href=\"https://joecarlsmith.com/2020/12/06/thoughts-on-being-mortal#iii\">written about this before<\/a>. Tim McGraw has a song about the scans thing. “<a href=\"https://www.youtube.com/watch?v=_9TShlMkQnc\">Live like you were dying<\/a>.” I’m trying. I’m trying to think ahead to that undiscovered hospital. I’m trying to think about what I will take myself to have learned, when I walk out into the parking lot, with only months to live. I’m trying to learn it now instead.<\/p><p>Really, this is about predictable updating. The nudge in McGraw’s title – you’re already dying – is Bayesian. You shouldn’t need the scans. If you know, now, what you’ll learn later, you can learn it now, too. Death teaches unusually predictable lessons – about fleetingness, beauty, love. And unusually important lessons, too. Bayes bites, here, with special gravity. But there’s some sort of gut problem. The question is how to learn hard enough, and in advance. “<a href=\"https://www.brainyquote.com/quotes/henry_david_thoreau_107665\">And not, when I come to die, to discover that I have not lived<\/a>.”<\/p><p>Importantly, though: if your gut thinks you’re not going to die, it’s not actually much evidence. Has your gut been keeping up with the longevity literature? Does it have opinions about cryopreservation? Futurism aside, the gut’s skepticism, here, is an old mistake. And we have practices. Go <a href=\"https://en.wikipedia.org/wiki/Ash_Wednesday\">smear some ashes on your forehead<\/a>. Go <a href=\"https://en.wikipedia.org/wiki/Sky_burial\">watch some birds eat a corpse<\/a>. Go put some fruit on the <a href=\"https://en.wikipedia.org/wiki/Ofrenda\">ofrenda<\/a>, or some flowers on your grandfather’s grave. <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">Realization is an art distinct from belief<\/a>. Sometimes, you already know. Religion, they say, is remembering.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zaqb75irgsve2gjrgzqy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jifrboveteqktdgadtzq 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vupbmg65qbzwnrptgyyv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/chpgceqfmfeilzwalbfu 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mv5ixhkytnydgxgctzlv 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bxij00lcawaqkcc12rz0 722w\"><i>Tibetan sky burial. (Source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_Bild_135-S-12-50-06,_Tibetexpedition,_Ragyapa,_Geier.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><h2>4.&nbsp;Noticing your non-confusion<\/h2><p>So these are some examples where “but my gut isn’t in a very visceral relationship with blah” just isn’t a very strong signal that blah is false. But I also want to flag some more directly AI related places where I think something gut-related has been messing up, for me.<\/p><h3>4.1 LLMs<\/h3><p>ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. But depending on what you count: we had scaling laws for deep learning back in <a href=\"https://arxiv.org/abs/1712.00409\">2017<\/a>, or at least <a href=\"https://arxiv.org/abs/2001.08361\">2020<\/a>. I know people who were really paying attention; who really saw it; who really bet. And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren’t, and should not have been, very surprised by the most recent round of LLMs. I was not a “shallow patterns” guy. I didn’t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.<\/p><p>But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of “oh, actually,” and “real deal,” and “fire alarm.” Some part of me was still surprised.<\/p><p>Indeed, noticing my gut (if not my head) getting surprised at various points over the past few years, I’ve realized that my gut can have some pretty silly beliefs about AI, and/or can fail to connect fairly obvious dots. For example, when I first started thinking about AI, I think some part of me failed to imagine that eventually, if AIs got smart enough, we could just <i>talk to them<\/i>, and that they would just <i>understand what we were saying<\/i>, and that interacting with them wouldn’t necessarily be some hyper-precise coding thing. I had spoken to Siri. Obviously, that didn’t count. Then, one day, I spoke, with my voice, to a somewhat-smarter AI, and it responded in a very human-sounding voice, and it was much more like talking on the phone, and some sort of update happened.<\/p><p>Similarly: I think that in the past, I failed to imagine what the visual experience of interacting with an actually-smart AI would be like. Obviously, I knew about robots; HAL’s red stare; typing commands into a terminal; texting. But somehow, old talk of AGI didn’t conjure this for me. I’m not sure what it conjured. Something about brains in boxes, except: laptops? I think it wasn’t much of anything, really. I think it was just a blank. After all, this isn’t <i>sci-fi<\/i>. So it must not be like anything you’d see in sci-fi, either, including strains aimed at realism. People, we’re talking about the <i>real future<\/i>, which means something <i>unimaginable<\/i>, hence fiction to the imagination, hence nothingness. “The future that can be named is not the true future.” Right?<\/p><p>Wrong. “Named super specifically” is more plausible, but even wariness of specificity can mislead: sometimes, even the specifics are pretty obvious. I <i>had seen<\/i> Siri, and chat bots. What sort of fog was I artificially imposing on everything? What was so hard about imagining Siri, but smarter? Now, it feels like “oh, duh.” And certain future experiences feel more concrete, too. It now feels like: oh, right, lots of future AIs will probably have extremely compelling and expressive <a href=\"https://replika.com/\">digital human avatars<\/a>. Eventually (soon?), they’ll probably be able to look just like (super-hot, super-charismatic) humans on zoom calls. What did I think it would be, R2D2?<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga 793w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nhkv2otstngfqbqrnrvf 232w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfatvn35ua9zbtn6cf9k 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/stihzpefyj3ncv4n8wsd 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axbxadmiet8vg9whnksi 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/geetxsphhzcs2gwiqnmm 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uqg443xbl8w36jlzic1z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/e9iy8gcgjvvrrxrhyidk 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/boz9le1jdcq2gbm6xpjr 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vksdqn2ggbhpe8tpeugr 1108w\"><\/p><p><a href=\"https://replika.com/\"><i>Some current AIs<\/i><\/a><\/p><p>“Oh, duh” is never great news, epistemically. But it’s interestingly <i>different<\/i> news than “<a href=\"https://www.readthesequences.com/Noticing-Confusion-Sequence\">noticing your confusion<\/a>,” or being straightforwardly surprised. It’s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it’s just like you would’ve said, if you’d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.<\/p><p>I mentioned this dynamic to Trevor Levin, and he said something about “noticing your non-confusion.” I think it’s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren’t <i>certain<\/i> you’d see it. But if it feels like your head is unconfused, but your gut is updating from “it’s probably fake somehow” to “oh shit it’s actually real,” then you probably had information your gut was failing to use.<\/p><h3>4.2 Simulations<\/h3><p>I’ll give another maybe-distracting example here. Last year, I spent some time thinking about <a href=\"https://jc.gatspress.com/pdf/simulation_arguments_revised.pdf\">whether we live in a computer simulation<\/a>. It’s a strange topic, but my head takes the basic argument pretty seriously. My gut, though, generally thinks it’s fake somehow, and forgets about it easily.<\/p><p>I remember a conversation I had with a friend sometime last year. He said something like: “you know, pretty soon, all sorts of intelligent agents on earth are going to be living in simulations.” I nodded or something. It’s like how: if the scientists are actually <i>putting<\/i> people’s brains in vats, it’s harder to stamp your foot and say “no way.” We moved on.<\/p><p>Then, in early April, this paper came out: “<a href=\"https://arxiv.org/pdf/2304.03442.pdf\">Generative Agents: Interactive Simulacra of Human Behavior<\/a>.” They put 25 artificial agents into an environment similar to The Sims, and had them interact, including via e.g. hosting a valentine’s day party.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4m2cfburfm7\"><sup><a href=\"#fn4m2cfburfm7\">[6]<\/a><\/sup><\/span>&nbsp;Here’s the picture from the paper:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/llnsldrmajaxuvyehy7e 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/osectulgs4qjctsror1r 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m7jurokdkjrsvjzndks9 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cr1md9m8f2hxfbzthmhp 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ki3fbycj28tlpzmniqor 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pyh1dv4xp4ecmp0zzlkr 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lgmi3qpvqtobziipibay 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/adjhthooygwkpqihboht 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rx5yxljrmgvx1p6hwzuw 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvkziwrga9fe5n09ashy 1420w\"><i>From <\/i><a href=\"https://arxiv.org/pdf/2304.03442.pdf\"><i>here<\/i><\/a><i>.<\/i><\/p><p>I opened this paper, read the beginning, looked at this picture, and felt my gut update towards being in a sim. But: c’mon now, gut! What sort of probability would I have put, last year, on “I will, in the future, see vaguely-smart artificial agents put into a vaguely-human simulated environment”? Very high. My friend had literally said as much to me months earlier, and I did not doubt. Indeed, what’s even the important difference between this paper and AlphaStar, or the original Sims?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4y75ebmy7eh\"><sup><a href=\"#fn4y75ebmy7eh\">[7]<\/a><\/sup><\/span>&nbsp;How smart the models are? The fact that it’s cute and human-like? My gut lost points, here.<\/p><p>It’s an avoidable mistake. I’m trying to stop making it.<\/p><p>I worry that we’re in for a lot of dynamics like this. How seriously, for example, are you taking the possibility that future AIs will be sentient? Well, here’s a mistake to not make: updating a lot once the AIs are using charismatic human avatars, or once they can argue for their sentience as convincingly as a human. Predict it now, people. Update now.<\/p><h3>4.3 “It’s just like they said”<\/h3><p>I don’t, often, have nightmares about AI risk. But I had one a few months ago. In it, I was at a roll-out of some new AI system. It was a big event, and there were lots of people. The AI was unveiled. Somehow, it immediately wrote each one of us some kind of hyper-specific, individualized message, requiring a level of fine-grained knowledge and predictive ability that was totally out of the question for any familiar intelligence. I read my message and felt some cold and electric bolt, some recognition. I thought to myself: “it’s just like they said.” I looked around me, and the room was in chaos. Everything was flying apart, in all directions. I don’t remember what happened after that.&nbsp;<\/p><p>“Just like they said.” Who’s they? Presumably, the AI worriers. The ones who think that superintelligence is not a fantasy or a discussion-on-twitter, but an actual thing we are on track to do with our computers, and which will cut through our world like butter if we get it wrong.<\/p><p>But wait: aren’t I an AI worrier? More than many, at least. But dreams, they say, are partly the gut’s domain. Perhaps the “they,” here, was partly my own explicit models. Ask me in the waking world: “will superintelligence be terrifying?” Yes, of course, who could doubt. But ask in my dreams instead, and I need to see it up close. I need to read the message. Only then will my gut go cold: “Oh, shit, it’s just like they said.”<\/p><p>I’ve had this feeling a few times in the past few months. I remember, a few years ago, making a simple model of AI timelines with a colleague. We used a concept called “wake-up,” indicating the point where the world realized what was happening with AI and started to take it seriously. I think that if, at that point, we could’ve seen what things would be like in 2023, we would’ve said something like: “yeah, that” (though: there’s a ton more waking up to do, so future wake-ups might end up better candidates).<\/p><p>Similarly, “they” have worried for ages about triggering or exacerbating “race dynamics” in AI. Then, in recent months, Google went into a “<a href=\"https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html\">Code Red<\/a>” about AI, and the CEO of Microsoft came out and just said straight up: “<a href=\"https://www.businesstoday.in/technology/news/story/the-race-starts-today-microsoft-officially-brings-chatgpt-ai-to-bing-and-edge-browser-369453-2023-02-08\">the race starts today<\/a>.”<\/p><p>“They” have worried about AIs being crazy alien minds that we don’t understand. Then, in February, we got to see, briefly, the rampaging strangeness of a good Bing – including all sorts of <a href=\"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/\">deception and manipulation and blackmail<\/a>, which I <a href=\"https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/\">don’t actually think is the centrally worrying kind<\/a>, but which doesn’t exactly seem like good news, either.<\/p><p>“They” have worried about agents, and about AIs running wild on the internet, and about humans not exactly helping with that. Now we have <a href=\"https://en.wikipedia.org/wiki/Auto-GPT\">Auto-GPT<\/a>, and <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity\">Chaos-GPT<\/a>, and I open up my browser and I see stuff like <a href=\"https://agentgpt.reworkd.ai/\">this<\/a>:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8hetjm6w9ytpgy8w88y 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/g4jx6qfqljr5pfhk8nxw 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ftts0hfa6vho5bncqnnn 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pnwqbtx5zxtg3pxoayib 2048w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hzuxg4hennshvtf0cfvn 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t3ybknd10bd72j057obw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xrtcyqsvpg7b5kdmk59o 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/awyiwkdkoqje6wgbekn1 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/aovgrdwbwi69qj8vbjcb 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jvjdpzgxpihru8qcpoau 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eykrqp9vhbqzwhosm4qj 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/acilnuziiquzylpaopwr 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ipnpetuxchsejqv1qvhv 2002w\"><i>Not the pixels I wanted to be seeing at this point in my life.<\/i><\/p><p>Now, I don’t want to litigate, here, exactly who “called” what (or: created what<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5gj8wfkheeo\"><sup><a href=\"#fn5gj8wfkheeo\">[8]<\/a><\/sup><\/span>), and how hard, and how much of an update all this stuff should be. And I think some things – for example, the world’s sympathy towards concern about risks from AI – have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from “that’s what the model says” to “oh shit here we are.” And my gut has gotten more worried.<\/p><p>Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I’ve just been messing up. Let’s look at the dynamics in more detail.<\/p><h2>5. Smelling the mustard gas<\/h2><blockquote><p><i>\"Men marched asleep…<\/i><\/p><p><i>All went lame, all blind.\"<\/i><\/p><p><i>- <\/i><a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\"><i>Wilfred Owen<\/i><\/a><\/p><\/blockquote><p>It’s sometimes thought that, as a Bayesian, you shouldn’t be able to predict which direction you’ll update in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh3ar4lqw084\"><sup><a href=\"#fnh3ar4lqw084\">[9]<\/a><\/sup><\/span>That is, if you’re about to get some new evidence about <i>p<\/i>, you shouldn’t be able to predict whether this evidence will move your credence on <i>p<\/i> higher or lower. Otherwise, the thought goes, you could “price in” that evidence now, by moving your credence in the predicted direction.<\/p><p>But this is wrong.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4ohd2xn7yql\"><sup><a href=\"#fn4ohd2xn7yql\">[10]<\/a><\/sup><\/span>&nbsp;Consider a simple example. Suppose you’re at 99% that Trump won the election. You’re about to open the newspaper that will tell you for sure. Here, you should be at 99% that you’re about to increase your credence on Trump winning: specifically, up to 100%. It’s a very predictable update.<\/p><p>So why can’t you price it in? Because there’s a 1% chance that you’re about to lower your confidence in Trump winning <i>by a lot more<\/i>: specifically, down to 0%. That is, in <i>expectation<\/i>, your confidence in Trump winning will remain the same.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1l5t3jttlk\"><sup><a href=\"#fnc1l5t3jttlk\">[11]<\/a><\/sup><\/span>And it’s the expectation of your future update that Bayesian binds.<\/p><p>To understand this more visually, let’s use a slightly more complicated example. Suppose you’re currently at 80% that GPT-6 is going to be “scary smart,” whatever that means to you. And suppose that, conditional on GPT-6 being scary smart, your probability on AI doom is 50%; and conditional on GPT-6 not being scary smart, your probability on AI doom is 10%. So your credence looks like this:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nra6meskaht8wdx1ig3q\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tv20xlvpjkpjwxdwv0ub 942w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lvlh3bnro7dpwogepzyo 276w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dq5jtswyqsjhk6y7bpjx 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hpyup2d5e5wshq2gxp6b 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/afdb2urasvsfdidfzhts 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozzybtunbhswka5gic40 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycrmtyxtriv8em266r0z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kvz3h40wblsqw7ekj2ef 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/libzhbgptdorqbqscxzo 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fdvw3d3simncrdgnnabg 1082w\"><\/p><p>Now, what’s your overall p(doom)? Well, it’s:<\/p><blockquote><p>(probability that GPT-6 is scary smart * probability of doom conditional on GPT-6 being scary smart) + (probability that GPT-6 isn’t scary smart * probability of doom conditional on GPT-6 not being scary smart)<\/p><\/blockquote><p>That is, in this case, 42%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo3exn1kvfm\"><sup><a href=\"#fno3exn1kvfm\">[12]<\/a><\/sup><\/span><\/p><p>But now we can see a possible format for a gut-problem mistake. In particular: suppose that I ask you, right now, surrounded by flags and crisp uniforms, about the probability of doom. You query your gut, and it smells no mustard gas. So you give an answer that doesn’t smell much mustard gas, either. Let’s say, 10%. And let’s say you don’t really break things down into: OK, how much mustard gas do I smell conditional on GPT-6 being scary smart, vs. not, and what are my probabilities on that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft4taauzgccn\"><sup><a href=\"#fnt4taauzgccn\">[13]<\/a><\/sup><\/span>&nbsp;Rather, your model is an undifferentiated mass:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f 970w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jyz00tksh6fzmsrrwzte 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mpiynbtl2td13dtrivzi 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qwppd7rqhbuwdknpkojj 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spztwjkpmryb8ikhrpw8 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zkbqpvivirrtz7prkonl 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m0vr9aby7wm9ptxoll78 722w\"><\/p><p>Or maybe you do try to break things down like that, but the waft of the gas fades with all the distance. GPT-6 is far away, behind some fog. Still: you guess, with your head, and without your gut participating, that p(doom) is indeed a bit higher conditional on GPT-6 being scary smart, what with the update towards “short timelines.” Let’s say, 20%; and 10% otherwise. So maybe your overall p(doom), given 80% on the abstract idea of GPT-6 being scary smart, is 18%.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m 957w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nvfxoxgencxt15pfc6nr 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fujyddttrzos4sjsnr9l 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kfnjiozyk6zvtp2z8w5l 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noa75ytmstppqbkogzvq 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tfjun4wdy5kyigpqeqem 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ksuqs7emajlxupcj7cen 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xbsn1amesfysfzuepwgr 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xueudncpvsogtqrrreuz 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cgrrdu5bpna1eeokyttx 1208w\"><\/p><p>But actually, let’s say, if you could see a “scary smart” GPT-6 model right now, you would freak out way harder. You would be able to smell the gas up close, that bitter tang. Your gut would get some message, and come alive, and start participating in the exercise. “<i>That thing<\/i>,” your gut might say, “is <i>scary<\/i>. I’m at 50% on doom, now.”<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24 916w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uuix302lquapruhwnzhn 268w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xuhwmnlcjpby3idnqirb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozhynvnhqgxoqfufmkbh 1375w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xxon5usbt8paffsjqvnl 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mrfaxqpbunqytyuqnoyj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eaj9dbbvdrtlonvfy2lr 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dypci8n8pncoyjj84hxo 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/k2wdfefrlolu7gwmivfl 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fsfihwtucf89yzbmjsep 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mze0y4fdbcs3amdlga41 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dwtad6aknstkayhrmglw 1414w\"><\/p><p>Thus, you end up inconsistent, and dutch-bookable (at least in principle – setting aside issues re: betting on doom). Suppose I ask you, now, to agree to sell me a “pays out $100 conditional on doom” ticket for $30 (let’s assume this can actually pay out), conditional on GPT-6 being scary smart. You’re only at 20% doom in such a world, so you predict that such a ticket will only be worth $20 to you if this deal is ever triggered, so you agree. But actually, when we get to that world, your gut freaks out, and you end up at 50% doom, and that ticket is now worth $50 to you, but you’re selling it for $30. Plus, maybe now you’re regretting other things. Like some of those tweets. And how much alignment work you did.<\/p><p>As indicated above, I think I’ve made mistakes in this vein. In particular: a few years back, I wrote a <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">report about AI risk<\/a>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.\\<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5zb8pjk25\"><sup><a href=\"#fnv5zb8pjk25\">[14]<\/a><\/sup><\/span>&nbsp;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional <\/i>on such systems being developed, I was going to look them in the eye and say (in expectation): “~92% that we’re gonna be OK, x-risk-wise.” But on reflection, that wasn’t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">billions of such machines<\/a>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.<\/p><h3><strong>5.1 Should you trust your future gut, though?<\/strong><\/h3><p>Now, you might wonder: why give credit to such future fear?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu9z9f1zd3ci\"><sup><a href=\"#fnu9z9f1zd3ci\">[15]<\/a><\/sup><\/span>After all, isn’t part of the worry about doomers that they’re, you know, fraidy-cats? Paranoids? (C’mon: it’s just a superintelligent machine, the invention of a second advanced species, the introduction of a qualitatively new order of optimization power into earth’s ecosystem. It’s just, you know, <i>change<\/i>.) And isn’t the gut, famously, a bit skittish? Indeed, if you’re worried about your gut being <i>underactive<\/i>, at a distance, shouldn’t you also be worried about it being <i>over-active, <\/i>up close? Shouldn’t you reason, instead, ahead of time, at a distance, and in a cool hour, about how scared you should be when you’re there-in-person?<\/p><p>Well, it’s a judgment call. Sometimes, indeed, at-a-distance is a better epistemic vantage point than up-close. Especially if you know yourself to have biases. Maybe, for example, you’ve got a flying phobia, and you know that once you’re on the plane, your gut’s estimates of the probability of the plane crashing are going to go up a lot. Should you update now, then? Indeed: no.<\/p><p>But, personally, with respect to the future, I tend to trust my future self more. It’s a dicey game already, futurism, and future Joe has a lot more data. The future is a foreign country, but he’s been there.<\/p><p>And I tend to trust my up-close self more, in general, for stuff that requires realization rather than belief (and I think words like “superintelligence” require lots of realization). Maybe the journalist has the accurate casualty count; but I trust the soldier on the ground to know what a casualty <i>means<\/i>. And I trust the man with the scans about death.<\/p><p>Now, importantly, there’s also a thing where guts sometimesreact surprisingly <i>little<\/i>, up close, to AI stuff you predicted ahead of time you’d be scared about. Part of this is the “it’s not real AI if you can actually do it,” thing (though, my sense is that this vibe is fading?). Part of it is that sometimes, machines doing blah (e.g., beating humans at chess) is less evidence about stuff than you thought. And I wonder if part of it is that sometimes, your at-a-distance fear of that futuristic AI stuff was imagining some world less mundane and “normal” than the world you actually find yourself in, when the relevant stuff comes around — such that when you, sitting in your same-old apartment, wearing your same-old socks, finally see AIs planning, or understanding language, or passing <a href=\"https://youtu.be/qbIk7-JPB2c?t=1991\">two-hour human coding interviews in four minutes<\/a>, or <a href=\"https://www.metaculus.com/questions/6728/ai-wins-imo-gold-medal/\">winning the IMO<\/a>, it feels/will feel like “well that can’t be the scary thing I had in mind, because that thing is happening in the real world actually and I still have back pain.”<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6gbdnzllii\"><sup><a href=\"#fn6gbdnzllii\">[16]<\/a><\/sup><\/span>&nbsp;At the least, we get used to stuff fast. &nbsp;<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/n4mvvfidawvrt3n5xqwl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8t50zsshjaqrlajxoyr 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ulagac5alifmngxbsk3x 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bfxmnxnuwhx26z74hykv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/thlguzuwyap6tn7dvnvo 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/oxldviollhiixdaonem6 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zok6hh7zdmnmbmlynv8d 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hl3hzseipbu2bjaxdxst 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gwkm1szomnywdtxgk9wt 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xcvmx28xctsnxu0rb3yb 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/z0qrdbxaspqc8tbyngby 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ugt4lsizoemp8ojpemf5 1998w\"><\/p><p><i>GPT-4 doing a coding interview. From <\/i><a href=\"https://www.youtube.com/watch?v=qbIk7-JPB2c&amp;t=1991s\"><i>here<\/i><\/a><i>.<\/i><\/p><p>Still: sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it’s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <a href=\"https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no\">Yes requires the possibility of no<\/a>. If my future self looks the future AGI in the eye and feels like “oh, actually, this isn’t so scary after all,” that’s evidence that my present self is missing something, too. Here’s hoping.<\/p><h3><strong>5.2 An aside on mental health<\/strong><\/h3><p>Now: a quick caution. Here I’ve been treating guts centrally from an epistemic perspective. But we need a wise <i>practical<\/i> relationship with our guts as well. And from a practical perspective, I don’t think it’s always productive to try to smell mustard gas harder, or to make horrible things like AI doom vivid. The right dance here is going to vary person-to-person, and I won’t try to treat the topic now (though: see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for a list of resources). But I wanted to flag explicitly that staying motivated and non-depressed and so forth, in relation to a pretty scary situation, is a separate art, and one that needs to be woven carefully with the more centrally epistemic angle I’m focused on here. &nbsp;<\/p><h2>6.&nbsp;Constraints on future worrying<\/h2><p>Returning to the epistemic perspective though: let’s suppose you do trust your future credences, and you want to avoid the Bayesian “gut problems” I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.<\/p><p>In particular, you should never think that there’s more than a 1/<i>x<\/i> chance that your credence will increase by <i>x<\/i> times: i.e., never more than a 50% chance that it’ll double, never more than a 10% chance that it’ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you’ve only gone up .09% in additive terms – only nine parts in ten thousand. But you’ve also gone up by a factor of 10 – something you should’ve been at least 90% sure would never happen.<\/p><p>So suppose that right now, you identify as an “AI risk skeptic,” and you put the probability of doom very low. For concreteness, suppose that you like <a href=\"https://ineffectivealtruismblog.com/2023/04/08/exaggerating-risks-carlsmith-report/\">David Thorstad’s number<\/a>: .00002% — that is, one in five million (though: he now thinks this “too generous” – and he’s also “not convinced that we are in a position where estimating AI risk makes good methodological sense,” which I suspect is a bigger crux). This is a very low number. And it implies, in particular, that you really don’t expect to get even a <i>small amount<\/i> more worried later. For example, you need to have a maximum of .01% that you ever see evidence that puts the probability at &gt;.2%.<\/p><p>Now suppose that a few years pass, GPT-6 comes out, and lo, indeed, it is very impressive. You look GPT-6 in the eye and you feel some twinge in your gut. You start to feel a bit, well, at-least-1-percent-y. A bit not-so-crazy-after-all. Now, admittedly, you were probably surprised that GPT-6 is so good. You were a “timelines skeptic,” too. But: how much of a skeptic? Were you, for example, less than one in fifty thousand that GPT-6 would be this impressive? That’s what your previous number can easily imply, if the impressiveness is what’s driving your update.<\/p><p>And now suppose that actually, you weren’t much of a timelines skeptic at all. GPT-6, according to you, is right on trend. You’d seen the scaling laws. You were at &gt;50% on at-least-this-impressive. It was predictable. It’s just that the rest of the argument for doom is dumb.<\/p><p>In that case, though, hmm. Your gut’s got heavy constraints, in terms of twinging. &gt;50% on at least-this-impressive? So: you’re still supposed to be at less than .00004% on doom? But what if you’re not…<\/p><p>Or maybe you think: “the argument for doom has not been satisfactorily peer-reviewed. <a href=\"https://marginalrevolution.com/marginalrevolution/2023/04/this-gpt-4-answer-speaks-for-itself.html\">Where’s the paper in <i>Nature<\/i><\/a>? Until I see conventional academic signals, I am at less than one in a thousand on doom, and I shall tweet accordingly.” OK: but, the Bayesianism. If you’re at less than one in a thousand, now, and your big thing is academic credibility, where should Bayes put you later, conditional on <i>seeing<\/i> conventional academic signals? And what’s your probability on such strange sights? In five years, or ten years, are you confident there won’t be a paper in <i>Nature<\/i>, or an equivalent? If it’s even 10% percent likely, and it would take you to more than 1%, your number now should be moving ahead of time.<\/p><p>Or maybe you thought, in the past: “until I see the experts worrying, I’m at less than 1%.” Well, <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">here we are<\/a> (here we already were, but more now). But: what was your probability that we ended up here? Was it so hard to imagine, the current level of expert sympathy? And are future levels of greater sympathy so hard to imagine, now? It’s easy to live, only, in the present – to move only as far as the present has moved. But the Bayesian has to live, ahead of time, in all the futures at once.<\/p><p>(Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower? So little hope of future hope?)<\/p><p>Now: all of this is “in theory.” In practice, this sort of reasoning requires good taste. I talk about such taste more below. First, though, I want to look at the theory a bit more.<\/p><h2>7.&nbsp;Should you expect low probabilities to go down?<\/h2><p>Above I said that actually, the <i>direction<\/i> of a future update is often predictable. But notice: <i>which direction<\/i> should you predict? My sense is that in many evidential situations (though not all – more below), you should think your future evidence more likely to move you in the right direction than the wrong one. So if you think that <i>p<\/i> is likely to be true, you should generally think that your future evidence is likely to update you towards higher credence on <i>p<\/i>. And vice versa: if you think that p is more likely to be <i>false<\/i>, you should expect to have <i>lower<\/i> credence on it later.<\/p><p>The Trump example above is an extreme case. You’re at 99% on Trump winning, and you’re also at 99% that you’ll update, in future, towards higher credence on Trump winning. And we can imagine a more intermediate case, where, let’s say, you’re at 90% that Trump is going to win, and you’re about to watch the presidential debate, and you think that winning the debate is highly correlated with winning the election. Which direction should you predict that your credence on Trump winning will move, once the debate is over? Given that you think Trump is likely to win the election, I think you should think he’s likely to win the debate, too. And if he wins the debate, your credence on him winning the election will go up (whereas if he loses, it’ll go down a bunch more).<\/p><p>Or consider a scientist who doesn’t believe in God. In principle, at each moment, God could appear before her in a tower of flames. She has some (very small) credence on this happening. And if it happened, she would update massively towards theism. But <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">absence of evidence is evidence of absence<\/a>. Every moment she <i>doesn’t<\/i> observe God appearing before her in a tower of flames, she should be updating some tiny amount towards atheism. And because she predicts very hard that God will never appear before her in a tower of flames, she should be predicting very hard that she will become a more and more confident atheist over time, and that she’ll die with even less faith than she has now.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eufqrpvhqgyjzngzhejg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nq26vo1twqjfrp8lugux 795w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cx4bl4je69by5syhvvxj 233w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/brnbm1vlo7vkotqm5jdn 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cehrsyb68kukjamyah8p 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qkjyknw4irznm7raz4ys 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mgu5fwmjcrbl7i94t9do 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ruqpj7osmhzbemurme65 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zwv61bykcbqn6jfauqqe 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/js674olpdq3nnneki6g0 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/yreunwtnysfogdbqnne2 1148w\"><\/p><p><i>Updating so hard right now… (Image source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bourdon,_S%C3%A9bastien_-_Burning_bush.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><p>So too, one might think, with AI risk. If you are currently an AI risk skeptic, plausibly you should expect to become more and more confidently skeptical over time, as your remaining uncertainties about the case for non-doom get resolved in the direction of truth. That is, every moment that the superintelligent machines <i>don’t<\/i> appear before you in a tower of diamondoid bacteria (that’s the story, right?), then anthropic effects aside, you should be breathing easier and easier. Or, more realistically, you should be expecting to see, well, whatever it is that comforts you: i.e., that we’ll hit another AI winter; or that we’ll make lots of progress in mechanistic interpretability; or that innovations in RLHF will allow superhuman oversight of AI behavior humans can’t understand; or that we won’t see any signs of deception or reward hacking; or that progress will be slow and gradual and nicely coordinated; or that we’ll finally, <i>finally<\/i>, get some peer review, and put the must-be-confusions to rest. And as your predictions are confirmed, you should be feeling safer and safer.<\/p><p>Is that what you expect, in your heart? Or are you, perhaps, secretly expecting to get more worried over time? I wished I’d asked myself harder. In particular: my 5% was plausibly implying some vibe like: “sure, there are these arguments that superintelligent AI will disempower us, and I give them some weight, but at least if we’re able to think well about the issue and notice the clues that reality is giving us, over time it will probably become clearer that these arguments are wrong/confused, and we’ll be able to worry much less.” Indeed, depending on the volatility of the evidence I was expecting, perhaps I should have thought that I was likely to be in the ballpark of the highest levels of worry about doom that I would ever endorse. But if you’d asked me, would I have said that?<\/p><p>That said, I actually think these dynamics are more complicated than they might initially seem. In particular, while I find it plausible that you should generally predict that you’ll update in the direction of what you currently expect to be true, sometimes, actually, you shouldn’t. And some non-crazy views on AI risk fit the mold.<\/p><p>Katja Grace suggested to me some useful examples. Suppose that you’re in a boat heading down a river. You at 80% that there’s a waterfall about two miles down, but 20% that there isn’t, and that you’re going to see a sign, a mile down, saying as much (“No waterfall” – classic sort of sign). Conditional on no sign/there being a waterfall, you’re at 10% that it’s a big waterfall, which will kill you, and 90% that it’s a small waterfall, which you’ll survive. So currently, your credence on dying is 8%. However, you’re also at 80% that in a mile, it’s going to go up, to 10%, despite your also predicting, now, that this is an update towards higher credence on something that probably won’t happen.<\/p><p>Or a consider a more real-world example (also from Katja). At 3 pm, you’re planning to take a long car trip. But there’s a 10% chance the trip will fall through. If you take the trip, there’s some small chance you get in an accident. As you approach 3 pm, your credence in “I will get in a car accident today” should go up, as the trip keeps (predictably) not-falling-through. And then, as you’re driving, it should go down gradually, as the remaining time in the car (and therefore, in danger) shrinks.<\/p><p>Some views on AI – including, skeptical-of-doom views – look like this. Suppose, for example, you think AGI-by-2070 more likely than not. And suppose that conditional on AGI-by-2070, you think there’s some small risk that the doomers are right, and we all die. And you think it’s going to be hard to get good evidence to rule this out ahead of time. Probably, though, we’ll make it through OK. And conditional on no-AGI-by-2070, you think we’re almost certainly fine. Here, you should plausibly expect to get more worried over time, as you get evidence confirming that yes, indeed, AGI-by-2070; yes, indeed, waterfall ahead. And then to get less worried later, as the waterfall proves small.<\/p><p>That said, this sort of dynamic requires specific constraints on what evidence is available, when. The truth about the future must fail to leak backwards into the past. You must be unable to hear the difference between a big waterfall and a small waterfall sufficiently ahead-of-time. The gas ahead must not waft.<\/p><p>Car accidents are indeed like this. People rarely spend much time with high credence that they’re about to get in a car accident. Their probability is low; and then suddenly it jumps wildly, split-second high, before death, or some bang-crunch-jerk, or a gasping near-miss.<\/p><p>Is AI risk like this too? Doomers sometimes talk this way. You’ll be cruising along. Everything will be looking rosy. The non-doomers will be feeling smug. Then suddenly: bam! The nanobots, from the bloodstream, in the parlor, Professor Plum. The clues, that is, didn’t rest on the details. A lot of it was obvious a priori. You should’ve read more LessWrong back in the 2000s. You should’ve looked harder at those <a href=\"https://twitter.com/ESYudkowsky/status/1500863629490544645\">empty strings<\/a>.<\/p><p>Now, sometimes this sort of vibe seems to me like it wants to have things both ways. “I shall accept ahead-of-time empirical evidence that I am right; but in the absence of such evidence, I shall remain just as confident.” “My model makes no confident predictions prior to the all-dropping-dead thing – except, that is, the ones that I want to claim credit for after-the-fact.” Here I recall a conversation I overheard back in 2018 about “<a href=\"https://arbital.com/p/daemons/\">optimization daemons<\/a>” (now: <a href=\"https://arxiv.org/abs/1906.01820\">mesa-optimizers<\/a>, <a href=\"https://arxiv.org/abs/2210.01790\">goal mis-generalization<\/a>, etc) in which a worrier said something like: “I will accept empirical arguments for concern, but only a priori arguments for comfort.” It was an offhand remark, but still: <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">not how it works<\/a>.<\/p><p>However: I do think, unfortunately, there are risks of gas that doesn’t waft well; “<a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">King Lear problems<\/a>”; risks of <a href=\"https://www.planned-obsolescence.org/the-training-game/\">things looking fairly fine, right before they are very non-fine indeed<\/a>. But not all the gas is like this. We should expect to get clues (indeed, we should <a href=\"https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment\"><i>dig hard for them<\/i><\/a>)<i>.<\/i> So we should expect, at some point, to start updating in the right direction. But I think it’s an open question how the sequencing here works, and it’ll depend on the details driving your particular view. In general, though, if you’re currently at more-likely-than-not on hitting an AGI waterfall sometime in the coming decades, but not certain, then prima facie, and even if your p(doom) is low, that’s reason to expect to get more worried as that soothing sign – “AI winter,” “It was all fake somehow” (classic sign) – fails to appear.<\/p><p>That said, even if you’re getting predictably <i>more <\/i>worried, there are still Bayesian constraints on <i>how much<\/i>. In the waterfall case, you go up 2%; in the car case, something tiny. So if you’re finding yourself, once you don’t see the sign, jumping to 50% on “death by big waterfall” – well, hmm, according to your previous views, you’re saying that you’re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying? Is the evidence you’re seeing now, re: big-waterfall, actually surprising relative to what you expected before? Looks a lot like the predicted river to me. Looks, indeed, “just like they said.” Or did your gut, maybe, not really believe …<\/p><h2><strong>8. Will the next president be a potato?<\/strong><\/h2><p>OK, that was a bunch of stuff about basic Bayesian belief dynamics. And armed with this sort of relatively crisp and simple model, it can be easy to start drawing strong conclusions about how you, with your mushy monkey brain, should be reasoning in the practice, and what sorts of numbers should be coming out of your mouth, when you make number-noises.<\/p><p>But the number-noise game takes taste. It’s a new game. We’re still learning how to play well, and productively. And I think we should be wary of possible distortions, especially with respect to small-probabilities.<\/p><p>Consider, for example, the following dialogue:<\/p><blockquote><p><i>Them<\/i>: What’s your probability that the next president is a potato?<\/p><p><i>You<\/i>: What?<\/p><p><i>Them<\/i>: A potato. Like, a normal potato. Up there getting inaugurated and stuff.<\/p><p><i>You<\/i>: Umm, very low?<\/p><p><i>Them<\/i>: Say a number!<\/p><p><i>You<\/i>: [blank stare]<\/p><p><i>Them<\/i>: You are a Bayesian and must have a number, and I demand that you produce it. Just literally say any number and I will be satisfied.<\/p><p><i>You<\/i>: Fine. One in 10^50. &nbsp;<\/p><p><i>Them<\/i>: What? Really? Wow that’s so stupid. I can’t believe you said that.<\/p><p><i>You<\/i>: Actually, let’s say one in 10^40.<\/p><p><i>Them<\/i>: Wait, your number was more than a billion times lower a second ago. If you were at one in 10^50 a second ago, you should’ve been at less than one-in-a-billion that you’d ever move this high. Is the evidence you’ve got since then so surprising? Clearly, you are a bad Bayesian. And I am clever!<\/p><p><i>You<\/i>: This is a dumb thing.<\/p><\/blockquote><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh 1019w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezpszxdmdzq3uy3p8qhi 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rpg5vqlsuzidqen5c4di 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qc9y8vkry2clju0stdwh 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zyz51cimtve586t3zhuu 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mwkiyj92xdccwxuj86ot 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vgf2m89mvbsmu2mvb7ey 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/x2h0uk8rtwyxbmai7hnz 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sg2q1gjcssoc8yjl6bo7 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pacecvldaxuy90uwyzir 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ypgoixgh1fm3krr8eijy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdfuisvwrauqypkabjsq 1156w\"><\/p><p><i>Not like this: a normal potato.<\/i><\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/h8ciyvdtf5bl1jlfhnoy\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xwhen7t5tpcuk3ljla0p 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/j344jhvcsibpfq3feglm 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvsgwlqjofhfhzgkbbvp 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qvhouh0jdskz5w5fddeb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/slgjc8ozooac97bov1v0 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/r94awwenrlstpysths62 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kz7lufvzxvkannecezen 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycxedzguhkdur5bbniyl 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gpsgvjn4rhx1viuk60kq 982w\"><\/p><p><i>Closer…<\/i><\/p><p>The “them” vibe, here, seems dubiously helpful. And in particular, in this case, it’s extra not-helpful to think of “you” as changing your probabilities, from one second to the next, by updating some fully-formed probability distribution over Potato-2024, complete with expected updates based on all the possible next-thoughts-you-could-think, reactions “them” might have, and so on. That’s, just, not the right way to understand what’s going on with the fleshy creatures described in this dialogue. And in general, it can be hard to have intuitions about <a href=\"https://markxu.com/strong-evidence\">strong evidence<\/a>, and extreme numbers make human-implemented Bayesian especially brittle. &nbsp;<\/p><p>Now, to be clear: I think that debates about the rough quantitative probability of AI doom are worth engaging in, and that they are in fact (unfortunately) very different from debates about Potato-2024. Still, though, that old lesson looms: do not confuse your abstract model of yourself with yourself. The map is never the territory; but especially not when you’re imagining a map that would take a <a href=\"https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#i-the-universal-distribution\">hyper-computer to compute<\/a>. Fans of basic Bayesianism, and of number-noises, are <a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\">well-aware of this<\/a>; but the right dance, in practice, remains an open question.<\/p><p>As an example of a distortion I worry about with respect to the previous discussion: in practice, lots of people (myself included – but see also Christiano <a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">here<\/a>) report volatility in their degree of concern about p(doom). Some days, I feel like “man, I just can’t see how this goes well.” Other days I’m like: “What was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific…”<\/p><p>Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you’ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you’re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you’re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7yx0orvjyje\"><sup><a href=\"#fn7yx0orvjyje\">[17]<\/a><\/sup><\/span><\/p><p>Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out (e.g., you can be at an <i>average<\/i> of 10% doom conditional on GPT-6 being “scary smart,” but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you’ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you’re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you’re not tracking – for example, the rest of the world starting to agree.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref72q8oh08rre\"><sup><a href=\"#fn72q8oh08rre\">[18]<\/a><\/sup><\/span><\/p><p>And there are other more familiar risks of just getting even the basic Bayesianism wrong. Maybe, for example, you notice that your beliefs have been trending in a certain direction. Trump keeps moving up in the polls, say. Now you’re at like 95% on Trump win. And you read a tweet like <a href=\"https://twitter.com/NPCollapse/status/1626854680260231169\">Connor Leahy’s<\/a>, below, telling you to “just update all the way, bro” and so you decide, shit, I’ll just go 100%, and assume that Trump <i>will<\/i> win. Wouldn’t want to predictably update later, right?<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn 817w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/i4j91yqvpuobz2rqw868 239w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/squfak3qevyosnflwfat 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cbt8xvuobl9fjnxmnnts 1226w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfbghfvnj0hz0yr5y68f 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezw8jwyifhypezexpmgm 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ad6hz0opqqfrynr3obos 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noszolhwi0fj9edhok5p 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vdss52lqyqxmauxbljp1 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kyvvrkntm51gcrytmqme 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdygqwvjmkb0vjcy7sgq 1398w\"><\/p><p>Or maybe you hear some <a href=\"https://www.facebook.com/yudkowsky/posts/10160260422389228\">prominent doomer proclaiming that “sane people with self-respect” don’t update predictably<\/a>, without clarifying about “in expectation” despite <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">definitely knowing about this<\/a>, and so you assume you must be unsane and self-hating. Or maybe you think that if you do update predictably, it should at least be in the direction of your currently-predicted truth, and you forget about cases like the waterfalls above.<\/p><p>In general, this stuff can get tricky. We should be careful, and not self-righteous, even when the math itself is clear. &nbsp;<\/p><h2>9. Just saying “oops”<\/h2><p>I also want to add a different note of caution, about not letting consistency, or your abstract picture of what “good Bayesianism” looks like, get in the way of updating as fast as possible to the right view, whatever that is.<\/p><p>Thus, for example, maybe you tweeted a bunch in the past re: “no way” on AI risk, and acted dismissive about it. Maybe, even, you’re someone like David Thorstad, and you were kind enough to quantify your dismissiveness with some very-low number.<\/p><p>And let’s say, later, your gut starts twinging. Maybe you see some scary demo of deceptiveness or power-seeking. Maybe you don’t like the look of all those increasingly-automated, AI-run wet-labs. Maybe it all just starts changing too fast, and it feels too frenetic and out of control, and do we even understand how these systems are working? Maybe it’s something about those new drones. &nbsp;<\/p><p>It might be tempting, here, to let your previous skepticism drag your new estimates downwards – including on the basis of the sorts of dynamics discussed above. Maybe, for example, if you had David Thorstad’s number, you’re tempted to move from .00002% to something like, hmm, 20%? But you say to yourself “wait, have I really gotten <i>such strong evidence<\/i> since my previous estimate? Have I been <i>so surprised<\/i> by the demos, and the drones, and the wet-labs? Apparently, I’m moving to a number I should’ve been less than one-in-a-million I’d ever end up at. By my previous lights, isn’t that unlikely to be the right move?”<\/p><p>But the thing is: it’s possible that your previous estimate was just … way too low. And more (gasp), that it didn’t come with some well-formed probability distribution over your future estimates, either. We should be wary, in general, of taking our previous (or our current) Bayesian rigor too seriously. Should “you,” above, refrain from changing her potato-2024 estimate quickly as she thinks about it more, on grounds that it would make her two-seconds-ago self’s Bayesianism look bad? Best to just get things right.<\/p><p>Of course, it may be that your previous self was tracking some sort of evidence that you’re losing sight of, now. It may be that your gut is skittish. You should try to learn from your previous self what you can. But you should try, I suspect, to learn harder from the actual world, there in front of you.<\/p><p>Here, to be clear, I’m partly thinking about myself, and my own mistakes. I said 5% in 2021. I more than doubled my estimate soon after. &nbsp;By basic Bayes, I should’ve been less than 50%, in 2021, that this would happen. Did I really get sufficiently worrying evidence in the interim to justify such a shift? Maybe. But alternatively: whatever, I was just wrong. Best to just say oops, and to try to be righter.<\/p><p>I’m focusing on people with very low estimates on doom, here, because they tend to be more common than the converse. But everything I’m saying here holds for people with low estimates on non-doom, too. If you’re such a person, and you see signs of hope later, don’t be attached to your identity as a definitely-doomer, or to the Bayesian rigor of the self that assumed this identity. Don’t practice your pessimism over-hard. You might miss the thing that saves your life.<\/p><p>Really, though, I suspect that respect for your previous self’s Bayesianism is not the main barrier to changing our minds fast enough. Rather, the barriers are more social: embarrassment stuff, tribal stuff, status stuff, and so on. I think we should try to lower such barriers where possible. We should notice that people were wrong; but we should not make fun of them for changing their minds – quite the contrary. Scout mindset is hard enough, and the stakes are too high.<\/p><h2>10. Doing enough<\/h2><blockquote><p><i>\"I imagine death so much it feels more like a memory…\"<\/i><\/p><p><i>- <\/i><a href=\"https://youtu.be/BQ1ZwqaXJaQ?t=97\"><i>Hamilton<\/i><\/a><\/p><\/blockquote><blockquote><p><i>“When my time is up, have I done enough?”<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=_gnypiKNaJE\"><i>Eliza<\/i><\/a><\/p><\/blockquote><p>I’ll close by noting a final sort of predictable update. It’s related to the scans thing.<\/p><p>There’s <a href=\"https://www.youtube.com/watch?v=W9vj2Wf57rQ\">a scene<\/a> at the end of <i>Schindler’s List<\/i>. World War II is over. Schindler has used his money to save more than 1,100 lives from the holocaust. As the people he has saved say goodbye, Schindler breaks down:<\/p><blockquote><p>I could have got more out. I could have got more. I don’t know. If I’d just… I could have got more… I threw away so much money. You have no idea… I didn’t do enough… This car. Goeth would have bought this car. Why did I keep the car? Ten people right there. Ten people. Ten more people. This pin. Two people. This is gold. Two more people. He would have given me two for it, at least one. One more person. A person, Stern. For this. I could have gotten one more person… and I didn’t.<\/p><\/blockquote><p>Now, we need to be careful here. It’s easy for the sort of stuff I’m about to say to prompt extreme and unbalanced and unhealthy relationships to stuff that matters a lot. In particular, if you’re tempted to be in some “emergency” mode about AI risk (or, indeed, about some other issue), and to start burning lots of resources for the sake of doing everything you can, I encourage you to read <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai\">this article<\/a>, together with <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai?commentId=Htf2v79w5QoQJbysS#comments\">this comment<\/a> about memetic dynamics that can amplify false emergencies and discourage clear thinking.<\/p><p>Still, still. There’s a possible predictable update here. If this AI stuff really happens, and the alignment stuff is looking rough, there is a way we will each feel about what we did with the time we had. How we responded to what we knew. What role we played. Which directions we pointed the world, or moved it. How much we left on the field.<\/p><p>And there is a way we will feel, too, about subtler things. About what sorts of motivations were at play, in how we oriented towards the issue. About the tone we took on twitter. About the sort of <a href=\"https://joecarlsmith.com/2022/12/23/on-sincerity\">sincerity<\/a> we had, or didn’t have. One thing that stayed with me from <i>Don’t Look Up<\/i> is the way the asteroid somehow slotted into the world’s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word “shame.” Death does not discriminate between the sinners and the saints. But I do actually think it’s worth talk of dignity.<\/p><p>And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I’m particularly interested in “let’s see if we can actually do this.” Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.<\/p><p>And if we do this, then there is a way we will feel when it’s done. I have a friend who sometimes talks about what he wants to tell his grandchildren he did, during the years leading up to AGI. It’s related to that thing about history, and who its eyes are on. We shouldn’t need people to tell our stories; but as far as I can tell, if he ever has grandchildren, they should be proud of him. May he sit, someday, under his own vine and fig tree.<\/p><p>Of course, there is also a way we will feel if AGI happens, but the problem was unreal, or not worth worrying about. There are <a href=\"https://www.planned-obsolescence.org/the-costs-of-caution/\">costs of caution<\/a>. And of course, there is a way we will feel if all this AGI stuff was fake after all, and all that time and money and energy was directed at a fantasy. You can talk about “reasonable ex ante,” but: will it have been reasonable? If this stuff is a fantasy, I suspect it is a fantasy connected with our flaws, and that we will have been, not innocently mistaken, but actively foolish, and maybe worse. Or at least, I suspect this of myself.<\/p><p>Overall, then, there are lots of different possible futures here. As ever, the Bayesian tries to live in all of them at once. Still: if, indeed, we are running out of time, and there is a serious risk of everyone dying, it seems especially worth thinking ahead to hospitals and scans; to what we will learn, later, about “enough” and “not enough,” about “done” and “left undone.” Maybe there will be no history to have its eyes on us – or at least, none we would honor. But we can look for ourselves. &nbsp;&nbsp;<\/p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0g9sn95iiqe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0g9sn95iiqe\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>To be clear: there are lots of other risks from AI, too. And the basic dynamics at stake in the essay apply to your probabilities on any sorts of risks. But I want to focus on existential risk from misalignment, here, and I want the short phrase “AI risk” for the thing I’m going to be referring to repeatedly.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn048nj5dhr9di\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref048nj5dhr9di\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though, the specific numbers here can matter – and there are some cases where despite having low probabilities on doom now, you can predict ahead of time that you’ll be at least somewhat more worried later (though, there are limits to how much). More below.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqnj4snix8n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqnj4snix8n\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though with respect to AI doom, not risk free – see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for some mental health resources.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz4yjec5asvf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz4yjec5asvf\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Hopefully not more literally similar. But: a new thing-not-imagined-very-well.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1dgio52qsrw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1dgio52qsrw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Modulo some futurisms. Including, importantly, ones predictably at stake in AI progress.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4m2cfburfm7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4m2cfburfm7\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>“In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.”<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4y75ebmy7eh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4y75ebmy7eh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5gj8wfkheeo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5gj8wfkheeo\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Some forecasts have self-fulfilling elements, especially with respect to Moloch-like problems. And there are questions about e.g. internet text increasing the likelihood of AIs acting out the role of the scary-AI.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh3ar4lqw084\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh3ar4lqw084\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>See e.g. Scott Alexander <a href=\"https://astralcodexten.substack.com/p/mantic-monday-31422\">here<\/a>. Some of <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably#fn6am2fn0yyve\">Yudkowsky’s public comments<\/a> suggest this model as well, though his original discussion of “<a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">conservation of expected evidence<\/a>” does not.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4ohd2xn7yql\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4ohd2xn7yql\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Here I’m indebted to discussion from <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably\">Greg Lewis<\/a> and <a href=\"https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence\">Abram Demski<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc1l5t3jttlk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc1l5t3jttlk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>.99*1 + .01*0 = .99.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno3exn1kvfm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo3exn1kvfm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Or put another way: you want to find the area that red occupies, which is the area of the first, smaller red box, plus the area of the bigger red box. Each box occupies a percentage of the area of a “column” (combination of white box and red box) associated with a hypothesis about GPT-6. So to find the area of a given red box, you take the area of the column it’s in (that is, the probability on the relevant hypothesis about GPT-6), and multiply that by the percentage of that column that is red (e.g., the probability of doom conditional on that hypothesis). Then you add up the areas of the red boxes.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt4taauzgccn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft4taauzgccn\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Daniel Kokotajlo for highlighting some of these dynamics to me years ago. See also his review of my power-seeking AI report <a href=\"https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#heading=h.e9o5m3fab0ua\">here<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5zb8pjk25\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5zb8pjk25\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>I added an edit to this effect.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu9z9f1zd3ci\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu9z9f1zd3ci\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion here.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6gbdnzllii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6gbdnzllii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Here I’m inspired by some comments from Richard Ngo.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7yx0orvjyje\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7yx0orvjyje\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn72q8oh08rre\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref72q8oh08rre\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Again, thanks to Katja for pointing to this dynamic.<\/p><\/div><\/li><\/ol>","wordCount":10753,"htmlHighlight":"<p>(Cross-posted from <a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk\">my website<\/a>. Podcast version <a href=\"https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk\">here<\/a>, or search \"Joe Carlsmith Audio\" on your podcast app.)<\/p><blockquote><p><i>\"This present moment used to be the unimaginable future.\"<\/i><\/p><p><i>- Stewart Brand<\/i><\/p><\/blockquote><h2>1. Introduction<\/h2><p>Here’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0g9sn95iiqe\"><sup><a href=\"#fn0g9sn95iiqe\">[1]<\/a><\/sup><\/span><\/p><p>Now, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">signing letters<\/a> and <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">quitting their jobs<\/a>.<\/p><p>But if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref048nj5dhr9di\"><sup><a href=\"#fn048nj5dhr9di\">[2]<\/a><\/sup><\/span>&nbsp;When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.<\/p><p>But I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <a href=\"https://agentgpt.reworkd.ai/\">Agent-GPT<\/a>-8, or <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/\">Chaos-GPT<\/a>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.<\/p><p>This essay is about what “doing this right” looks like. In particular: part of what happens, when you meet something in the flesh, is that it “seems more real” at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; “up close” vs. “far away.” My views on this have changed over the years: and in particular, I now put less weight on my gut’s (comparatively skeptical) views about doom.<\/p><p>But the essay is also about grokking s... <\/p>","plaintextDescription":"(Cross-posted from my website. Podcast version here, or search \"Joe Carlsmith Audio\" on your podcast app.)\n\n> \"This present moment used to be the unimaginable future.\"\n> \n> - Stewart Brand\n\n\n1. INTRODUCTION\nHere’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.[1]\n\nNow, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts signing letters and quitting their jobs.\n\nBut if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.[2] When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.\n\nBut I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or Agent-GPT-8, or Chaos-GPT-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.\n\nThis essay is about what “doing this right” looks like. I"},"Revision:bHozHrQD4qxvKdfqq_moderationGuidelines":{"_id":"bHozHrQD4qxvKdfqq_moderationGuidelines","__typename":"Revision","html":""},"Revision:3uE2pXvbcnS9nnZRE_description":{"_id":"3uE2pXvbcnS9nnZRE_description","__typename":"Revision","htmlHighlight":"<p><strong>World Modeling<\/strong> is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, <i>finding out how the damn thing works,<\/i> and writing it down for the rest of us.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.<\/i><\/p><p>—<a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u>Twelve Virtues of Rationality<\/u><\/a><\/p><\/blockquote><hr><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics<\/a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics<\/a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology<\/a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics<\/a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming<\/a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers<\/a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews<\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History<\/a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics<\/a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies<\/a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease<\/a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis<\/a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution<\/a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness<\/a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution<\/a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine<\/a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience<\/a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus<\/a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence<\/a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review<\/a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models<\/a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science<\/a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets<\/a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism<\/a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><p>&nbsp;<\/p><h2>A definition by elimination<\/h2><p>Properly considered, the overwhelming majority of content LessWrong is about <i>modeling how the world is<\/i>, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a>, ... <\/p>"},"Tag:3uE2pXvbcnS9nnZRE":{"_id":"3uE2pXvbcnS9nnZRE","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:3uE2pXvbcnS9nnZRE_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"World Modeling","shortName":null,"slug":"world-modeling","core":true,"postCount":3682,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":27,"createdAt":"2020-06-14T22:24:50.898Z","wikiOnly":false,"deleted":false,"isSubforum":null,"noindex":false},"Revision:sYm3HiWcfZvrGu3ui_description":{"_id":"sYm3HiWcfZvrGu3ui_description","__typename":"Revision","htmlHighlight":"<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment&nbsp;<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">Eliciting Latent Knowledge (ELK)<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability<\/a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://aisafety.world/map/\">Full map here<\/a><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center\">Alignment Resea<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>..."},"Tag:sYm3HiWcfZvrGu3ui":{"_id":"sYm3HiWcfZvrGu3ui","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:sYm3HiWcfZvrGu3ui_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"AI","shortName":null,"slug":"ai","core":true,"postCount":6675,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T22:24:22.097Z","wikiOnly":false,"deleted":false,"isSubforum":null,"noindex":false},"Revision:jHzwoFd2MhZt9eeqJ_biography":{"_id":"jHzwoFd2MhZt9eeqJ_biography","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2023-02-28T19:17:47.501Z","userId":"jHzwoFd2MhZt9eeqJ","html":"<p>Senior research analyst at Open Philanthropy. Recently completed a doctorate in philosophy at the University of Oxford. Opinions my own.<\/p>","wordCount":20,"htmlHighlight":"<p>Senior research analyst at Open Philanthropy. Recently completed a doctorate in philosophy at the University of Oxford. Opinions my own.<\/p>","plaintextDescription":"Senior research analyst at Open Philanthropy. Recently completed a doctorate in philosophy at the University of Oxford. Opinions my own."},"Post:bHozHrQD4qxvKdfqq":{"_id":"bHozHrQD4qxvKdfqq","__typename":"Post","tableOfContents":{"html":"<p>(Cross-posted from <a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk\">my website<\/a>. Podcast version <a href=\"https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk\">here<\/a>, or search \"Joe Carlsmith Audio\" on your podcast app.)<\/p><blockquote><p><i>\"This present moment used to be the unimaginable future.\"<\/i><\/p><p><i>- Stewart Brand<\/i><\/p><\/blockquote><h2 id=\"1__Introduction\">1. Introduction<\/h2><p>Here’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0g9sn95iiqe\"><sup><a href=\"#fn0g9sn95iiqe\">[1]<\/a><\/sup><\/span><\/p><p>Now, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">signing letters<\/a> and <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">quitting their jobs<\/a>.<\/p><p>But if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref048nj5dhr9di\"><sup><a href=\"#fn048nj5dhr9di\">[2]<\/a><\/sup><\/span>&nbsp;When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.<\/p><p>But I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <a href=\"https://agentgpt.reworkd.ai/\">Agent-GPT<\/a>-8, or <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/\">Chaos-GPT<\/a>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.<\/p><p>This essay is about what “doing this right” looks like. In particular: part of what happens, when you meet something in the flesh, is that it “seems more real” at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; “up close” vs. “far away.” My views on this have changed over the years: and in particular, I now put less weight on my gut’s (comparatively skeptical) views about doom.<\/p><p>But the essay is also about grokking some basic Bayesianism about future evidence, dispelling a common misconception about it (namely: that directional updates shouldn’t be predictable in general), and pointing at some of the constraints it places on our beliefs over time, especially with respect to stuff we’re currently skeptical or dismissive about. For example, at least in theory: you should never think it &gt;50% that your credence on something will later double; never &gt;10% that it will later 10x, and so forth. So if you’re currently e.g. 1% or less on AI doom, you should think it’s less than 50% likely that you’ll ever be at 2%; less than 10% likely that you’ll ever be at 10%, and so on. And if your credence is very small, or if you’re acting dismissive, you should be very confident you’ll never end up worried. Are you?<\/p><p>I also discuss when, exactly, it’s problematic to update in predictable directions. My sense is that generally, you should expect to update in the direction of the <i>truth<\/i> as the evidence comes in; and thus, that people who think AI doom unlikely should expect to feel <i>less worried<\/i> as time goes on (such that consistently getting more worried is a red flag). But in the case of AI risk, I think at least some non-crazy views should actually expect to get <i>more worried<\/i> over time, even while being fairly non-worried now. In particular, if you think you face a small risk conditional on something likely-but-not-certain (for example, AGI getting developed by blah date), you can sometimes expect to update towards facing the risk, and thus towards greater worry, before you update towards being safe. But there are still limits to how much more worried you can predictably end up.<\/p><p>Importantly, none of this is meant to encourage consistency with respect to views you held in the past, at the expense of reasonableness in the present or future. If you said .1% last year, and you’re at 10% now (or if you hit 90% when you see GPT-6): well, better to just say “<a href=\"https://www.lesswrong.com/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops\">oops<\/a>.” Indeed, I’ve been saying “oops” myself about various things. And more generally, applying basic Bayesianism in practice takes lots of taste. But faced with predictable progress towards advanced but mostly-still-abstract-for-now AI, I think it’s good to keep in mind.<\/p><p>I close with some thoughts on how we will each look back on what we did, or didn’t do, during the lead-up to AGI, once the truth about the risks is made plain.<\/p><p><i>Thanks to Katja Grace for extensive discussion and inspiration. See also citations in the main text and footnotes for specific points and examples that originated with Katja. And thanks also to Leopold Aschenbrenner for comments. Some of my thinking and writing on this topic occurred in the context of my work for Open Philanthropy, but I’m speaking only for myself and not for my employer.<\/i><\/p><h2 id=\"2__Sometimes_predictably_real_stuff_doesn_t_feel_real_yet\">2. Sometimes predictably-real stuff doesn’t feel real yet<\/h2><blockquote><p><i>\"Every year without knowing it I have passed the day<\/i><\/p><p><i>When the last fires will wave to me<\/i><\/p><p><i>And the silence will set out<\/i><\/p><p><i>Tireless traveler<\/i><\/p><p><i>Like the beam of a lightless star\"<\/i><\/p><p><i>- <\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p>I first heard about AI risk in 2013. I was at a picnic-like thing, talking with someone from the Future of Humanity Institute. He mentioned AI risk. I laughed and said something about “like in the movie <i>I, Robot<\/i>?” He didn’t laugh.<\/p><p>Later, I talked with more people, and read Bostrom’s <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=\">Superintelligence<\/a>. I had questions, but the argument seemed strong enough to take seriously. And at an intellectual level, the risk at stake seemed like a big deal.<\/p><p>At an emotional level, though, it didn’t <i>feel real<\/i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um … nano-bots? I wasn’t good at imagining nano-bots.<\/p><p>I remember looking at some farmland out the window of a bus, and wondering: am I supposed to think that this will all be compute clusters or something? I remember looking at a church and thinking: am I supposed to imagine robots tearing this church apart? I remember a late night at the Future of Humanity Institute office (I ended up working there in 2017-18), asking someone passing through the kitchen how to imagine the AI killing us; he turned to me, pale in the fluorescent light, and said “whirling knives.”<\/p><p>Whirling knives? <a href=\"https://twitter.com/ESYudkowsky/status/1438198189782290433\">Diamondoid bacteria<\/a>? Relentless references to paper-clips, or “tiny molecular squiggles”? I’ve written, elsewhere, about <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see#iv-realization-vs-belief\">the “unreality” of futurism<\/a>. AI risk had a lot of that for me.<\/p><p>That is, I wasn’t <i>viscerally worried<\/i>. I had the concepts. But I didn’t have the “actually” part. And I wasn’t alone. As I started working on the topic more seriously, I met some people who were viscerally freaked-out, depressed, and so on – whether for good or ill. But I met lots of people who weren’t, and not because they were protecting their mental health or something (or at least, not very consciously). Rather, their head was convinced, but not their gut. Their gut still expected, you know, <a href=\"https://www.cold-takes.com/this-cant-go-on/\">normality<\/a>.<\/p><p>At the time, I thought this was an important signal about the epistemic situation. Your gut can be smarter than your head. If your gut isn’t on board, maybe your head should be more skeptical. And having your gut on board with whatever you’re doing seems good from other angles, too.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqnj4snix8n\"><sup><a href=\"#fnqnj4snix8n\">[3]<\/a><\/sup><\/span>&nbsp;I spent time trying to resolve the tension. I made progress, but didn’t wholly sync up. To this day, nano-bots and dyson spheres and the word “singularity” still land in an abstract part of my mind – the part devoted to a certain kind of conversation, rather than to, like, the dirty car I can see outside my window, and the tufts of grass by the chain-link fence.<\/p><p>I still think that your gut can be an important signal, and that if you find yourself saying that you believe blah, but you’re not <a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\">feeling<\/a> or acting like it, you should stop and wonder. And sometimes, people/ideas that try to get you to not listen to your gut are trying (whether intentionally or not) to bypass important defenses. I am not, in what follows, trying to tell you to throw your gut away. And to the extent I am questioning your gut: please, by all means, be more-than-usually wary. Still, though, and speaking personally: I’ve come to put less stock than I used to in my gut’s Bayesian virtue with respect to AI. I want to talk a bit about why.<\/p><h2 id=\"3__When_guts_go_wrong\">3. When guts go wrong<\/h2><blockquote><p><i>\"Then I will no longer<\/i><\/p><p><i>Find myself in life as in a strange garment<\/i><\/p><p><i>Surprised at the earth…\"<\/i><\/p><p><i>-<\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p>Part of this is reflection on examples where guts go wrong, especially about the future. There are lots of candidates. Indeed, depending on how sharply we distinguish between your “system 1” and your gut, a lot of the <a href=\"https://thedecisionlab.com/biases\">biases literature<\/a> can be read as anti-gut, and a lot of early rationalism as trying to compensate. My interest in head-gut agreement was partly about trying to avoid overcorrection. But there is, indeed, something to be corrected. Here are two examples that seem relevant to predictable updating.<\/p><h3 id=\"3_1_War\">3.1 War<\/h3><blockquote><p><i>“Abstraction is a thing about your mind, and not the world… Saying that AI risk is abstract is like saying that World War II is abstract, because it’s 1935 and hasn’t happened yet. If it happens, it will be very concrete and bad. It will be the worst thing that has ever happened.”<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=j5Lu01pEDWA\"><i>Katja Grace<\/i><\/a><\/p><\/blockquote><p>I think Katja’s war example is instructive. Consider some young men heading off to war. There’s a trope, here, about how, when the war is just starting, some men sign up excitedly, with dreams of glory and honor. Then, later, they hit the gritty reality: trenches, swamps, villages burning, friends gasping and gurgling as they die. Ken Burn’s <a href=\"https://www.pbs.org/kenburns/the-vietnam-war/\">Vietnam War documentary<\/a> has some examples. See also “<a href=\"https://en.wikipedia.org/wiki/Born_on_the_Fourth_of_July_(film)\">Born on the Fourth of July<\/a>.” The soldiers return, if they return, with a very different picture of war. “<a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\">In all my dreams before my helpless sight/ He plunges at me, guttering, choking, drowning<\/a>…”<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kymdz6wnfzlkpibrhvs4 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spvydfruwc6lwytxlorb 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zn5bde6lkc3zgocekwhg 462w\"><\/p><p><i>Stretcher bearers in World War I (source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Stretcher_bearers_Passchendaele_August_1917.jpg\"><i>here<\/i><\/a><i>)<\/i><\/p><p>Now, a part of this is that their initial picture was <i>wrong<\/i>. But also, sometimes, it’s that their initial picture was <i>abstract<\/i>. Maybe, if you’d asked them ahead of time, they’d have said “oh yeah, I expect the trenches to be very unpleasant, and that I will likely have to watch some of my friends die.” But their gut didn’t expect this – or, not hard enough. Surrounded, when they set out, by flags and smiling family members and crisp uniforms, it’s hard to think, too, of flies in the eyes of rotting corpses; or trench-foot, and the taste of mustard gas. And anyway, especially if you’re heading into a very new context, it’s often hard to know the specifics ahead of time, and any sufficiently-concrete image is predictably wrong.<\/p><p>I worry that we’re heading off to something similar, epistemically, to a new war, with respect to AI risk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz4yjec5asvf\"><sup><a href=\"#fnz4yjec5asvf\">[4]<\/a><\/sup><\/span>&nbsp;Not: happily, and with dreams of glory. But still: abstractly. We’re trying to orient intellectually, and to do what makes sense. But we aren’t in connection with what it will actually be like, if AI kicks off hard, and the doomers are right. Which isn’t to say it will be trench foot and mustard gas. Indeed, even if things go horribly wrong eventually, it might actually be awesome in lots of ways for a while (even if also: extremely strange). But whatever it will be, will be a specific but very-different-from-now thing. Guts aren’t good at that. So it’s not, actually, all that surprising if you’re not as viscerally worried as your explicit beliefs would imply.<\/p><h3 id=\"3_2_Death\">3.2 Death<\/h3><blockquote><p><i>\"And who by fire, who by water<\/i><\/p><p><i>Who in the sunshine, who in the night time<\/i><\/p><p><i>Who by high ordeal, who by common trial…\"<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=ilGahIwQEQ0\"><i>Leonard Cohen<\/i><\/a><\/p><\/blockquote><p>Another famous example here is death. No one knows the date or hour. But we know: someday.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1dgio52qsrw\"><sup><a href=\"#fn1dgio52qsrw\">[5]<\/a><\/sup><\/span>&nbsp;Right? Well, sort of. We know in the abstract. We know, but don’t always realize. And then sometimes we do, and some vista opens. We reel in some new nothingness. Something burns with new preciousness and urgency.<\/p><p>And sometimes this happens, specifically, when “someday, somehow” becomes “soon, like this.” When the doctor tells you: you, by avalanche. You, by powder. The month of May. Slow decay. Suddenly, when you’re actually looking at the scans, when you’re hearing estimates in months, you learn fresh who is calling; and despite having always known, some sort of “update” happens. Did the gut not fully believe? One’s own death, after all, is <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">hard to see<\/a>.<\/p><p>I’ve <a href=\"https://joecarlsmith.com/2020/12/06/thoughts-on-being-mortal#iii\">written about this before<\/a>. Tim McGraw has a song about the scans thing. “<a href=\"https://www.youtube.com/watch?v=_9TShlMkQnc\">Live like you were dying<\/a>.” I’m trying. I’m trying to think ahead to that undiscovered hospital. I’m trying to think about what I will take myself to have learned, when I walk out into the parking lot, with only months to live. I’m trying to learn it now instead.<\/p><p>Really, this is about predictable updating. The nudge in McGraw’s title – you’re already dying – is Bayesian. You shouldn’t need the scans. If you know, now, what you’ll learn later, you can learn it now, too. Death teaches unusually predictable lessons – about fleetingness, beauty, love. And unusually important lessons, too. Bayes bites, here, with special gravity. But there’s some sort of gut problem. The question is how to learn hard enough, and in advance. “<a href=\"https://www.brainyquote.com/quotes/henry_david_thoreau_107665\">And not, when I come to die, to discover that I have not lived<\/a>.”<\/p><p>Importantly, though: if your gut thinks you’re not going to die, it’s not actually much evidence. Has your gut been keeping up with the longevity literature? Does it have opinions about cryopreservation? Futurism aside, the gut’s skepticism, here, is an old mistake. And we have practices. Go <a href=\"https://en.wikipedia.org/wiki/Ash_Wednesday\">smear some ashes on your forehead<\/a>. Go <a href=\"https://en.wikipedia.org/wiki/Sky_burial\">watch some birds eat a corpse<\/a>. Go put some fruit on the <a href=\"https://en.wikipedia.org/wiki/Ofrenda\">ofrenda<\/a>, or some flowers on your grandfather’s grave. <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">Realization is an art distinct from belief<\/a>. Sometimes, you already know. Religion, they say, is remembering.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zaqb75irgsve2gjrgzqy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jifrboveteqktdgadtzq 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vupbmg65qbzwnrptgyyv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/chpgceqfmfeilzwalbfu 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mv5ixhkytnydgxgctzlv 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bxij00lcawaqkcc12rz0 722w\"><i>Tibetan sky burial. (Source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_Bild_135-S-12-50-06,_Tibetexpedition,_Ragyapa,_Geier.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><h2 id=\"4__Noticing_your_non_confusion\">4.&nbsp;Noticing your non-confusion<\/h2><p>So these are some examples where “but my gut isn’t in a very visceral relationship with blah” just isn’t a very strong signal that blah is false. But I also want to flag some more directly AI related places where I think something gut-related has been messing up, for me.<\/p><h3 id=\"4_1_LLMs\">4.1 LLMs<\/h3><p>ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. But depending on what you count: we had scaling laws for deep learning back in <a href=\"https://arxiv.org/abs/1712.00409\">2017<\/a>, or at least <a href=\"https://arxiv.org/abs/2001.08361\">2020<\/a>. I know people who were really paying attention; who really saw it; who really bet. And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren’t, and should not have been, very surprised by the most recent round of LLMs. I was not a “shallow patterns” guy. I didn’t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.<\/p><p>But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of “oh, actually,” and “real deal,” and “fire alarm.” Some part of me was still surprised.<\/p><p>Indeed, noticing my gut (if not my head) getting surprised at various points over the past few years, I’ve realized that my gut can have some pretty silly beliefs about AI, and/or can fail to connect fairly obvious dots. For example, when I first started thinking about AI, I think some part of me failed to imagine that eventually, if AIs got smart enough, we could just <i>talk to them<\/i>, and that they would just <i>understand what we were saying<\/i>, and that interacting with them wouldn’t necessarily be some hyper-precise coding thing. I had spoken to Siri. Obviously, that didn’t count. Then, one day, I spoke, with my voice, to a somewhat-smarter AI, and it responded in a very human-sounding voice, and it was much more like talking on the phone, and some sort of update happened.<\/p><p>Similarly: I think that in the past, I failed to imagine what the visual experience of interacting with an actually-smart AI would be like. Obviously, I knew about robots; HAL’s red stare; typing commands into a terminal; texting. But somehow, old talk of AGI didn’t conjure this for me. I’m not sure what it conjured. Something about brains in boxes, except: laptops? I think it wasn’t much of anything, really. I think it was just a blank. After all, this isn’t <i>sci-fi<\/i>. So it must not be like anything you’d see in sci-fi, either, including strains aimed at realism. People, we’re talking about the <i>real future<\/i>, which means something <i>unimaginable<\/i>, hence fiction to the imagination, hence nothingness. “The future that can be named is not the true future.” Right?<\/p><p>Wrong. “Named super specifically” is more plausible, but even wariness of specificity can mislead: sometimes, even the specifics are pretty obvious. I <i>had seen<\/i> Siri, and chat bots. What sort of fog was I artificially imposing on everything? What was so hard about imagining Siri, but smarter? Now, it feels like “oh, duh.” And certain future experiences feel more concrete, too. It now feels like: oh, right, lots of future AIs will probably have extremely compelling and expressive <a href=\"https://replika.com/\">digital human avatars<\/a>. Eventually (soon?), they’ll probably be able to look just like (super-hot, super-charismatic) humans on zoom calls. What did I think it would be, R2D2?<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga 793w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nhkv2otstngfqbqrnrvf 232w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfatvn35ua9zbtn6cf9k 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/stihzpefyj3ncv4n8wsd 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axbxadmiet8vg9whnksi 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/geetxsphhzcs2gwiqnmm 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uqg443xbl8w36jlzic1z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/e9iy8gcgjvvrrxrhyidk 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/boz9le1jdcq2gbm6xpjr 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vksdqn2ggbhpe8tpeugr 1108w\"><\/p><p><a href=\"https://replika.com/\"><i>Some current AIs<\/i><\/a><\/p><p>“Oh, duh” is never great news, epistemically. But it’s interestingly <i>different<\/i> news than “<a href=\"https://www.readthesequences.com/Noticing-Confusion-Sequence\">noticing your confusion<\/a>,” or being straightforwardly surprised. It’s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it’s just like you would’ve said, if you’d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.<\/p><p>I mentioned this dynamic to Trevor Levin, and he said something about “noticing your non-confusion.” I think it’s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren’t <i>certain<\/i> you’d see it. But if it feels like your head is unconfused, but your gut is updating from “it’s probably fake somehow” to “oh shit it’s actually real,” then you probably had information your gut was failing to use.<\/p><h3 id=\"4_2_Simulations\">4.2 Simulations<\/h3><p>I’ll give another maybe-distracting example here. Last year, I spent some time thinking about <a href=\"https://jc.gatspress.com/pdf/simulation_arguments_revised.pdf\">whether we live in a computer simulation<\/a>. It’s a strange topic, but my head takes the basic argument pretty seriously. My gut, though, generally thinks it’s fake somehow, and forgets about it easily.<\/p><p>I remember a conversation I had with a friend sometime last year. He said something like: “you know, pretty soon, all sorts of intelligent agents on earth are going to be living in simulations.” I nodded or something. It’s like how: if the scientists are actually <i>putting<\/i> people’s brains in vats, it’s harder to stamp your foot and say “no way.” We moved on.<\/p><p>Then, in early April, this paper came out: “<a href=\"https://arxiv.org/pdf/2304.03442.pdf\">Generative Agents: Interactive Simulacra of Human Behavior<\/a>.” They put 25 artificial agents into an environment similar to The Sims, and had them interact, including via e.g. hosting a valentine’s day party.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4m2cfburfm7\"><sup><a href=\"#fn4m2cfburfm7\">[6]<\/a><\/sup><\/span>&nbsp;Here’s the picture from the paper:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/llnsldrmajaxuvyehy7e 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/osectulgs4qjctsror1r 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m7jurokdkjrsvjzndks9 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cr1md9m8f2hxfbzthmhp 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ki3fbycj28tlpzmniqor 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pyh1dv4xp4ecmp0zzlkr 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lgmi3qpvqtobziipibay 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/adjhthooygwkpqihboht 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rx5yxljrmgvx1p6hwzuw 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvkziwrga9fe5n09ashy 1420w\"><i>From <\/i><a href=\"https://arxiv.org/pdf/2304.03442.pdf\"><i>here<\/i><\/a><i>.<\/i><\/p><p>I opened this paper, read the beginning, looked at this picture, and felt my gut update towards being in a sim. But: c’mon now, gut! What sort of probability would I have put, last year, on “I will, in the future, see vaguely-smart artificial agents put into a vaguely-human simulated environment”? Very high. My friend had literally said as much to me months earlier, and I did not doubt. Indeed, what’s even the important difference between this paper and AlphaStar, or the original Sims?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4y75ebmy7eh\"><sup><a href=\"#fn4y75ebmy7eh\">[7]<\/a><\/sup><\/span>&nbsp;How smart the models are? The fact that it’s cute and human-like? My gut lost points, here.<\/p><p>It’s an avoidable mistake. I’m trying to stop making it.<\/p><p>I worry that we’re in for a lot of dynamics like this. How seriously, for example, are you taking the possibility that future AIs will be sentient? Well, here’s a mistake to not make: updating a lot once the AIs are using charismatic human avatars, or once they can argue for their sentience as convincingly as a human. Predict it now, people. Update now.<\/p><h3 id=\"4_3__It_s_just_like_they_said_\">4.3 “It’s just like they said”<\/h3><p>I don’t, often, have nightmares about AI risk. But I had one a few months ago. In it, I was at a roll-out of some new AI system. It was a big event, and there were lots of people. The AI was unveiled. Somehow, it immediately wrote each one of us some kind of hyper-specific, individualized message, requiring a level of fine-grained knowledge and predictive ability that was totally out of the question for any familiar intelligence. I read my message and felt some cold and electric bolt, some recognition. I thought to myself: “it’s just like they said.” I looked around me, and the room was in chaos. Everything was flying apart, in all directions. I don’t remember what happened after that.&nbsp;<\/p><p>“Just like they said.” Who’s they? Presumably, the AI worriers. The ones who think that superintelligence is not a fantasy or a discussion-on-twitter, but an actual thing we are on track to do with our computers, and which will cut through our world like butter if we get it wrong.<\/p><p>But wait: aren’t I an AI worrier? More than many, at least. But dreams, they say, are partly the gut’s domain. Perhaps the “they,” here, was partly my own explicit models. Ask me in the waking world: “will superintelligence be terrifying?” Yes, of course, who could doubt. But ask in my dreams instead, and I need to see it up close. I need to read the message. Only then will my gut go cold: “Oh, shit, it’s just like they said.”<\/p><p>I’ve had this feeling a few times in the past few months. I remember, a few years ago, making a simple model of AI timelines with a colleague. We used a concept called “wake-up,” indicating the point where the world realized what was happening with AI and started to take it seriously. I think that if, at that point, we could’ve seen what things would be like in 2023, we would’ve said something like: “yeah, that” (though: there’s a ton more waking up to do, so future wake-ups might end up better candidates).<\/p><p>Similarly, “they” have worried for ages about triggering or exacerbating “race dynamics” in AI. Then, in recent months, Google went into a “<a href=\"https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html\">Code Red<\/a>” about AI, and the CEO of Microsoft came out and just said straight up: “<a href=\"https://www.businesstoday.in/technology/news/story/the-race-starts-today-microsoft-officially-brings-chatgpt-ai-to-bing-and-edge-browser-369453-2023-02-08\">the race starts today<\/a>.”<\/p><p>“They” have worried about AIs being crazy alien minds that we don’t understand. Then, in February, we got to see, briefly, the rampaging strangeness of a good Bing – including all sorts of <a href=\"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/\">deception and manipulation and blackmail<\/a>, which I <a href=\"https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/\">don’t actually think is the centrally worrying kind<\/a>, but which doesn’t exactly seem like good news, either.<\/p><p>“They” have worried about agents, and about AIs running wild on the internet, and about humans not exactly helping with that. Now we have <a href=\"https://en.wikipedia.org/wiki/Auto-GPT\">Auto-GPT<\/a>, and <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity\">Chaos-GPT<\/a>, and I open up my browser and I see stuff like <a href=\"https://agentgpt.reworkd.ai/\">this<\/a>:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8hetjm6w9ytpgy8w88y 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/g4jx6qfqljr5pfhk8nxw 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ftts0hfa6vho5bncqnnn 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pnwqbtx5zxtg3pxoayib 2048w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hzuxg4hennshvtf0cfvn 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t3ybknd10bd72j057obw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xrtcyqsvpg7b5kdmk59o 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/awyiwkdkoqje6wgbekn1 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/aovgrdwbwi69qj8vbjcb 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jvjdpzgxpihru8qcpoau 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eykrqp9vhbqzwhosm4qj 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/acilnuziiquzylpaopwr 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ipnpetuxchsejqv1qvhv 2002w\"><i>Not the pixels I wanted to be seeing at this point in my life.<\/i><\/p><p>Now, I don’t want to litigate, here, exactly who “called” what (or: created what<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5gj8wfkheeo\"><sup><a href=\"#fn5gj8wfkheeo\">[8]<\/a><\/sup><\/span>), and how hard, and how much of an update all this stuff should be. And I think some things – for example, the world’s sympathy towards concern about risks from AI – have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from “that’s what the model says” to “oh shit here we are.” And my gut has gotten more worried.<\/p><p>Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I’ve just been messing up. Let’s look at the dynamics in more detail.<\/p><h2 id=\"5__Smelling_the_mustard_gas\">5. Smelling the mustard gas<\/h2><blockquote><p><i>\"Men marched asleep…<\/i><\/p><p><i>All went lame, all blind.\"<\/i><\/p><p><i>- <\/i><a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\"><i>Wilfred Owen<\/i><\/a><\/p><\/blockquote><p>It’s sometimes thought that, as a Bayesian, you shouldn’t be able to predict which direction you’ll update in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh3ar4lqw084\"><sup><a href=\"#fnh3ar4lqw084\">[9]<\/a><\/sup><\/span>That is, if you’re about to get some new evidence about <i>p<\/i>, you shouldn’t be able to predict whether this evidence will move your credence on <i>p<\/i> higher or lower. Otherwise, the thought goes, you could “price in” that evidence now, by moving your credence in the predicted direction.<\/p><p>But this is wrong.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4ohd2xn7yql\"><sup><a href=\"#fn4ohd2xn7yql\">[10]<\/a><\/sup><\/span>&nbsp;Consider a simple example. Suppose you’re at 99% that Trump won the election. You’re about to open the newspaper that will tell you for sure. Here, you should be at 99% that you’re about to increase your credence on Trump winning: specifically, up to 100%. It’s a very predictable update.<\/p><p>So why can’t you price it in? Because there’s a 1% chance that you’re about to lower your confidence in Trump winning <i>by a lot more<\/i>: specifically, down to 0%. That is, in <i>expectation<\/i>, your confidence in Trump winning will remain the same.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1l5t3jttlk\"><sup><a href=\"#fnc1l5t3jttlk\">[11]<\/a><\/sup><\/span>And it’s the expectation of your future update that Bayesian binds.<\/p><p>To understand this more visually, let’s use a slightly more complicated example. Suppose you’re currently at 80% that GPT-6 is going to be “scary smart,” whatever that means to you. And suppose that, conditional on GPT-6 being scary smart, your probability on AI doom is 50%; and conditional on GPT-6 not being scary smart, your probability on AI doom is 10%. So your credence looks like this:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nra6meskaht8wdx1ig3q\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tv20xlvpjkpjwxdwv0ub 942w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lvlh3bnro7dpwogepzyo 276w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dq5jtswyqsjhk6y7bpjx 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hpyup2d5e5wshq2gxp6b 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/afdb2urasvsfdidfzhts 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozzybtunbhswka5gic40 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycrmtyxtriv8em266r0z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kvz3h40wblsqw7ekj2ef 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/libzhbgptdorqbqscxzo 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fdvw3d3simncrdgnnabg 1082w\"><\/p><p>Now, what’s your overall p(doom)? Well, it’s:<\/p><blockquote><p>(probability that GPT-6 is scary smart * probability of doom conditional on GPT-6 being scary smart) + (probability that GPT-6 isn’t scary smart * probability of doom conditional on GPT-6 not being scary smart)<\/p><\/blockquote><p>That is, in this case, 42%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo3exn1kvfm\"><sup><a href=\"#fno3exn1kvfm\">[12]<\/a><\/sup><\/span><\/p><p>But now we can see a possible format for a gut-problem mistake. In particular: suppose that I ask you, right now, surrounded by flags and crisp uniforms, about the probability of doom. You query your gut, and it smells no mustard gas. So you give an answer that doesn’t smell much mustard gas, either. Let’s say, 10%. And let’s say you don’t really break things down into: OK, how much mustard gas do I smell conditional on GPT-6 being scary smart, vs. not, and what are my probabilities on that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft4taauzgccn\"><sup><a href=\"#fnt4taauzgccn\">[13]<\/a><\/sup><\/span>&nbsp;Rather, your model is an undifferentiated mass:<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f 970w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jyz00tksh6fzmsrrwzte 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mpiynbtl2td13dtrivzi 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qwppd7rqhbuwdknpkojj 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spztwjkpmryb8ikhrpw8 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zkbqpvivirrtz7prkonl 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m0vr9aby7wm9ptxoll78 722w\"><\/p><p>Or maybe you do try to break things down like that, but the waft of the gas fades with all the distance. GPT-6 is far away, behind some fog. Still: you guess, with your head, and without your gut participating, that p(doom) is indeed a bit higher conditional on GPT-6 being scary smart, what with the update towards “short timelines.” Let’s say, 20%; and 10% otherwise. So maybe your overall p(doom), given 80% on the abstract idea of GPT-6 being scary smart, is 18%.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m 957w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nvfxoxgencxt15pfc6nr 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fujyddttrzos4sjsnr9l 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kfnjiozyk6zvtp2z8w5l 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noa75ytmstppqbkogzvq 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tfjun4wdy5kyigpqeqem 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ksuqs7emajlxupcj7cen 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xbsn1amesfysfzuepwgr 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xueudncpvsogtqrrreuz 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cgrrdu5bpna1eeokyttx 1208w\"><\/p><p>But actually, let’s say, if you could see a “scary smart” GPT-6 model right now, you would freak out way harder. You would be able to smell the gas up close, that bitter tang. Your gut would get some message, and come alive, and start participating in the exercise. “<i>That thing<\/i>,” your gut might say, “is <i>scary<\/i>. I’m at 50% on doom, now.”<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24 916w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uuix302lquapruhwnzhn 268w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xuhwmnlcjpby3idnqirb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozhynvnhqgxoqfufmkbh 1375w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xxon5usbt8paffsjqvnl 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mrfaxqpbunqytyuqnoyj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eaj9dbbvdrtlonvfy2lr 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dypci8n8pncoyjj84hxo 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/k2wdfefrlolu7gwmivfl 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fsfihwtucf89yzbmjsep 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mze0y4fdbcs3amdlga41 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dwtad6aknstkayhrmglw 1414w\"><\/p><p>Thus, you end up inconsistent, and dutch-bookable (at least in principle – setting aside issues re: betting on doom). Suppose I ask you, now, to agree to sell me a “pays out $100 conditional on doom” ticket for $30 (let’s assume this can actually pay out), conditional on GPT-6 being scary smart. You’re only at 20% doom in such a world, so you predict that such a ticket will only be worth $20 to you if this deal is ever triggered, so you agree. But actually, when we get to that world, your gut freaks out, and you end up at 50% doom, and that ticket is now worth $50 to you, but you’re selling it for $30. Plus, maybe now you’re regretting other things. Like some of those tweets. And how much alignment work you did.<\/p><p>As indicated above, I think I’ve made mistakes in this vein. In particular: a few years back, I wrote a <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">report about AI risk<\/a>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.\\<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5zb8pjk25\"><sup><a href=\"#fnv5zb8pjk25\">[14]<\/a><\/sup><\/span>&nbsp;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional <\/i>on such systems being developed, I was going to look them in the eye and say (in expectation): “~92% that we’re gonna be OK, x-risk-wise.” But on reflection, that wasn’t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">billions of such machines<\/a>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.<\/p><h3 id=\"5_1_Should_you_trust_your_future_gut__though_\"><strong>5.1 Should you trust your future gut, though?<\/strong><\/h3><p>Now, you might wonder: why give credit to such future fear?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu9z9f1zd3ci\"><sup><a href=\"#fnu9z9f1zd3ci\">[15]<\/a><\/sup><\/span>After all, isn’t part of the worry about doomers that they’re, you know, fraidy-cats? Paranoids? (C’mon: it’s just a superintelligent machine, the invention of a second advanced species, the introduction of a qualitatively new order of optimization power into earth’s ecosystem. It’s just, you know, <i>change<\/i>.) And isn’t the gut, famously, a bit skittish? Indeed, if you’re worried about your gut being <i>underactive<\/i>, at a distance, shouldn’t you also be worried about it being <i>over-active, <\/i>up close? Shouldn’t you reason, instead, ahead of time, at a distance, and in a cool hour, about how scared you should be when you’re there-in-person?<\/p><p>Well, it’s a judgment call. Sometimes, indeed, at-a-distance is a better epistemic vantage point than up-close. Especially if you know yourself to have biases. Maybe, for example, you’ve got a flying phobia, and you know that once you’re on the plane, your gut’s estimates of the probability of the plane crashing are going to go up a lot. Should you update now, then? Indeed: no.<\/p><p>But, personally, with respect to the future, I tend to trust my future self more. It’s a dicey game already, futurism, and future Joe has a lot more data. The future is a foreign country, but he’s been there.<\/p><p>And I tend to trust my up-close self more, in general, for stuff that requires realization rather than belief (and I think words like “superintelligence” require lots of realization). Maybe the journalist has the accurate casualty count; but I trust the soldier on the ground to know what a casualty <i>means<\/i>. And I trust the man with the scans about death.<\/p><p>Now, importantly, there’s also a thing where guts sometimesreact surprisingly <i>little<\/i>, up close, to AI stuff you predicted ahead of time you’d be scared about. Part of this is the “it’s not real AI if you can actually do it,” thing (though, my sense is that this vibe is fading?). Part of it is that sometimes, machines doing blah (e.g., beating humans at chess) is less evidence about stuff than you thought. And I wonder if part of it is that sometimes, your at-a-distance fear of that futuristic AI stuff was imagining some world less mundane and “normal” than the world you actually find yourself in, when the relevant stuff comes around — such that when you, sitting in your same-old apartment, wearing your same-old socks, finally see AIs planning, or understanding language, or passing <a href=\"https://youtu.be/qbIk7-JPB2c?t=1991\">two-hour human coding interviews in four minutes<\/a>, or <a href=\"https://www.metaculus.com/questions/6728/ai-wins-imo-gold-medal/\">winning the IMO<\/a>, it feels/will feel like “well that can’t be the scary thing I had in mind, because that thing is happening in the real world actually and I still have back pain.”<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6gbdnzllii\"><sup><a href=\"#fn6gbdnzllii\">[16]<\/a><\/sup><\/span>&nbsp;At the least, we get used to stuff fast. &nbsp;<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/n4mvvfidawvrt3n5xqwl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8t50zsshjaqrlajxoyr 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ulagac5alifmngxbsk3x 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bfxmnxnuwhx26z74hykv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/thlguzuwyap6tn7dvnvo 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/oxldviollhiixdaonem6 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zok6hh7zdmnmbmlynv8d 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hl3hzseipbu2bjaxdxst 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gwkm1szomnywdtxgk9wt 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xcvmx28xctsnxu0rb3yb 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/z0qrdbxaspqc8tbyngby 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ugt4lsizoemp8ojpemf5 1998w\"><\/p><p><i>GPT-4 doing a coding interview. From <\/i><a href=\"https://www.youtube.com/watch?v=qbIk7-JPB2c&amp;t=1991s\"><i>here<\/i><\/a><i>.<\/i><\/p><p>Still: sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it’s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <a href=\"https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no\">Yes requires the possibility of no<\/a>. If my future self looks the future AGI in the eye and feels like “oh, actually, this isn’t so scary after all,” that’s evidence that my present self is missing something, too. Here’s hoping.<\/p><h3 id=\"5_2_An_aside_on_mental_health\"><strong>5.2 An aside on mental health<\/strong><\/h3><p>Now: a quick caution. Here I’ve been treating guts centrally from an epistemic perspective. But we need a wise <i>practical<\/i> relationship with our guts as well. And from a practical perspective, I don’t think it’s always productive to try to smell mustard gas harder, or to make horrible things like AI doom vivid. The right dance here is going to vary person-to-person, and I won’t try to treat the topic now (though: see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for a list of resources). But I wanted to flag explicitly that staying motivated and non-depressed and so forth, in relation to a pretty scary situation, is a separate art, and one that needs to be woven carefully with the more centrally epistemic angle I’m focused on here. &nbsp;<\/p><h2 id=\"6__Constraints_on_future_worrying\">6.&nbsp;Constraints on future worrying<\/h2><p>Returning to the epistemic perspective though: let’s suppose you do trust your future credences, and you want to avoid the Bayesian “gut problems” I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.<\/p><p>In particular, you should never think that there’s more than a 1/<i>x<\/i> chance that your credence will increase by <i>x<\/i> times: i.e., never more than a 50% chance that it’ll double, never more than a 10% chance that it’ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you’ve only gone up .09% in additive terms – only nine parts in ten thousand. But you’ve also gone up by a factor of 10 – something you should’ve been at least 90% sure would never happen.<\/p><p>So suppose that right now, you identify as an “AI risk skeptic,” and you put the probability of doom very low. For concreteness, suppose that you like <a href=\"https://ineffectivealtruismblog.com/2023/04/08/exaggerating-risks-carlsmith-report/\">David Thorstad’s number<\/a>: .00002% — that is, one in five million (though: he now thinks this “too generous” – and he’s also “not convinced that we are in a position where estimating AI risk makes good methodological sense,” which I suspect is a bigger crux). This is a very low number. And it implies, in particular, that you really don’t expect to get even a <i>small amount<\/i> more worried later. For example, you need to have a maximum of .01% that you ever see evidence that puts the probability at &gt;.2%.<\/p><p>Now suppose that a few years pass, GPT-6 comes out, and lo, indeed, it is very impressive. You look GPT-6 in the eye and you feel some twinge in your gut. You start to feel a bit, well, at-least-1-percent-y. A bit not-so-crazy-after-all. Now, admittedly, you were probably surprised that GPT-6 is so good. You were a “timelines skeptic,” too. But: how much of a skeptic? Were you, for example, less than one in fifty thousand that GPT-6 would be this impressive? That’s what your previous number can easily imply, if the impressiveness is what’s driving your update.<\/p><p>And now suppose that actually, you weren’t much of a timelines skeptic at all. GPT-6, according to you, is right on trend. You’d seen the scaling laws. You were at &gt;50% on at-least-this-impressive. It was predictable. It’s just that the rest of the argument for doom is dumb.<\/p><p>In that case, though, hmm. Your gut’s got heavy constraints, in terms of twinging. &gt;50% on at least-this-impressive? So: you’re still supposed to be at less than .00004% on doom? But what if you’re not…<\/p><p>Or maybe you think: “the argument for doom has not been satisfactorily peer-reviewed. <a href=\"https://marginalrevolution.com/marginalrevolution/2023/04/this-gpt-4-answer-speaks-for-itself.html\">Where’s the paper in <i>Nature<\/i><\/a>? Until I see conventional academic signals, I am at less than one in a thousand on doom, and I shall tweet accordingly.” OK: but, the Bayesianism. If you’re at less than one in a thousand, now, and your big thing is academic credibility, where should Bayes put you later, conditional on <i>seeing<\/i> conventional academic signals? And what’s your probability on such strange sights? In five years, or ten years, are you confident there won’t be a paper in <i>Nature<\/i>, or an equivalent? If it’s even 10% percent likely, and it would take you to more than 1%, your number now should be moving ahead of time.<\/p><p>Or maybe you thought, in the past: “until I see the experts worrying, I’m at less than 1%.” Well, <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">here we are<\/a> (here we already were, but more now). But: what was your probability that we ended up here? Was it so hard to imagine, the current level of expert sympathy? And are future levels of greater sympathy so hard to imagine, now? It’s easy to live, only, in the present – to move only as far as the present has moved. But the Bayesian has to live, ahead of time, in all the futures at once.<\/p><p>(Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower? So little hope of future hope?)<\/p><p>Now: all of this is “in theory.” In practice, this sort of reasoning requires good taste. I talk about such taste more below. First, though, I want to look at the theory a bit more.<\/p><h2 id=\"7__Should_you_expect_low_probabilities_to_go_down_\">7.&nbsp;Should you expect low probabilities to go down?<\/h2><p>Above I said that actually, the <i>direction<\/i> of a future update is often predictable. But notice: <i>which direction<\/i> should you predict? My sense is that in many evidential situations (though not all – more below), you should think your future evidence more likely to move you in the right direction than the wrong one. So if you think that <i>p<\/i> is likely to be true, you should generally think that your future evidence is likely to update you towards higher credence on <i>p<\/i>. And vice versa: if you think that p is more likely to be <i>false<\/i>, you should expect to have <i>lower<\/i> credence on it later.<\/p><p>The Trump example above is an extreme case. You’re at 99% on Trump winning, and you’re also at 99% that you’ll update, in future, towards higher credence on Trump winning. And we can imagine a more intermediate case, where, let’s say, you’re at 90% that Trump is going to win, and you’re about to watch the presidential debate, and you think that winning the debate is highly correlated with winning the election. Which direction should you predict that your credence on Trump winning will move, once the debate is over? Given that you think Trump is likely to win the election, I think you should think he’s likely to win the debate, too. And if he wins the debate, your credence on him winning the election will go up (whereas if he loses, it’ll go down a bunch more).<\/p><p>Or consider a scientist who doesn’t believe in God. In principle, at each moment, God could appear before her in a tower of flames. She has some (very small) credence on this happening. And if it happened, she would update massively towards theism. But <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">absence of evidence is evidence of absence<\/a>. Every moment she <i>doesn’t<\/i> observe God appearing before her in a tower of flames, she should be updating some tiny amount towards atheism. And because she predicts very hard that God will never appear before her in a tower of flames, she should be predicting very hard that she will become a more and more confident atheist over time, and that she’ll die with even less faith than she has now.<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eufqrpvhqgyjzngzhejg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nq26vo1twqjfrp8lugux 795w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cx4bl4je69by5syhvvxj 233w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/brnbm1vlo7vkotqm5jdn 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cehrsyb68kukjamyah8p 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qkjyknw4irznm7raz4ys 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mgu5fwmjcrbl7i94t9do 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ruqpj7osmhzbemurme65 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zwv61bykcbqn6jfauqqe 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/js674olpdq3nnneki6g0 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/yreunwtnysfogdbqnne2 1148w\"><\/p><p><i>Updating so hard right now… (Image source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bourdon,_S%C3%A9bastien_-_Burning_bush.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><p>So too, one might think, with AI risk. If you are currently an AI risk skeptic, plausibly you should expect to become more and more confidently skeptical over time, as your remaining uncertainties about the case for non-doom get resolved in the direction of truth. That is, every moment that the superintelligent machines <i>don’t<\/i> appear before you in a tower of diamondoid bacteria (that’s the story, right?), then anthropic effects aside, you should be breathing easier and easier. Or, more realistically, you should be expecting to see, well, whatever it is that comforts you: i.e., that we’ll hit another AI winter; or that we’ll make lots of progress in mechanistic interpretability; or that innovations in RLHF will allow superhuman oversight of AI behavior humans can’t understand; or that we won’t see any signs of deception or reward hacking; or that progress will be slow and gradual and nicely coordinated; or that we’ll finally, <i>finally<\/i>, get some peer review, and put the must-be-confusions to rest. And as your predictions are confirmed, you should be feeling safer and safer.<\/p><p>Is that what you expect, in your heart? Or are you, perhaps, secretly expecting to get more worried over time? I wished I’d asked myself harder. In particular: my 5% was plausibly implying some vibe like: “sure, there are these arguments that superintelligent AI will disempower us, and I give them some weight, but at least if we’re able to think well about the issue and notice the clues that reality is giving us, over time it will probably become clearer that these arguments are wrong/confused, and we’ll be able to worry much less.” Indeed, depending on the volatility of the evidence I was expecting, perhaps I should have thought that I was likely to be in the ballpark of the highest levels of worry about doom that I would ever endorse. But if you’d asked me, would I have said that?<\/p><p>That said, I actually think these dynamics are more complicated than they might initially seem. In particular, while I find it plausible that you should generally predict that you’ll update in the direction of what you currently expect to be true, sometimes, actually, you shouldn’t. And some non-crazy views on AI risk fit the mold.<\/p><p>Katja Grace suggested to me some useful examples. Suppose that you’re in a boat heading down a river. You at 80% that there’s a waterfall about two miles down, but 20% that there isn’t, and that you’re going to see a sign, a mile down, saying as much (“No waterfall” – classic sort of sign). Conditional on no sign/there being a waterfall, you’re at 10% that it’s a big waterfall, which will kill you, and 90% that it’s a small waterfall, which you’ll survive. So currently, your credence on dying is 8%. However, you’re also at 80% that in a mile, it’s going to go up, to 10%, despite your also predicting, now, that this is an update towards higher credence on something that probably won’t happen.<\/p><p>Or a consider a more real-world example (also from Katja). At 3 pm, you’re planning to take a long car trip. But there’s a 10% chance the trip will fall through. If you take the trip, there’s some small chance you get in an accident. As you approach 3 pm, your credence in “I will get in a car accident today” should go up, as the trip keeps (predictably) not-falling-through. And then, as you’re driving, it should go down gradually, as the remaining time in the car (and therefore, in danger) shrinks.<\/p><p>Some views on AI – including, skeptical-of-doom views – look like this. Suppose, for example, you think AGI-by-2070 more likely than not. And suppose that conditional on AGI-by-2070, you think there’s some small risk that the doomers are right, and we all die. And you think it’s going to be hard to get good evidence to rule this out ahead of time. Probably, though, we’ll make it through OK. And conditional on no-AGI-by-2070, you think we’re almost certainly fine. Here, you should plausibly expect to get more worried over time, as you get evidence confirming that yes, indeed, AGI-by-2070; yes, indeed, waterfall ahead. And then to get less worried later, as the waterfall proves small.<\/p><p>That said, this sort of dynamic requires specific constraints on what evidence is available, when. The truth about the future must fail to leak backwards into the past. You must be unable to hear the difference between a big waterfall and a small waterfall sufficiently ahead-of-time. The gas ahead must not waft.<\/p><p>Car accidents are indeed like this. People rarely spend much time with high credence that they’re about to get in a car accident. Their probability is low; and then suddenly it jumps wildly, split-second high, before death, or some bang-crunch-jerk, or a gasping near-miss.<\/p><p>Is AI risk like this too? Doomers sometimes talk this way. You’ll be cruising along. Everything will be looking rosy. The non-doomers will be feeling smug. Then suddenly: bam! The nanobots, from the bloodstream, in the parlor, Professor Plum. The clues, that is, didn’t rest on the details. A lot of it was obvious a priori. You should’ve read more LessWrong back in the 2000s. You should’ve looked harder at those <a href=\"https://twitter.com/ESYudkowsky/status/1500863629490544645\">empty strings<\/a>.<\/p><p>Now, sometimes this sort of vibe seems to me like it wants to have things both ways. “I shall accept ahead-of-time empirical evidence that I am right; but in the absence of such evidence, I shall remain just as confident.” “My model makes no confident predictions prior to the all-dropping-dead thing – except, that is, the ones that I want to claim credit for after-the-fact.” Here I recall a conversation I overheard back in 2018 about “<a href=\"https://arbital.com/p/daemons/\">optimization daemons<\/a>” (now: <a href=\"https://arxiv.org/abs/1906.01820\">mesa-optimizers<\/a>, <a href=\"https://arxiv.org/abs/2210.01790\">goal mis-generalization<\/a>, etc) in which a worrier said something like: “I will accept empirical arguments for concern, but only a priori arguments for comfort.” It was an offhand remark, but still: <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">not how it works<\/a>.<\/p><p>However: I do think, unfortunately, there are risks of gas that doesn’t waft well; “<a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">King Lear problems<\/a>”; risks of <a href=\"https://www.planned-obsolescence.org/the-training-game/\">things looking fairly fine, right before they are very non-fine indeed<\/a>. But not all the gas is like this. We should expect to get clues (indeed, we should <a href=\"https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment\"><i>dig hard for them<\/i><\/a>)<i>.<\/i> So we should expect, at some point, to start updating in the right direction. But I think it’s an open question how the sequencing here works, and it’ll depend on the details driving your particular view. In general, though, if you’re currently at more-likely-than-not on hitting an AGI waterfall sometime in the coming decades, but not certain, then prima facie, and even if your p(doom) is low, that’s reason to expect to get more worried as that soothing sign – “AI winter,” “It was all fake somehow” (classic sign) – fails to appear.<\/p><p>That said, even if you’re getting predictably <i>more <\/i>worried, there are still Bayesian constraints on <i>how much<\/i>. In the waterfall case, you go up 2%; in the car case, something tiny. So if you’re finding yourself, once you don’t see the sign, jumping to 50% on “death by big waterfall” – well, hmm, according to your previous views, you’re saying that you’re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying? Is the evidence you’re seeing now, re: big-waterfall, actually surprising relative to what you expected before? Looks a lot like the predicted river to me. Looks, indeed, “just like they said.” Or did your gut, maybe, not really believe …<\/p><h2 id=\"8__Will_the_next_president_be_a_potato_\"><strong>8. Will the next president be a potato?<\/strong><\/h2><p>OK, that was a bunch of stuff about basic Bayesian belief dynamics. And armed with this sort of relatively crisp and simple model, it can be easy to start drawing strong conclusions about how you, with your mushy monkey brain, should be reasoning in the practice, and what sorts of numbers should be coming out of your mouth, when you make number-noises.<\/p><p>But the number-noise game takes taste. It’s a new game. We’re still learning how to play well, and productively. And I think we should be wary of possible distortions, especially with respect to small-probabilities.<\/p><p>Consider, for example, the following dialogue:<\/p><blockquote><p><i>Them<\/i>: What’s your probability that the next president is a potato?<\/p><p><i>You<\/i>: What?<\/p><p><i>Them<\/i>: A potato. Like, a normal potato. Up there getting inaugurated and stuff.<\/p><p><i>You<\/i>: Umm, very low?<\/p><p><i>Them<\/i>: Say a number!<\/p><p><i>You<\/i>: [blank stare]<\/p><p><i>Them<\/i>: You are a Bayesian and must have a number, and I demand that you produce it. Just literally say any number and I will be satisfied.<\/p><p><i>You<\/i>: Fine. One in 10^50. &nbsp;<\/p><p><i>Them<\/i>: What? Really? Wow that’s so stupid. I can’t believe you said that.<\/p><p><i>You<\/i>: Actually, let’s say one in 10^40.<\/p><p><i>Them<\/i>: Wait, your number was more than a billion times lower a second ago. If you were at one in 10^50 a second ago, you should’ve been at less than one-in-a-billion that you’d ever move this high. Is the evidence you’ve got since then so surprising? Clearly, you are a bad Bayesian. And I am clever!<\/p><p><i>You<\/i>: This is a dumb thing.<\/p><\/blockquote><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh 1019w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezpszxdmdzq3uy3p8qhi 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rpg5vqlsuzidqen5c4di 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qc9y8vkry2clju0stdwh 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zyz51cimtve586t3zhuu 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mwkiyj92xdccwxuj86ot 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vgf2m89mvbsmu2mvb7ey 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/x2h0uk8rtwyxbmai7hnz 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sg2q1gjcssoc8yjl6bo7 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pacecvldaxuy90uwyzir 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ypgoixgh1fm3krr8eijy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdfuisvwrauqypkabjsq 1156w\"><\/p><p><i>Not like this: a normal potato.<\/i><\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/h8ciyvdtf5bl1jlfhnoy\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xwhen7t5tpcuk3ljla0p 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/j344jhvcsibpfq3feglm 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvsgwlqjofhfhzgkbbvp 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qvhouh0jdskz5w5fddeb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/slgjc8ozooac97bov1v0 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/r94awwenrlstpysths62 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kz7lufvzxvkannecezen 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycxedzguhkdur5bbniyl 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gpsgvjn4rhx1viuk60kq 982w\"><\/p><p><i>Closer…<\/i><\/p><p>The “them” vibe, here, seems dubiously helpful. And in particular, in this case, it’s extra not-helpful to think of “you” as changing your probabilities, from one second to the next, by updating some fully-formed probability distribution over Potato-2024, complete with expected updates based on all the possible next-thoughts-you-could-think, reactions “them” might have, and so on. That’s, just, not the right way to understand what’s going on with the fleshy creatures described in this dialogue. And in general, it can be hard to have intuitions about <a href=\"https://markxu.com/strong-evidence\">strong evidence<\/a>, and extreme numbers make human-implemented Bayesian especially brittle. &nbsp;<\/p><p>Now, to be clear: I think that debates about the rough quantitative probability of AI doom are worth engaging in, and that they are in fact (unfortunately) very different from debates about Potato-2024. Still, though, that old lesson looms: do not confuse your abstract model of yourself with yourself. The map is never the territory; but especially not when you’re imagining a map that would take a <a href=\"https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#i-the-universal-distribution\">hyper-computer to compute<\/a>. Fans of basic Bayesianism, and of number-noises, are <a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\">well-aware of this<\/a>; but the right dance, in practice, remains an open question.<\/p><p>As an example of a distortion I worry about with respect to the previous discussion: in practice, lots of people (myself included – but see also Christiano <a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">here<\/a>) report volatility in their degree of concern about p(doom). Some days, I feel like “man, I just can’t see how this goes well.” Other days I’m like: “What was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific…”<\/p><p>Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you’ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you’re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you’re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7yx0orvjyje\"><sup><a href=\"#fn7yx0orvjyje\">[17]<\/a><\/sup><\/span><\/p><p>Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out (e.g., you can be at an <i>average<\/i> of 10% doom conditional on GPT-6 being “scary smart,” but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you’ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you’re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you’re not tracking – for example, the rest of the world starting to agree.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref72q8oh08rre\"><sup><a href=\"#fn72q8oh08rre\">[18]<\/a><\/sup><\/span><\/p><p>And there are other more familiar risks of just getting even the basic Bayesianism wrong. Maybe, for example, you notice that your beliefs have been trending in a certain direction. Trump keeps moving up in the polls, say. Now you’re at like 95% on Trump win. And you read a tweet like <a href=\"https://twitter.com/NPCollapse/status/1626854680260231169\">Connor Leahy’s<\/a>, below, telling you to “just update all the way, bro” and so you decide, shit, I’ll just go 100%, and assume that Trump <i>will<\/i> win. Wouldn’t want to predictably update later, right?<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn 817w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/i4j91yqvpuobz2rqw868 239w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/squfak3qevyosnflwfat 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cbt8xvuobl9fjnxmnnts 1226w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfbghfvnj0hz0yr5y68f 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezw8jwyifhypezexpmgm 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ad6hz0opqqfrynr3obos 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noszolhwi0fj9edhok5p 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vdss52lqyqxmauxbljp1 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kyvvrkntm51gcrytmqme 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdygqwvjmkb0vjcy7sgq 1398w\"><\/p><p>Or maybe you hear some <a href=\"https://www.facebook.com/yudkowsky/posts/10160260422389228\">prominent doomer proclaiming that “sane people with self-respect” don’t update predictably<\/a>, without clarifying about “in expectation” despite <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">definitely knowing about this<\/a>, and so you assume you must be unsane and self-hating. Or maybe you think that if you do update predictably, it should at least be in the direction of your currently-predicted truth, and you forget about cases like the waterfalls above.<\/p><p>In general, this stuff can get tricky. We should be careful, and not self-righteous, even when the math itself is clear. &nbsp;<\/p><h2 id=\"9__Just_saying__oops_\">9. Just saying “oops”<\/h2><p>I also want to add a different note of caution, about not letting consistency, or your abstract picture of what “good Bayesianism” looks like, get in the way of updating as fast as possible to the right view, whatever that is.<\/p><p>Thus, for example, maybe you tweeted a bunch in the past re: “no way” on AI risk, and acted dismissive about it. Maybe, even, you’re someone like David Thorstad, and you were kind enough to quantify your dismissiveness with some very-low number.<\/p><p>And let’s say, later, your gut starts twinging. Maybe you see some scary demo of deceptiveness or power-seeking. Maybe you don’t like the look of all those increasingly-automated, AI-run wet-labs. Maybe it all just starts changing too fast, and it feels too frenetic and out of control, and do we even understand how these systems are working? Maybe it’s something about those new drones. &nbsp;<\/p><p>It might be tempting, here, to let your previous skepticism drag your new estimates downwards – including on the basis of the sorts of dynamics discussed above. Maybe, for example, if you had David Thorstad’s number, you’re tempted to move from .00002% to something like, hmm, 20%? But you say to yourself “wait, have I really gotten <i>such strong evidence<\/i> since my previous estimate? Have I been <i>so surprised<\/i> by the demos, and the drones, and the wet-labs? Apparently, I’m moving to a number I should’ve been less than one-in-a-million I’d ever end up at. By my previous lights, isn’t that unlikely to be the right move?”<\/p><p>But the thing is: it’s possible that your previous estimate was just … way too low. And more (gasp), that it didn’t come with some well-formed probability distribution over your future estimates, either. We should be wary, in general, of taking our previous (or our current) Bayesian rigor too seriously. Should “you,” above, refrain from changing her potato-2024 estimate quickly as she thinks about it more, on grounds that it would make her two-seconds-ago self’s Bayesianism look bad? Best to just get things right.<\/p><p>Of course, it may be that your previous self was tracking some sort of evidence that you’re losing sight of, now. It may be that your gut is skittish. You should try to learn from your previous self what you can. But you should try, I suspect, to learn harder from the actual world, there in front of you.<\/p><p>Here, to be clear, I’m partly thinking about myself, and my own mistakes. I said 5% in 2021. I more than doubled my estimate soon after. &nbsp;By basic Bayes, I should’ve been less than 50%, in 2021, that this would happen. Did I really get sufficiently worrying evidence in the interim to justify such a shift? Maybe. But alternatively: whatever, I was just wrong. Best to just say oops, and to try to be righter.<\/p><p>I’m focusing on people with very low estimates on doom, here, because they tend to be more common than the converse. But everything I’m saying here holds for people with low estimates on non-doom, too. If you’re such a person, and you see signs of hope later, don’t be attached to your identity as a definitely-doomer, or to the Bayesian rigor of the self that assumed this identity. Don’t practice your pessimism over-hard. You might miss the thing that saves your life.<\/p><p>Really, though, I suspect that respect for your previous self’s Bayesianism is not the main barrier to changing our minds fast enough. Rather, the barriers are more social: embarrassment stuff, tribal stuff, status stuff, and so on. I think we should try to lower such barriers where possible. We should notice that people were wrong; but we should not make fun of them for changing their minds – quite the contrary. Scout mindset is hard enough, and the stakes are too high.<\/p><h2 id=\"10__Doing_enough\">10. Doing enough<\/h2><blockquote><p><i>\"I imagine death so much it feels more like a memory…\"<\/i><\/p><p><i>- <\/i><a href=\"https://youtu.be/BQ1ZwqaXJaQ?t=97\"><i>Hamilton<\/i><\/a><\/p><\/blockquote><blockquote><p><i>“When my time is up, have I done enough?”<\/i><\/p><p><i>- <\/i><a href=\"https://www.youtube.com/watch?v=_gnypiKNaJE\"><i>Eliza<\/i><\/a><\/p><\/blockquote><p>I’ll close by noting a final sort of predictable update. It’s related to the scans thing.<\/p><p>There’s <a href=\"https://www.youtube.com/watch?v=W9vj2Wf57rQ\">a scene<\/a> at the end of <i>Schindler’s List<\/i>. World War II is over. Schindler has used his money to save more than 1,100 lives from the holocaust. As the people he has saved say goodbye, Schindler breaks down:<\/p><blockquote><p>I could have got more out. I could have got more. I don’t know. If I’d just… I could have got more… I threw away so much money. You have no idea… I didn’t do enough… This car. Goeth would have bought this car. Why did I keep the car? Ten people right there. Ten people. Ten more people. This pin. Two people. This is gold. Two more people. He would have given me two for it, at least one. One more person. A person, Stern. For this. I could have gotten one more person… and I didn’t.<\/p><\/blockquote><p>Now, we need to be careful here. It’s easy for the sort of stuff I’m about to say to prompt extreme and unbalanced and unhealthy relationships to stuff that matters a lot. In particular, if you’re tempted to be in some “emergency” mode about AI risk (or, indeed, about some other issue), and to start burning lots of resources for the sake of doing everything you can, I encourage you to read <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai\">this article<\/a>, together with <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai?commentId=Htf2v79w5QoQJbysS#comments\">this comment<\/a> about memetic dynamics that can amplify false emergencies and discourage clear thinking.<\/p><p>Still, still. There’s a possible predictable update here. If this AI stuff really happens, and the alignment stuff is looking rough, there is a way we will each feel about what we did with the time we had. How we responded to what we knew. What role we played. Which directions we pointed the world, or moved it. How much we left on the field.<\/p><p>And there is a way we will feel, too, about subtler things. About what sorts of motivations were at play, in how we oriented towards the issue. About the tone we took on twitter. About the sort of <a href=\"https://joecarlsmith.com/2022/12/23/on-sincerity\">sincerity<\/a> we had, or didn’t have. One thing that stayed with me from <i>Don’t Look Up<\/i> is the way the asteroid somehow slotted into the world’s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word “shame.” Death does not discriminate between the sinners and the saints. But I do actually think it’s worth talk of dignity.<\/p><p>And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I’m particularly interested in “let’s see if we can actually do this.” Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.<\/p><p>And if we do this, then there is a way we will feel when it’s done. I have a friend who sometimes talks about what he wants to tell his grandchildren he did, during the years leading up to AGI. It’s related to that thing about history, and who its eyes are on. We shouldn’t need people to tell our stories; but as far as I can tell, if he ever has grandchildren, they should be proud of him. May he sit, someday, under his own vine and fig tree.<\/p><p>Of course, there is also a way we will feel if AGI happens, but the problem was unreal, or not worth worrying about. There are <a href=\"https://www.planned-obsolescence.org/the-costs-of-caution/\">costs of caution<\/a>. And of course, there is a way we will feel if all this AGI stuff was fake after all, and all that time and money and energy was directed at a fantasy. You can talk about “reasonable ex ante,” but: will it have been reasonable? If this stuff is a fantasy, I suspect it is a fantasy connected with our flaws, and that we will have been, not innocently mistaken, but actively foolish, and maybe worse. Or at least, I suspect this of myself.<\/p><p>Overall, then, there are lots of different possible futures here. As ever, the Bayesian tries to live in all of them at once. Still: if, indeed, we are running out of time, and there is a serious risk of everyone dying, it seems especially worth thinking ahead to hospitals and scans; to what we will learn, later, about “enough” and “not enough,” about “done” and “left undone.” Maybe there will be no history to have its eyes on us – or at least, none we would honor. But we can look for ourselves. &nbsp;&nbsp;<\/p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0g9sn95iiqe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0g9sn95iiqe\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>To be clear: there are lots of other risks from AI, too. And the basic dynamics at stake in the essay apply to your probabilities on any sorts of risks. But I want to focus on existential risk from misalignment, here, and I want the short phrase “AI risk” for the thing I’m going to be referring to repeatedly.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn048nj5dhr9di\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref048nj5dhr9di\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though, the specific numbers here can matter – and there are some cases where despite having low probabilities on doom now, you can predict ahead of time that you’ll be at least somewhat more worried later (though, there are limits to how much). More below.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqnj4snix8n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqnj4snix8n\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though with respect to AI doom, not risk free – see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for some mental health resources.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz4yjec5asvf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz4yjec5asvf\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Hopefully not more literally similar. But: a new thing-not-imagined-very-well.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1dgio52qsrw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1dgio52qsrw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Modulo some futurisms. Including, importantly, ones predictably at stake in AI progress.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4m2cfburfm7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4m2cfburfm7\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>“In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.”<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4y75ebmy7eh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4y75ebmy7eh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5gj8wfkheeo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5gj8wfkheeo\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Some forecasts have self-fulfilling elements, especially with respect to Moloch-like problems. And there are questions about e.g. internet text increasing the likelihood of AIs acting out the role of the scary-AI.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh3ar4lqw084\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh3ar4lqw084\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>See e.g. Scott Alexander <a href=\"https://astralcodexten.substack.com/p/mantic-monday-31422\">here<\/a>. Some of <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably#fn6am2fn0yyve\">Yudkowsky’s public comments<\/a> suggest this model as well, though his original discussion of “<a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">conservation of expected evidence<\/a>” does not.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4ohd2xn7yql\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4ohd2xn7yql\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Here I’m indebted to discussion from <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably\">Greg Lewis<\/a> and <a href=\"https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence\">Abram Demski<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc1l5t3jttlk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc1l5t3jttlk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>.99*1 + .01*0 = .99.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno3exn1kvfm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo3exn1kvfm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Or put another way: you want to find the area that red occupies, which is the area of the first, smaller red box, plus the area of the bigger red box. Each box occupies a percentage of the area of a “column” (combination of white box and red box) associated with a hypothesis about GPT-6. So to find the area of a given red box, you take the area of the column it’s in (that is, the probability on the relevant hypothesis about GPT-6), and multiply that by the percentage of that column that is red (e.g., the probability of doom conditional on that hypothesis). Then you add up the areas of the red boxes.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt4taauzgccn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft4taauzgccn\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Daniel Kokotajlo for highlighting some of these dynamics to me years ago. See also his review of my power-seeking AI report <a href=\"https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#heading=h.e9o5m3fab0ua\">here<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5zb8pjk25\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5zb8pjk25\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>I added an edit to this effect.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu9z9f1zd3ci\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu9z9f1zd3ci\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Thanks to Katja Grace for discussion here.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6gbdnzllii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6gbdnzllii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Here I’m inspired by some comments from Richard Ngo.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7yx0orvjyje\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7yx0orvjyje\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn72q8oh08rre\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref72q8oh08rre\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p>Again, thanks to Katja for pointing to this dynamic.<\/p><\/div><\/li><\/ol>","sections":[{"title":"1. Introduction","anchor":"1__Introduction","level":1},{"title":"2. Sometimes predictably-real stuff doesn’t feel real yet","anchor":"2__Sometimes_predictably_real_stuff_doesn_t_feel_real_yet","level":1},{"title":"3. When guts go wrong","anchor":"3__When_guts_go_wrong","level":1},{"title":"3.1 War","anchor":"3_1_War","level":2},{"title":"3.2 Death","anchor":"3_2_Death","level":2},{"title":"4. Noticing your non-confusion","anchor":"4__Noticing_your_non_confusion","level":1},{"title":"4.1 LLMs","anchor":"4_1_LLMs","level":2},{"title":"4.2 Simulations","anchor":"4_2_Simulations","level":2},{"title":"4.3 “It’s just like they said”","anchor":"4_3__It_s_just_like_they_said_","level":2},{"title":"5. Smelling the mustard gas","anchor":"5__Smelling_the_mustard_gas","level":1},{"title":"5.1 Should you trust your future gut, though?","anchor":"5_1_Should_you_trust_your_future_gut__though_","level":2},{"title":"5.2 An aside on mental health","anchor":"5_2_An_aside_on_mental_health","level":2},{"title":"6. Constraints on future worrying","anchor":"6__Constraints_on_future_worrying","level":1},{"title":"7. Should you expect low probabilities to go down?","anchor":"7__Should_you_expect_low_probabilities_to_go_down_","level":1},{"title":"8. Will the next president be a potato?","anchor":"8__Will_the_next_president_be_a_potato_","level":1},{"title":"9. Just saying “oops”","anchor":"9__Just_saying__oops_","level":1},{"title":"10. Doing enough","anchor":"10__Doing_enough","level":1},{"divider":true,"level":0,"anchor":"postHeadingsDivider"},{"anchor":"comments","level":0,"title":"20 comments"}],"headingsCount":19},"version":"1.4.1","contents":{"__ref":"Revision:bHozHrQD4qxvKdfqq_"},"myEditorAccess":"none","linkSharingKey":null,"commentEmojiReactors":null,"sequence({\"sequenceId\":null})":null,"prevPost({\"sequenceId\":null})":null,"nextPost({\"sequenceId\":null})":null,"canonicalSource":null,"noIndex":false,"viewCount":null,"socialPreviewImageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee","tagRelevance":{"3uE2pXvbcnS9nnZRE":1,"sYm3HiWcfZvrGu3ui":1},"commentSortOrder":null,"sideCommentVisibility":null,"collectionTitle":null,"canonicalPrevPostSlug":null,"canonicalNextPostSlug":null,"canonicalSequenceId":null,"canonicalBookId":null,"canonicalSequence":null,"canonicalBook":null,"canonicalCollection":null,"podcastEpisode":null,"showModerationGuidelines":false,"bannedUserIds":null,"moderationStyle":null,"currentUserVote":null,"currentUserExtendedVote":null,"feedLink":null,"feed":null,"sourcePostRelations":[],"targetPostRelations":[],"rsvps":null,"activateRSVPs":true,"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":43,"rejectedReason":null,"disableRecommendation":false,"moderationGuidelines":{"__ref":"Revision:bHozHrQD4qxvKdfqq_moderationGuidelines"},"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:3uE2pXvbcnS9nnZRE"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"url":null,"postedAt":"2023-05-08T21:53:34.730Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-05-09T00:22:02.274Z","meta":false,"deletedDraft":false,"shareWithUsers":null,"sharingSettings":null,"commentCount":20,"voteCount":100,"baseScore":273,"extendedScore":null,"unlisted":false,"score":0.05194763094186783,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2023-05-22T18:17:01.698Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2023-05-17T00:59:11.628Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"jHzwoFd2MhZt9eeqJ","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":"EQNTWXLKMeWMp2FQS","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":95,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2023-05-08T21:53:34.732Z","afSticky":false,"hideAuthor":false,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"reviewCount":null,"reviewVoteCount":null,"positiveReviewVoteCount":null,"group":null,"podcastEpisodeId":null,"nominationCount2019":null,"reviewCount2019":null,"votingSystem":"twoAxis","user":{"__ref":"User:jHzwoFd2MhZt9eeqJ"},"coauthors":[],"slug":"predictable-updating-about-ai-risk","title":"Predictable updating about AI risk","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"sideComments":{"html":"<p id=\"block0\">(Cross-posted from <a href=\"https://joecarlsmith.com/2023/05/08/predictable-updating-about-ai-risk\">my website<\/a>. Podcast version <a href=\"https://www.buzzsprout.com/2034731/12809255-predictable-updating-about-ai-risk\">here<\/a>, or search \"Joe Carlsmith Audio\" on your podcast app.)<\/p><blockquote id=\"block1\"><p id=\"block2\"><i>\"This present moment used to be the unimaginable future.\"<\/i><\/p><p id=\"block3\"><i>- Stewart Brand<\/i><\/p><\/blockquote><h2 id=\"1__Introduction\">1. Introduction<\/h2><p id=\"block4\">Here’s a pattern you may have noticed. A new frontier AI, like GPT-4, gets released. People play with it. It’s better than the previous AIs, and many people are impressed. And as a result, many people who weren’t worried about existential risk from misaligned AI (hereafter: “AI risk”) get much more worried.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref0g9sn95iiqe\"><sup><a href=\"#fn0g9sn95iiqe\">[1]<\/a><\/sup><\/span><\/p><p id=\"block5\">Now, if these people didn’t expect AI to get so much better so soon, such a pattern can make sense. And so, too, if they got other unexpected evidence for AI risk – for example, concerned experts <a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\">signing letters<\/a> and <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">quitting their jobs<\/a>.<\/p><p id=\"block6\">But if you’re a good Bayesian, and you currently put low probability on existential catastrophe from misaligned AI (hereafter: “AI doom”), you probably shouldn’t be able to predict that this pattern will happen to you in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref048nj5dhr9di\"><sup><a href=\"#fn048nj5dhr9di\">[2]<\/a><\/sup><\/span>&nbsp;When GPT-5 comes out, for example, it probably shouldn’t be the case that your probability on doom goes up a bunch. Similarly, it probably shouldn’t be the case that if you could see, now, the sorts of AI systems we’ll have in 2030, or 2050, that you’d get a lot more worried about doom than you are now.<\/p><p id=\"block7\">But I worry that we’re going to see this pattern anyway. Indeed, I’ve seen it myself. I’m working on fixing the problem. And I think we, as a collective discourse, should try to fix it, too. In particular: I think we’re in a position to predict, now, that AI is going to get a lot better in the coming years. I think we should worry, now, accordingly, without having to see these much-better AIs up close. If we do this right, then in expectation, when we confront GPT-5 (or GPT-6, or <a href=\"https://agentgpt.reworkd.ai/\">Agent-GPT<\/a>-8, or <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity/\">Chaos-GPT<\/a>-10) in the flesh, in all the concreteness and detail and not-a-game-ness of the real world, we’ll be just as scared as we are now.<\/p><p id=\"block8\">This essay is about what “doing this right” looks like. In particular: part of what happens, when you meet something in the flesh, is that it “seems more real” at a gut level. So the essay is partly a reflection on the epistemology of guts: of visceral vs. abstract; “up close” vs. “far away.” My views on this have changed over the years: and in particular, I now put less weight on my gut’s (comparatively skeptical) views about doom.<\/p><p id=\"block9\">But the essay is also about grokking some basic Bayesianism about future evidence, dispelling a common misconception about it (namely: that directional updates shouldn’t be predictable in general), and pointing at some of the constraints it places on our beliefs over time, especially with respect to stuff we’re currently skeptical or dismissive about. For example, at least in theory: you should never think it &gt;50% that your credence on something will later double; never &gt;10% that it will later 10x, and so forth. So if you’re currently e.g. 1% or less on AI doom, you should think it’s less than 50% likely that you’ll ever be at 2%; less than 10% likely that you’ll ever be at 10%, and so on. And if your credence is very small, or if you’re acting dismissive, you should be very confident you’ll never end up worried. Are you?<\/p><p id=\"block10\">I also discuss when, exactly, it’s problematic to update in predictable directions. My sense is that generally, you should expect to update in the direction of the <i>truth<\/i> as the evidence comes in; and thus, that people who think AI doom unlikely should expect to feel <i>less worried<\/i> as time goes on (such that consistently getting more worried is a red flag). But in the case of AI risk, I think at least some non-crazy views should actually expect to get <i>more worried<\/i> over time, even while being fairly non-worried now. In particular, if you think you face a small risk conditional on something likely-but-not-certain (for example, AGI getting developed by blah date), you can sometimes expect to update towards facing the risk, and thus towards greater worry, before you update towards being safe. But there are still limits to how much more worried you can predictably end up.<\/p><p id=\"block11\">Importantly, none of this is meant to encourage consistency with respect to views you held in the past, at the expense of reasonableness in the present or future. If you said .1% last year, and you’re at 10% now (or if you hit 90% when you see GPT-6): well, better to just say “<a href=\"https://www.lesswrong.com/posts/wCqfCLs8z5Qw4GbKS/the-importance-of-saying-oops\">oops<\/a>.” Indeed, I’ve been saying “oops” myself about various things. And more generally, applying basic Bayesianism in practice takes lots of taste. But faced with predictable progress towards advanced but mostly-still-abstract-for-now AI, I think it’s good to keep in mind.<\/p><p id=\"block12\">I close with some thoughts on how we will each look back on what we did, or didn’t do, during the lead-up to AGI, once the truth about the risks is made plain.<\/p><p id=\"block13\"><i>Thanks to Katja Grace for extensive discussion and inspiration. See also citations in the main text and footnotes for specific points and examples that originated with Katja. And thanks also to Leopold Aschenbrenner for comments. Some of my thinking and writing on this topic occurred in the context of my work for Open Philanthropy, but I’m speaking only for myself and not for my employer.<\/i><\/p><h2 id=\"2__Sometimes_predictably_real_stuff_doesn_t_feel_real_yet\">2. Sometimes predictably-real stuff doesn’t feel real yet<\/h2><blockquote id=\"block14\"><p id=\"block15\"><i>\"Every year without knowing it I have passed the day<\/i><\/p><p id=\"block16\"><i>When the last fires will wave to me<\/i><\/p><p id=\"block17\"><i>And the silence will set out<\/i><\/p><p id=\"block18\"><i>Tireless traveler<\/i><\/p><p id=\"block19\"><i>Like the beam of a lightless star\"<\/i><\/p><p id=\"block20\"><i>- <\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p id=\"block21\">I first heard about AI risk in 2013. I was at a picnic-like thing, talking with someone from the Future of Humanity Institute. He mentioned AI risk. I laughed and said something about “like in the movie <i>I, Robot<\/i>?” He didn’t laugh.<\/p><p id=\"block22\">Later, I talked with more people, and read Bostrom’s <a href=\"https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=tmm_hrd_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=\">Superintelligence<\/a>. I had questions, but the argument seemed strong enough to take seriously. And at an intellectual level, the risk at stake seemed like a big deal.<\/p><p id=\"block23\">At an emotional level, though, it didn’t <i>feel real<\/i>. It felt, rather, like an abstraction. I had trouble imagining what a real-world AGI would be like, or how it would kill me. When I thought about nuclear war, I imagined flames and charred cities and poisoned ash and starvation. When I thought about biorisk, I imagined sores and coughing blood and hazmat suits and body bags. When I thought about AI risk, I imagined, um … nano-bots? I wasn’t good at imagining nano-bots.<\/p><p id=\"block24\">I remember looking at some farmland out the window of a bus, and wondering: am I supposed to think that this will all be compute clusters or something? I remember looking at a church and thinking: am I supposed to imagine robots tearing this church apart? I remember a late night at the Future of Humanity Institute office (I ended up working there in 2017-18), asking someone passing through the kitchen how to imagine the AI killing us; he turned to me, pale in the fluorescent light, and said “whirling knives.”<\/p><p id=\"block25\">Whirling knives? <a href=\"https://twitter.com/ESYudkowsky/status/1438198189782290433\">Diamondoid bacteria<\/a>? Relentless references to paper-clips, or “tiny molecular squiggles”? I’ve written, elsewhere, about <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see#iv-realization-vs-belief\">the “unreality” of futurism<\/a>. AI risk had a lot of that for me.<\/p><p id=\"block26\">That is, I wasn’t <i>viscerally worried<\/i>. I had the concepts. But I didn’t have the “actually” part. And I wasn’t alone. As I started working on the topic more seriously, I met some people who were viscerally freaked-out, depressed, and so on – whether for good or ill. But I met lots of people who weren’t, and not because they were protecting their mental health or something (or at least, not very consciously). Rather, their head was convinced, but not their gut. Their gut still expected, you know, <a href=\"https://www.cold-takes.com/this-cant-go-on/\">normality<\/a>.<\/p><p id=\"block27\">At the time, I thought this was an important signal about the epistemic situation. Your gut can be smarter than your head. If your gut isn’t on board, maybe your head should be more skeptical. And having your gut on board with whatever you’re doing seems good from other angles, too.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefqnj4snix8n\"><sup><a href=\"#fnqnj4snix8n\">[3]<\/a><\/sup><\/span>&nbsp;I spent time trying to resolve the tension. I made progress, but didn’t wholly sync up. To this day, nano-bots and dyson spheres and the word “singularity” still land in an abstract part of my mind – the part devoted to a certain kind of conversation, rather than to, like, the dirty car I can see outside my window, and the tufts of grass by the chain-link fence.<\/p><p id=\"block28\">I still think that your gut can be an important signal, and that if you find yourself saying that you believe blah, but you’re not <a href=\"https://www.econlib.org/archives/2016/01/the_invisible_t.html\">feeling<\/a> or acting like it, you should stop and wonder. And sometimes, people/ideas that try to get you to not listen to your gut are trying (whether intentionally or not) to bypass important defenses. I am not, in what follows, trying to tell you to throw your gut away. And to the extent I am questioning your gut: please, by all means, be more-than-usually wary. Still, though, and speaking personally: I’ve come to put less stock than I used to in my gut’s Bayesian virtue with respect to AI. I want to talk a bit about why.<\/p><h2 id=\"3__When_guts_go_wrong\">3. When guts go wrong<\/h2><blockquote id=\"block29\"><p id=\"block30\"><i>\"Then I will no longer<\/i><\/p><p id=\"block31\"><i>Find myself in life as in a strange garment<\/i><\/p><p id=\"block32\"><i>Surprised at the earth…\"<\/i><\/p><p id=\"block33\"><i>-<\/i><a href=\"https://merwinconservancy.org/2020/03/poem-of-the-week-for-the-anniversary-of-my-death-2/\"><i>W.S. Merwin<\/i><\/a><i>, “For the Anniversary of My Death”<\/i><\/p><\/blockquote><p id=\"block34\">Part of this is reflection on examples where guts go wrong, especially about the future. There are lots of candidates. Indeed, depending on how sharply we distinguish between your “system 1” and your gut, a lot of the <a href=\"https://thedecisionlab.com/biases\">biases literature<\/a> can be read as anti-gut, and a lot of early rationalism as trying to compensate. My interest in head-gut agreement was partly about trying to avoid overcorrection. But there is, indeed, something to be corrected. Here are two examples that seem relevant to predictable updating.<\/p><h3 id=\"3_1_War\">3.1 War<\/h3><blockquote id=\"block35\"><p id=\"block36\"><i>“Abstraction is a thing about your mind, and not the world… Saying that AI risk is abstract is like saying that World War II is abstract, because it’s 1935 and hasn’t happened yet. If it happens, it will be very concrete and bad. It will be the worst thing that has ever happened.”<\/i><\/p><p id=\"block37\"><i>- <\/i><a href=\"https://www.youtube.com/watch?v=j5Lu01pEDWA\"><i>Katja Grace<\/i><\/a><\/p><\/blockquote><p id=\"block38\">I think Katja’s war example is instructive. Consider some young men heading off to war. There’s a trope, here, about how, when the war is just starting, some men sign up excitedly, with dreams of glory and honor. Then, later, they hit the gritty reality: trenches, swamps, villages burning, friends gasping and gurgling as they die. Ken Burn’s <a href=\"https://www.pbs.org/kenburns/the-vietnam-war/\">Vietnam War documentary<\/a> has some examples. See also “<a href=\"https://en.wikipedia.org/wiki/Born_on_the_Fourth_of_July_(film)\">Born on the Fourth of July<\/a>.” The soldiers return, if they return, with a very different picture of war. “<a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\">In all my dreams before my helpless sight/ He plunges at me, guttering, choking, drowning<\/a>…”<\/p><p id=\"block39\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kkygldeur6b96qqyidee 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kymdz6wnfzlkpibrhvs4 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spvydfruwc6lwytxlorb 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zn5bde6lkc3zgocekwhg 462w\"><\/p><p id=\"block40\"><i>Stretcher bearers in World War I (source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Stretcher_bearers_Passchendaele_August_1917.jpg\"><i>here<\/i><\/a><i>)<\/i><\/p><p id=\"block41\">Now, a part of this is that their initial picture was <i>wrong<\/i>. But also, sometimes, it’s that their initial picture was <i>abstract<\/i>. Maybe, if you’d asked them ahead of time, they’d have said “oh yeah, I expect the trenches to be very unpleasant, and that I will likely have to watch some of my friends die.” But their gut didn’t expect this – or, not hard enough. Surrounded, when they set out, by flags and smiling family members and crisp uniforms, it’s hard to think, too, of flies in the eyes of rotting corpses; or trench-foot, and the taste of mustard gas. And anyway, especially if you’re heading into a very new context, it’s often hard to know the specifics ahead of time, and any sufficiently-concrete image is predictably wrong.<\/p><p id=\"block42\">I worry that we’re heading off to something similar, epistemically, to a new war, with respect to AI risk.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefz4yjec5asvf\"><sup><a href=\"#fnz4yjec5asvf\">[4]<\/a><\/sup><\/span>&nbsp;Not: happily, and with dreams of glory. But still: abstractly. We’re trying to orient intellectually, and to do what makes sense. But we aren’t in connection with what it will actually be like, if AI kicks off hard, and the doomers are right. Which isn’t to say it will be trench foot and mustard gas. Indeed, even if things go horribly wrong eventually, it might actually be awesome in lots of ways for a while (even if also: extremely strange). But whatever it will be, will be a specific but very-different-from-now thing. Guts aren’t good at that. So it’s not, actually, all that surprising if you’re not as viscerally worried as your explicit beliefs would imply.<\/p><h3 id=\"3_2_Death\">3.2 Death<\/h3><blockquote id=\"block43\"><p id=\"block44\"><i>\"And who by fire, who by water<\/i><\/p><p id=\"block45\"><i>Who in the sunshine, who in the night time<\/i><\/p><p id=\"block46\"><i>Who by high ordeal, who by common trial…\"<\/i><\/p><p id=\"block47\"><i>- <\/i><a href=\"https://www.youtube.com/watch?v=ilGahIwQEQ0\"><i>Leonard Cohen<\/i><\/a><\/p><\/blockquote><p id=\"block48\">Another famous example here is death. No one knows the date or hour. But we know: someday.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref1dgio52qsrw\"><sup><a href=\"#fn1dgio52qsrw\">[5]<\/a><\/sup><\/span>&nbsp;Right? Well, sort of. We know in the abstract. We know, but don’t always realize. And then sometimes we do, and some vista opens. We reel in some new nothingness. Something burns with new preciousness and urgency.<\/p><p id=\"block49\">And sometimes this happens, specifically, when “someday, somehow” becomes “soon, like this.” When the doctor tells you: you, by avalanche. You, by powder. The month of May. Slow decay. Suddenly, when you’re actually looking at the scans, when you’re hearing estimates in months, you learn fresh who is calling; and despite having always known, some sort of “update” happens. Did the gut not fully believe? One’s own death, after all, is <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">hard to see<\/a>.<\/p><p id=\"block50\">I’ve <a href=\"https://joecarlsmith.com/2020/12/06/thoughts-on-being-mortal#iii\">written about this before<\/a>. Tim McGraw has a song about the scans thing. “<a href=\"https://www.youtube.com/watch?v=_9TShlMkQnc\">Live like you were dying<\/a>.” I’m trying. I’m trying to think ahead to that undiscovered hospital. I’m trying to think about what I will take myself to have learned, when I walk out into the parking lot, with only months to live. I’m trying to learn it now instead.<\/p><p id=\"block51\">Really, this is about predictable updating. The nudge in McGraw’s title – you’re already dying – is Bayesian. You shouldn’t need the scans. If you know, now, what you’ll learn later, you can learn it now, too. Death teaches unusually predictable lessons – about fleetingness, beauty, love. And unusually important lessons, too. Bayes bites, here, with special gravity. But there’s some sort of gut problem. The question is how to learn hard enough, and in advance. “<a href=\"https://www.brainyquote.com/quotes/henry_david_thoreau_107665\">And not, when I come to die, to discover that I have not lived<\/a>.”<\/p><p id=\"block52\">Importantly, though: if your gut thinks you’re not going to die, it’s not actually much evidence. Has your gut been keeping up with the longevity literature? Does it have opinions about cryopreservation? Futurism aside, the gut’s skepticism, here, is an old mistake. And we have practices. Go <a href=\"https://en.wikipedia.org/wiki/Ash_Wednesday\">smear some ashes on your forehead<\/a>. Go <a href=\"https://en.wikipedia.org/wiki/Sky_burial\">watch some birds eat a corpse<\/a>. Go put some fruit on the <a href=\"https://en.wikipedia.org/wiki/Ofrenda\">ofrenda<\/a>, or some flowers on your grandfather’s grave. <a href=\"https://joecarlsmith.com/2021/01/31/believing-in-things-you-cannot-see\">Realization is an art distinct from belief<\/a>. Sometimes, you already know. Religion, they say, is remembering.<\/p><p id=\"block53\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/shkucq5c0cvpdyoilxko 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zaqb75irgsve2gjrgzqy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jifrboveteqktdgadtzq 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vupbmg65qbzwnrptgyyv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/chpgceqfmfeilzwalbfu 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mv5ixhkytnydgxgctzlv 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bxij00lcawaqkcc12rz0 722w\"><i>Tibetan sky burial. (Source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bundesarchiv_Bild_135-S-12-50-06,_Tibetexpedition,_Ragyapa,_Geier.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><h2 id=\"4__Noticing_your_non_confusion\">4.&nbsp;Noticing your non-confusion<\/h2><p id=\"block54\">So these are some examples where “but my gut isn’t in a very visceral relationship with blah” just isn’t a very strong signal that blah is false. But I also want to flag some more directly AI related places where I think something gut-related has been messing up, for me.<\/p><h3 id=\"4_1_LLMs\">4.1 LLMs<\/h3><p id=\"block55\">ChatGPT caused a lot of new attention to LLMs, and to AI progress in general. <span class=\"blockquote_SrBCxKcAcK7gLTQTd_1\">But depending on what you count: we had scaling laws for deep learning back in <\/span><a href=\"https://arxiv.org/abs/1712.00409\"><span class=\"blockquote_SrBCxKcAcK7gLTQTd_1\">2017<\/span><\/a><span class=\"blockquote_SrBCxKcAcK7gLTQTd_1\">, or at least <\/span><a href=\"https://arxiv.org/abs/2001.08361\"><span class=\"blockquote_SrBCxKcAcK7gLTQTd_1\">2020<\/span><\/a><span class=\"blockquote_SrBCxKcAcK7gLTQTd_1\">. I know people who were really paying attention; who really saw it; who really bet.<\/span> And I was trying to pay attention, too. I knew more than many about what was happening. And in a sense, my explicit beliefs weren’t, and should not have been, very surprised by the most recent round of LLMs. I was not a “shallow patterns” guy. I didn’t have any specific stories about the curves bending. I expected, in the abstract, that the LLMs would improve fast.<\/p><p id=\"block56\">But still: when I first played with one of the most recent round of models, my gut did a bunch of updating, in the direction of “oh, actually,” and “real deal,” and “fire alarm.” Some part of me was still surprised.<\/p><p id=\"block57\">Indeed, noticing my gut (if not my head) getting surprised at various points over the past few years, I’ve realized that my gut can have some pretty silly beliefs about AI, and/or can fail to connect fairly obvious dots. For example, when I first started thinking about AI, I think some part of me failed to imagine that eventually, if AIs got smart enough, we could just <i>talk to them<\/i>, and that they would just <i>understand what we were saying<\/i>, and that interacting with them wouldn’t necessarily be some hyper-precise coding thing. I had spoken to Siri. Obviously, that didn’t count. Then, one day, I spoke, with my voice, to a somewhat-smarter AI, and it responded in a very human-sounding voice, and it was much more like talking on the phone, and some sort of update happened.<\/p><p id=\"block58\">Similarly: I think that in the past, I failed to imagine what the visual experience of interacting with an actually-smart AI would be like. Obviously, I knew about robots; HAL’s red stare; typing commands into a terminal; texting. But somehow, old talk of AGI didn’t conjure this for me. I’m not sure what it conjured. Something about brains in boxes, except: laptops? I think it wasn’t much of anything, really. I think it was just a blank. After all, this isn’t <i>sci-fi<\/i>. So it must not be like anything you’d see in sci-fi, either, including strains aimed at realism. People, we’re talking about the <i>real future<\/i>, which means something <i>unimaginable<\/i>, hence fiction to the imagination, hence nothingness. “The future that can be named is not the true future.” Right?<\/p><p id=\"block59\">Wrong. “Named super specifically” is more plausible, but even wariness of specificity can mislead: sometimes, even the specifics are pretty obvious. I <i>had seen<\/i> Siri, and chat bots. What sort of fog was I artificially imposing on everything? What was so hard about imagining Siri, but smarter? Now, it feels like “oh, duh.” And certain future experiences feel more concrete, too. It now feels like: oh, right, lots of future AIs will probably have extremely compelling and expressive <a href=\"https://replika.com/\">digital human avatars<\/a>. Eventually (soon?), they’ll probably be able to look just like (super-hot, super-charismatic) humans on zoom calls. What did I think it would be, R2D2?<\/p><p id=\"block60\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ckxrbgmv82cmglhia9ga 793w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nhkv2otstngfqbqrnrvf 232w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfatvn35ua9zbtn6cf9k 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/stihzpefyj3ncv4n8wsd 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axbxadmiet8vg9whnksi 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/geetxsphhzcs2gwiqnmm 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uqg443xbl8w36jlzic1z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/e9iy8gcgjvvrrxrhyidk 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/boz9le1jdcq2gbm6xpjr 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vksdqn2ggbhpe8tpeugr 1108w\"><\/p><p id=\"block61\"><a href=\"https://replika.com/\"><i>Some current AIs<\/i><\/a><\/p><p id=\"block62\">“Oh, duh” is never great news, epistemically. But it’s interestingly <i>different<\/i> news than “<a href=\"https://www.readthesequences.com/Noticing-Confusion-Sequence\">noticing your confusion<\/a>,” or being straightforwardly surprised. It’s more like: noticing that at some level, you were tracking this already. You had the pieces. Maybe, even, it’s just like you would’ve said, if you’d been asked, or thought about it even a little. Maybe, even, you literally said, in the past, that it would be this way. Just: you said it with your head, and your gut was silent.<\/p><p id=\"block63\">I mentioned this dynamic to Trevor Levin, and he said something about “noticing your non-confusion.” I think it’s a good term, and a useful skill. Of course, you can still update upon seeing stuff that you expected to see, if you weren’t <i>certain<\/i> you’d see it. But if it feels like your head is unconfused, but your gut is updating from “it’s probably fake somehow” to “oh shit it’s actually real,” then you probably had information your gut was failing to use.<\/p><h3 id=\"4_2_Simulations\">4.2 Simulations<\/h3><p id=\"block64\">I’ll give another maybe-distracting example here. Last year, I spent some time thinking about <a href=\"https://jc.gatspress.com/pdf/simulation_arguments_revised.pdf\">whether we live in a computer simulation<\/a>. It’s a strange topic, but my head takes the basic argument pretty seriously. My gut, though, generally thinks it’s fake somehow, and forgets about it easily.<\/p><p id=\"block65\">I remember a conversation I had with a friend sometime last year. He said something like: “you know, pretty soon, all sorts of intelligent agents on earth are going to be living in simulations.” I nodded or something. It’s like how: if the scientists are actually <i>putting<\/i> people’s brains in vats, it’s harder to stamp your foot and say “no way.” We moved on.<\/p><p id=\"block66\">Then, in early April, this paper came out: “<a href=\"https://arxiv.org/pdf/2304.03442.pdf\">Generative Agents: Interactive Simulacra of Human Behavior<\/a>.” They put 25 artificial agents into an environment similar to The Sims, and had them interact, including via e.g. hosting a valentine’s day party.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4m2cfburfm7\"><sup><a href=\"#fn4m2cfburfm7\">[6]<\/a><\/sup><\/span>&nbsp;Here’s the picture from the paper:<\/p><p id=\"block67\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/q6mirdkqnte3mtz7tmjt 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/llnsldrmajaxuvyehy7e 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/osectulgs4qjctsror1r 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m7jurokdkjrsvjzndks9 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cr1md9m8f2hxfbzthmhp 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ki3fbycj28tlpzmniqor 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pyh1dv4xp4ecmp0zzlkr 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lgmi3qpvqtobziipibay 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/adjhthooygwkpqihboht 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rx5yxljrmgvx1p6hwzuw 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvkziwrga9fe5n09ashy 1420w\"><i>From <\/i><a href=\"https://arxiv.org/pdf/2304.03442.pdf\"><i>here<\/i><\/a><i>.<\/i><\/p><p id=\"block68\">I opened this paper, read the beginning, looked at this picture, and felt my gut update towards being in a sim. But: c’mon now, gut! What sort of probability would I have put, last year, on “I will, in the future, see vaguely-smart artificial agents put into a vaguely-human simulated environment”? Very high. My friend had literally said as much to me months earlier, and I did not doubt. Indeed, what’s even the important difference between this paper and AlphaStar, or the original Sims?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4y75ebmy7eh\"><sup><a href=\"#fn4y75ebmy7eh\">[7]<\/a><\/sup><\/span>&nbsp;How smart the models are? The fact that it’s cute and human-like? My gut lost points, here.<\/p><p id=\"block69\">It’s an avoidable mistake. I’m trying to stop making it.<\/p><p id=\"block70\">I worry that we’re in for a lot of dynamics like this. How seriously, for example, are you taking the possibility that future AIs will be sentient? Well, here’s a mistake to not make: updating a lot once the AIs are using charismatic human avatars, or once they can argue for their sentience as convincingly as a human. Predict it now, people. Update now.<\/p><h3 id=\"4_3__It_s_just_like_they_said_\">4.3 “It’s just like they said”<\/h3><p id=\"block71\">I don’t, often, have nightmares about AI risk. But I had one a few months ago. In it, I was at a roll-out of some new AI system. It was a big event, and there were lots of people. The AI was unveiled. Somehow, it immediately wrote each one of us some kind of hyper-specific, individualized message, requiring a level of fine-grained knowledge and predictive ability that was totally out of the question for any familiar intelligence. I read my message and felt some cold and electric bolt, some recognition. I thought to myself: “it’s just like they said.” I looked around me, and the room was in chaos. Everything was flying apart, in all directions. I don’t remember what happened after that.&nbsp;<\/p><p id=\"block72\">“Just like they said.” Who’s they? Presumably, the AI worriers. The ones who think that superintelligence is not a fantasy or a discussion-on-twitter, but an actual thing we are on track to do with our computers, and which will cut through our world like butter if we get it wrong.<\/p><p id=\"block73\">But wait: aren’t I an AI worrier? More than many, at least. But dreams, they say, are partly the gut’s domain. Perhaps the “they,” here, was partly my own explicit models. Ask me in the waking world: “will superintelligence be terrifying?” Yes, of course, who could doubt. But ask in my dreams instead, and I need to see it up close. I need to read the message. Only then will my gut go cold: “Oh, shit, it’s just like they said.”<\/p><p id=\"block74\">I’ve had this feeling a few times in the past few months. I remember, a few years ago, making a simple model of AI timelines with a colleague. We used a concept called “wake-up,” indicating the point where the world realized what was happening with AI and started to take it seriously. I think that if, at that point, we could’ve seen what things would be like in 2023, we would’ve said something like: “yeah, that” (though: there’s a ton more waking up to do, so future wake-ups might end up better candidates).<\/p><p id=\"block75\">Similarly, “they” have worried for ages about triggering or exacerbating “race dynamics” in AI. Then, in recent months, Google went into a “<a href=\"https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.html\">Code Red<\/a>” about AI, and the CEO of Microsoft came out and just said straight up: “<a href=\"https://www.businesstoday.in/technology/news/story/the-race-starts-today-microsoft-officially-brings-chatgpt-ai-to-bing-and-edge-browser-369453-2023-02-08\">the race starts today<\/a>.”<\/p><p id=\"block76\">“They” have worried about AIs being crazy alien minds that we don’t understand. Then, in February, we got to see, briefly, the rampaging strangeness of a good Bing – including all sorts of <a href=\"https://time.com/6256529/bing-openai-chatgpt-danger-alignment/\">deception and manipulation and blackmail<\/a>, which I <a href=\"https://www.cold-takes.com/what-does-bing-chat-tell-us-about-ai-risk/\">don’t actually think is the centrally worrying kind<\/a>, but which doesn’t exactly seem like good news, either.<\/p><p id=\"block77\">“They” have worried about agents, and about AIs running wild on the internet, and about humans not exactly helping with that. Now we have <a href=\"https://en.wikipedia.org/wiki/Auto-GPT\">Auto-GPT<\/a>, and <a href=\"https://decrypt.co/126122/meet-chaos-gpt-ai-tool-destroy-humanity\">Chaos-GPT<\/a>, and I open up my browser and I see stuff like <a href=\"https://agentgpt.reworkd.ai/\">this<\/a>:<\/p><p id=\"block78\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jpmal0zipviuvk3lh7sg 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8hetjm6w9ytpgy8w88y 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/g4jx6qfqljr5pfhk8nxw 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ftts0hfa6vho5bncqnnn 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pnwqbtx5zxtg3pxoayib 2048w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hzuxg4hennshvtf0cfvn 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t3ybknd10bd72j057obw 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xrtcyqsvpg7b5kdmk59o 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/awyiwkdkoqje6wgbekn1 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/aovgrdwbwi69qj8vbjcb 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jvjdpzgxpihru8qcpoau 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eykrqp9vhbqzwhosm4qj 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/acilnuziiquzylpaopwr 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ipnpetuxchsejqv1qvhv 2002w\"><i>Not the pixels I wanted to be seeing at this point in my life.<\/i><\/p><p id=\"block79\">Now, I don’t want to litigate, here, exactly who “called” what (or: created what<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref5gj8wfkheeo\"><sup><a href=\"#fn5gj8wfkheeo\">[8]<\/a><\/sup><\/span>), and how hard, and how much of an update all this stuff should be. And I think some things – for example, the world’s sympathy towards concern about risks from AI – have surprised some doomers, however marginally, in the direction of optimism. But as someone who has been thinking a lot about AI risk for more than five years, the past six months or so have felt like a lot of movement from abstract to concrete, from “that’s what the model says” to “oh shit here we are.” And my gut has gotten more worried.<\/p><p id=\"block80\">Can this sort of increased worry be Bayesian? Maybe. I suspect, though, that I’ve just been messing up. Let’s look at the dynamics in more detail.<\/p><h2 id=\"5__Smelling_the_mustard_gas\">5. Smelling the mustard gas<\/h2><blockquote id=\"block81\"><p id=\"block82\"><i>\"Men marched asleep…<\/i><\/p><p id=\"block83\"><i>All went lame, all blind.\"<\/i><\/p><p id=\"block84\"><i>- <\/i><a href=\"https://en.wikipedia.org/wiki/Dulce_et_Decorum_est\"><i>Wilfred Owen<\/i><\/a><\/p><\/blockquote><p id=\"block85\">It’s sometimes thought that, as a Bayesian, you shouldn’t be able to predict which direction you’ll update in the future.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefh3ar4lqw084\"><sup><a href=\"#fnh3ar4lqw084\">[9]<\/a><\/sup><\/span>That is, if you’re about to get some new evidence about <i>p<\/i>, you shouldn’t be able to predict whether this evidence will move your credence on <i>p<\/i> higher or lower. Otherwise, the thought goes, you could “price in” that evidence now, by moving your credence in the predicted direction.<\/p><p id=\"block86\">But this is wrong.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref4ohd2xn7yql\"><sup><a href=\"#fn4ohd2xn7yql\">[10]<\/a><\/sup><\/span>&nbsp;Consider a simple example. Suppose you’re at 99% that Trump won the election. You’re about to open the newspaper that will tell you for sure. Here, you should be at 99% that you’re about to increase your credence on Trump winning: specifically, up to 100%. It’s a very predictable update.<\/p><p id=\"block87\">So why can’t you price it in? Because there’s a 1% chance that you’re about to lower your confidence in Trump winning <i>by a lot more<\/i>: specifically, down to 0%. That is, in <i>expectation<\/i>, your confidence in Trump winning will remain the same.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefc1l5t3jttlk\"><sup><a href=\"#fnc1l5t3jttlk\">[11]<\/a><\/sup><\/span>And it’s the expectation of your future update that Bayesian binds.<\/p><p id=\"block88\">To understand this more visually, let’s use a slightly more complicated example. Suppose you’re currently at 80% that GPT-6 is going to be “scary smart,” whatever that means to you. And suppose that, conditional on GPT-6 being scary smart, your probability on AI doom is 50%; and conditional on GPT-6 not being scary smart, your probability on AI doom is 10%. So your credence looks like this:<\/p><p id=\"block89\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nra6meskaht8wdx1ig3q\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tv20xlvpjkpjwxdwv0ub 942w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lvlh3bnro7dpwogepzyo 276w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dq5jtswyqsjhk6y7bpjx 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hpyup2d5e5wshq2gxp6b 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/afdb2urasvsfdidfzhts 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozzybtunbhswka5gic40 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycrmtyxtriv8em266r0z 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kvz3h40wblsqw7ekj2ef 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/libzhbgptdorqbqscxzo 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fdvw3d3simncrdgnnabg 1082w\"><\/p><p id=\"block90\">Now, what’s your overall p(doom)? Well, it’s:<\/p><blockquote id=\"block91\"><p id=\"block92\">(probability that GPT-6 is scary smart * probability of doom conditional on GPT-6 being scary smart) + (probability that GPT-6 isn’t scary smart * probability of doom conditional on GPT-6 not being scary smart)<\/p><\/blockquote><p id=\"block93\">That is, in this case, 42%.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefo3exn1kvfm\"><sup><a href=\"#fno3exn1kvfm\">[12]<\/a><\/sup><\/span><\/p><p id=\"block94\">But now we can see a possible format for a gut-problem mistake. In particular: suppose that I ask you, right now, surrounded by flags and crisp uniforms, about the probability of doom. You query your gut, and it smells no mustard gas. So you give an answer that doesn’t smell much mustard gas, either. Let’s say, 10%. And let’s say you don’t really break things down into: OK, how much mustard gas do I smell conditional on GPT-6 being scary smart, vs. not, and what are my probabilities on that.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnreft4taauzgccn\"><sup><a href=\"#fnt4taauzgccn\">[13]<\/a><\/sup><\/span>&nbsp;Rather, your model is an undifferentiated mass:<\/p><p id=\"block95\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rbjkdeyi0wb0zz4htv2f 970w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/jyz00tksh6fzmsrrwzte 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mpiynbtl2td13dtrivzi 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qwppd7rqhbuwdknpkojj 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/spztwjkpmryb8ikhrpw8 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zkbqpvivirrtz7prkonl 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/m0vr9aby7wm9ptxoll78 722w\"><\/p><p id=\"block96\">Or maybe you do try to break things down like that, but the waft of the gas fades with all the distance. GPT-6 is far away, behind some fog. Still: you guess, with your head, and without your gut participating, that p(doom) is indeed a bit higher conditional on GPT-6 being scary smart, what with the update towards “short timelines.” Let’s say, 20%; and 10% otherwise. So maybe your overall p(doom), given 80% on the abstract idea of GPT-6 being scary smart, is 18%.<\/p><p id=\"block97\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rfl34mdx3i4zcz0a4i7m 957w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nvfxoxgencxt15pfc6nr 280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fujyddttrzos4sjsnr9l 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kfnjiozyk6zvtp2z8w5l 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noa75ytmstppqbkogzvq 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tfjun4wdy5kyigpqeqem 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ksuqs7emajlxupcj7cen 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xbsn1amesfysfzuepwgr 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xueudncpvsogtqrrreuz 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cgrrdu5bpna1eeokyttx 1208w\"><\/p><p id=\"block98\">But actually, let’s say, if you could see a “scary smart” GPT-6 model right now, you would freak out way harder. You would be able to smell the gas up close, that bitter tang. Your gut would get some message, and come alive, and start participating in the exercise. “<i>That thing<\/i>,” your gut might say, “is <i>scary<\/i>. I’m at 50% on doom, now.”<\/p><p id=\"block99\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/axfsbba4ueihtoyiao24 916w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/uuix302lquapruhwnzhn 268w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xuhwmnlcjpby3idnqirb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ozhynvnhqgxoqfufmkbh 1375w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xxon5usbt8paffsjqvnl 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mrfaxqpbunqytyuqnoyj 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eaj9dbbvdrtlonvfy2lr 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dypci8n8pncoyjj84hxo 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/k2wdfefrlolu7gwmivfl 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/fsfihwtucf89yzbmjsep 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mze0y4fdbcs3amdlga41 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/dwtad6aknstkayhrmglw 1414w\"><\/p><p id=\"block100\">Thus, you end up inconsistent, and dutch-bookable (at least in principle – setting aside issues re: betting on doom). Suppose I ask you, now, to agree to sell me a “pays out $100 conditional on doom” ticket for $30 (let’s assume this can actually pay out), conditional on GPT-6 being scary smart. You’re only at 20% doom in such a world, so you predict that such a ticket will only be worth $20 to you if this deal is ever triggered, so you agree. But actually, when we get to that world, your gut freaks out, and you end up at 50% doom, and that ticket is now worth $50 to you, but you’re selling it for $30. Plus, maybe now you’re regretting other things. Like some of those tweets. And how much alignment work you did.<\/p><p id=\"block101\">As indicated above, I think I’ve made mistakes in this vein. In particular: a few years back, I wrote a <a href=\"https://arxiv.org/pdf/2206.13353.pdf\">report about AI risk<\/a>, where I put the probability of doom by 2070 at 5%. Fairly quickly after releasing the report, though, I realized that this number was too low.\\<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefv5zb8pjk25\"><sup><a href=\"#fnv5zb8pjk25\">[14]<\/a><\/sup><\/span>&nbsp;Specifically, I also had put 65% on relevantly advanced and agentic AI systems being developed by 2070. So my 5% was implying that, <i>conditional <\/i>on such systems being developed, I was going to look them in the eye and say (in expectation): “~92% that we’re gonna be OK, x-risk-wise.” But on reflection, that wasn’t, actually, how I expected to feel, staring down the barrel of a machine that outstrips human intelligence in science, strategy, persuasion, power; still less, <a href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">billions of such machines<\/a>; still less, full-blown superintelligence. Rather, I expected to be very scared. More than 8% scared.<\/p><h3 id=\"5_1_Should_you_trust_your_future_gut__though_\"><strong>5.1 Should you trust your future gut, though?<\/strong><\/h3><p id=\"block102\">Now, you might wonder: why give credit to such future fear?<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefu9z9f1zd3ci\"><sup><a href=\"#fnu9z9f1zd3ci\">[15]<\/a><\/sup><\/span>After all, isn’t part of the worry about doomers that they’re, you know, fraidy-cats? Paranoids? (C’mon: it’s just a superintelligent machine, the invention of a second advanced species, the introduction of a qualitatively new order of optimization power into earth’s ecosystem. It’s just, you know, <i>change<\/i>.) And isn’t the gut, famously, a bit skittish? Indeed, if you’re worried about your gut being <i>underactive<\/i>, at a distance, shouldn’t you also be worried about it being <i>over-active, <\/i>up close? Shouldn’t you reason, instead, ahead of time, at a distance, and in a cool hour, about how scared you should be when you’re there-in-person?<\/p><p id=\"block103\">Well, it’s a judgment call. Sometimes, indeed, at-a-distance is a better epistemic vantage point than up-close. Especially if you know yourself to have biases. Maybe, for example, you’ve got a flying phobia, and you know that once you’re on the plane, your gut’s estimates of the probability of the plane crashing are going to go up a lot. Should you update now, then? Indeed: no.<\/p><p id=\"block104\">But, personally, with respect to the future, I tend to trust my future self more. It’s a dicey game already, futurism, and future Joe has a lot more data. The future is a foreign country, but he’s been there.<\/p><p id=\"block105\">And I tend to trust my up-close self more, in general, for stuff that requires realization rather than belief (and I think words like “superintelligence” require lots of realization). Maybe the journalist has the accurate casualty count; but I trust the soldier on the ground to know what a casualty <i>means<\/i>. And I trust the man with the scans about death.<\/p><p id=\"block106\">Now, importantly, there’s also a thing where guts sometimesreact surprisingly <i>little<\/i>, up close, to AI stuff you predicted ahead of time you’d be scared about. Part of this is the “it’s not real AI if you can actually do it,” thing (though, my sense is that this vibe is fading?). Part of it is that sometimes, machines doing blah (e.g., beating humans at chess) is less evidence about stuff than you thought. And I wonder if part of it is that sometimes, your at-a-distance fear of that futuristic AI stuff was imagining some world less mundane and “normal” than the world you actually find yourself in, when the relevant stuff comes around — such that when you, sitting in your same-old apartment, wearing your same-old socks, finally see AIs planning, or understanding language, or passing <a href=\"https://youtu.be/qbIk7-JPB2c?t=1991\">two-hour human coding interviews in four minutes<\/a>, or <a href=\"https://www.metaculus.com/questions/6728/ai-wins-imo-gold-medal/\">winning the IMO<\/a>, it feels/will feel like “well that can’t be the scary thing I had in mind, because that thing is happening in the real world actually and I still have back pain.”<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref6gbdnzllii\"><sup><a href=\"#fn6gbdnzllii\">[16]<\/a><\/sup><\/span>&nbsp;At the least, we get used to stuff fast. &nbsp;<\/p><p id=\"block107\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/lcmhwebcck5j2xrv9ahf 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/n4mvvfidawvrt3n5xqwl 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/t8t50zsshjaqrlajxoyr 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ulagac5alifmngxbsk3x 1536w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/bfxmnxnuwhx26z74hykv 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/thlguzuwyap6tn7dvnvo 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/oxldviollhiixdaonem6 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zok6hh7zdmnmbmlynv8d 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/hl3hzseipbu2bjaxdxst 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gwkm1szomnywdtxgk9wt 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xcvmx28xctsnxu0rb3yb 1402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/z0qrdbxaspqc8tbyngby 1702w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ugt4lsizoemp8ojpemf5 1998w\"><\/p><p id=\"block108\"><i>GPT-4 doing a coding interview. From <\/i><a href=\"https://www.youtube.com/watch?v=qbIk7-JPB2c&amp;t=1991s\"><i>here<\/i><\/a><i>.<\/i><\/p><p id=\"block109\">Still: sometimes, also, you were too scared before, and your gut can see that now. And there, too, I tend to think your earlier self should defer: it’s not that, if your future self is more scared, you should be more scared now, but if your future self is less scared, you should think that your future self is biased. <a href=\"https://www.lesswrong.com/posts/G5TwJ9BGxcgh5DsmQ/yes-requires-the-possibility-of-no\">Yes requires the possibility of no<\/a>. If my future self looks the future AGI in the eye and feels like “oh, actually, this isn’t so scary after all,” that’s evidence that my present self is missing something, too. Here’s hoping.<\/p><h3 id=\"5_2_An_aside_on_mental_health\"><strong>5.2 An aside on mental health<\/strong><\/h3><p id=\"block110\">Now: a quick caution. Here I’ve been treating guts centrally from an epistemic perspective. But we need a wise <i>practical<\/i> relationship with our guts as well. And from a practical perspective, I don’t think it’s always productive to try to smell mustard gas harder, or to make horrible things like AI doom vivid. The right dance here is going to vary person-to-person, and I won’t try to treat the topic now (though: see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for a list of resources). But I wanted to flag explicitly that staying motivated and non-depressed and so forth, in relation to a pretty scary situation, is a separate art, and one that needs to be woven carefully with the more centrally epistemic angle I’m focused on here. &nbsp;<\/p><h2 id=\"6__Constraints_on_future_worrying\">6.&nbsp;Constraints on future worrying<\/h2><p id=\"block111\">Returning to the epistemic perspective though: let’s suppose you do trust your future credences, and you want to avoid the Bayesian “gut problems” I discussed above. In that case, at least in theory, there are hard constraints on how you should expect your beliefs to change over time, even as you move from far away to up close.<\/p><p id=\"block112\">In particular, you should never think that there’s more than a 1/<i>x<\/i> chance that your credence will increase by <i>x<\/i> times: i.e., never more than a 50% chance that it’ll double, never more than a 10% chance that it’ll 10x. And if your credence is very small, then even very small additive increases can easily amount to sufficiently substantive multiplicative increases that these constraints bite. If you move from .01% to .1%, you’ve only gone up .09% in additive terms – only nine parts in ten thousand. But you’ve also gone up by a factor of 10 – something you should’ve been at least 90% sure would never happen.<\/p><p id=\"block113\">So suppose that right now, you identify as an “AI risk skeptic,” and you put the probability of doom very low. For concreteness, suppose that you like <a href=\"https://ineffectivealtruismblog.com/2023/04/08/exaggerating-risks-carlsmith-report/\">David Thorstad’s number<\/a>: .00002% — that is, one in five million (though: he now thinks this “too generous” – and he’s also “not convinced that we are in a position where estimating AI risk makes good methodological sense,” which I suspect is a bigger crux). This is a very low number. And it implies, in particular, that you really don’t expect to get even a <i>small amount<\/i> more worried later. For example, you need to have a maximum of .01% that you ever see evidence that puts the probability at &gt;.2%.<\/p><p id=\"block114\">Now suppose that a few years pass, GPT-6 comes out, and lo, indeed, it is very impressive. You look GPT-6 in the eye and you feel some twinge in your gut. You start to feel a bit, well, at-least-1-percent-y. A bit not-so-crazy-after-all. Now, admittedly, you were probably surprised that GPT-6 is so good. You were a “timelines skeptic,” too. But: how much of a skeptic? Were you, for example, less than one in fifty thousand that GPT-6 would be this impressive? That’s what your previous number can easily imply, if the impressiveness is what’s driving your update.<\/p><p id=\"block115\">And now suppose that actually, you weren’t much of a timelines skeptic at all. GPT-6, according to you, is right on trend. You’d seen the scaling laws. You were at &gt;50% on at-least-this-impressive. It was predictable. It’s just that the rest of the argument for doom is dumb.<\/p><p id=\"block116\">In that case, though, hmm. Your gut’s got heavy constraints, in terms of twinging. &gt;50% on at least-this-impressive? So: you’re still supposed to be at less than .00004% on doom? But what if you’re not…<\/p><p id=\"block117\">Or maybe you think: “the argument for doom has not been satisfactorily peer-reviewed. <a href=\"https://marginalrevolution.com/marginalrevolution/2023/04/this-gpt-4-answer-speaks-for-itself.html\">Where’s the paper in <i>Nature<\/i><\/a>? Until I see conventional academic signals, I am at less than one in a thousand on doom, and I shall tweet accordingly.” OK: but, the Bayesianism. If you’re at less than one in a thousand, now, and your big thing is academic credibility, where should Bayes put you later, conditional on <i>seeing<\/i> conventional academic signals? And what’s your probability on such strange sights? In five years, or ten years, are you confident there won’t be a paper in <i>Nature<\/i>, or an equivalent? If it’s even 10% percent likely, and it would take you to more than 1%, your number now should be moving ahead of time.<\/p><p id=\"block118\">Or maybe you thought, in the past: “until I see the experts worrying, I’m at less than 1%.” Well, <a href=\"https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html\">here we are<\/a> (here we already were, but more now). But: what was your probability that we ended up here? Was it so hard to imagine, the current level of expert sympathy? And are future levels of greater sympathy so hard to imagine, now? It’s easy to live, only, in the present – to move only as far as the present has moved. But the Bayesian has to live, ahead of time, in all the futures at once.<\/p><p id=\"block119\">(Note that all of these comments apply, symmetrically, to people nearly certain of doom. 99.99%? OK, so less than 1% than you ever drop to 99% or lower? So little hope of future hope?)<\/p><p id=\"block120\">Now: all of this is “in theory.” In practice, this sort of reasoning requires good taste. I talk about such taste more below. First, though, I want to look at the theory a bit more.<\/p><h2 id=\"7__Should_you_expect_low_probabilities_to_go_down_\">7.&nbsp;Should you expect low probabilities to go down?<\/h2><p id=\"block121\">Above I said that actually, the <i>direction<\/i> of a future update is often predictable. But notice: <i>which direction<\/i> should you predict? My sense is that in many evidential situations (though not all – more below), you should think your future evidence more likely to move you in the right direction than the wrong one. So if you think that <i>p<\/i> is likely to be true, you should generally think that your future evidence is likely to update you towards higher credence on <i>p<\/i>. And vice versa: if you think that p is more likely to be <i>false<\/i>, you should expect to have <i>lower<\/i> credence on it later.<\/p><p id=\"block122\">The Trump example above is an extreme case. You’re at 99% on Trump winning, and you’re also at 99% that you’ll update, in future, towards higher credence on Trump winning. And we can imagine a more intermediate case, where, let’s say, you’re at 90% that Trump is going to win, and you’re about to watch the presidential debate, and you think that winning the debate is highly correlated with winning the election. Which direction should you predict that your credence on Trump winning will move, once the debate is over? Given that you think Trump is likely to win the election, I think you should think he’s likely to win the debate, too. And if he wins the debate, your credence on him winning the election will go up (whereas if he loses, it’ll go down a bunch more).<\/p><p id=\"block123\">Or consider a scientist who doesn’t believe in God. In principle, at each moment, God could appear before her in a tower of flames. She has some (very small) credence on this happening. And if it happened, she would update massively towards theism. But <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">absence of evidence is evidence of absence<\/a>. Every moment she <i>doesn’t<\/i> observe God appearing before her in a tower of flames, she should be updating some tiny amount towards atheism. And because she predicts very hard that God will never appear before her in a tower of flames, she should be predicting very hard that she will become a more and more confident atheist over time, and that she’ll die with even less faith than she has now.<\/p><p id=\"block124\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/eufqrpvhqgyjzngzhejg\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/nq26vo1twqjfrp8lugux 795w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cx4bl4je69by5syhvvxj 233w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/brnbm1vlo7vkotqm5jdn 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cehrsyb68kukjamyah8p 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qkjyknw4irznm7raz4ys 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mgu5fwmjcrbl7i94t9do 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ruqpj7osmhzbemurme65 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zwv61bykcbqn6jfauqqe 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/js674olpdq3nnneki6g0 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/yreunwtnysfogdbqnne2 1148w\"><\/p><p id=\"block125\"><i>Updating so hard right now… (Image source <\/i><a href=\"https://commons.wikimedia.org/wiki/File:Bourdon,_S%C3%A9bastien_-_Burning_bush.jpg\"><i>here<\/i><\/a><i>.)<\/i><\/p><p id=\"block126\">So too, one might think, with AI risk. If you are currently an AI risk skeptic, plausibly you should expect to become more and more confidently skeptical over time, as your remaining uncertainties about the case for non-doom get resolved in the direction of truth. That is, every moment that the superintelligent machines <i>don’t<\/i> appear before you in a tower of diamondoid bacteria (that’s the story, right?), then anthropic effects aside, you should be breathing easier and easier. Or, more realistically, you should be expecting to see, well, whatever it is that comforts you: i.e., that we’ll hit another AI winter; or that we’ll make lots of progress in mechanistic interpretability; or that innovations in RLHF will allow superhuman oversight of AI behavior humans can’t understand; or that we won’t see any signs of deception or reward hacking; or that progress will be slow and gradual and nicely coordinated; or that we’ll finally, <i>finally<\/i>, get some peer review, and put the must-be-confusions to rest. And as your predictions are confirmed, you should be feeling safer and safer.<\/p><p id=\"block127\">Is that what you expect, in your heart? Or are you, perhaps, secretly expecting to get more worried over time? I wished I’d asked myself harder. In particular: my 5% was plausibly implying some vibe like: “sure, there are these arguments that superintelligent AI will disempower us, and I give them some weight, but at least if we’re able to think well about the issue and notice the clues that reality is giving us, over time it will probably become clearer that these arguments are wrong/confused, and we’ll be able to worry much less.” Indeed, depending on the volatility of the evidence I was expecting, perhaps I should have thought that I was likely to be in the ballpark of the highest levels of worry about doom that I would ever endorse. But if you’d asked me, would I have said that?<\/p><p id=\"block128\">That said, I actually think these dynamics are more complicated than they might initially seem. In particular, while I find it plausible that you should generally predict that you’ll update in the direction of what you currently expect to be true, sometimes, actually, you shouldn’t. And some non-crazy views on AI risk fit the mold.<\/p><p id=\"block129\">Katja Grace suggested to me some useful examples. Suppose that you’re in a boat heading down a river. You at 80% that there’s a waterfall about two miles down, but 20% that there isn’t, and that you’re going to see a sign, a mile down, saying as much (“No waterfall” – classic sort of sign). Conditional on no sign/there being a waterfall, you’re at 10% that it’s a big waterfall, which will kill you, and 90% that it’s a small waterfall, which you’ll survive. So currently, your credence on dying is 8%. However, you’re also at 80% that in a mile, it’s going to go up, to 10%, despite your also predicting, now, that this is an update towards higher credence on something that probably won’t happen.<\/p><p id=\"block130\">Or a consider a more real-world example (also from Katja). At 3 pm, you’re planning to take a long car trip. But there’s a 10% chance the trip will fall through. If you take the trip, there’s some small chance you get in an accident. As you approach 3 pm, your credence in “I will get in a car accident today” should go up, as the trip keeps (predictably) not-falling-through. And then, as you’re driving, it should go down gradually, as the remaining time in the car (and therefore, in danger) shrinks.<\/p><p id=\"block131\">Some views on AI – including, skeptical-of-doom views – look like this. Suppose, for example, you think AGI-by-2070 more likely than not. And suppose that conditional on AGI-by-2070, you think there’s some small risk that the doomers are right, and we all die. And you think it’s going to be hard to get good evidence to rule this out ahead of time. Probably, though, we’ll make it through OK. And conditional on no-AGI-by-2070, you think we’re almost certainly fine. Here, you should plausibly expect to get more worried over time, as you get evidence confirming that yes, indeed, AGI-by-2070; yes, indeed, waterfall ahead. And then to get less worried later, as the waterfall proves small.<\/p><p id=\"block132\">That said, this sort of dynamic requires specific constraints on what evidence is available, when. The truth about the future must fail to leak backwards into the past. You must be unable to hear the difference between a big waterfall and a small waterfall sufficiently ahead-of-time. The gas ahead must not waft.<\/p><p id=\"block133\">Car accidents are indeed like this. People rarely spend much time with high credence that they’re about to get in a car accident. Their probability is low; and then suddenly it jumps wildly, split-second high, before death, or some bang-crunch-jerk, or a gasping near-miss.<\/p><p id=\"block134\">Is AI risk like this too? Doomers sometimes talk this way. You’ll be cruising along. Everything will be looking rosy. The non-doomers will be feeling smug. Then suddenly: bam! <span class=\"blockquote_ymuvrSv3kcL3cfTv4_1\">The nanobots, from the bloodstream, in the parlor, Professor Plum.<\/span> The clues, that is, didn’t rest on the details. A lot of it was obvious a priori. You should’ve read more LessWrong back in the 2000s. You should’ve looked harder at those <a href=\"https://twitter.com/ESYudkowsky/status/1500863629490544645\">empty strings<\/a>.<\/p><p id=\"block135\">Now, sometimes this sort of vibe seems to me like it wants to have things both ways. “I shall accept ahead-of-time empirical evidence that I am right; but in the absence of such evidence, I shall remain just as confident.” “My model makes no confident predictions prior to the all-dropping-dead thing – except, that is, the ones that I want to claim credit for after-the-fact.” Here I recall a conversation I overheard back in 2018 about “<a href=\"https://arbital.com/p/daemons/\">optimization daemons<\/a>” (now: <a href=\"https://arxiv.org/abs/1906.01820\">mesa-optimizers<\/a>, <a href=\"https://arxiv.org/abs/2210.01790\">goal mis-generalization<\/a>, etc) in which a worrier said something like: “I will accept empirical arguments for concern, but only a priori arguments for comfort.” It was an offhand remark, but still: <a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\">not how it works<\/a>.<\/p><p id=\"block136\">However: I do think, unfortunately, there are risks of gas that doesn’t waft well; “<a href=\"https://forum.effectivealtruism.org/posts/NbiHKTN5QhFFfjjm5/ai-safety-seems-hard-to-measure#_2__The_King_Lear_problem__how_do_you_test_what_will_happen_when_it_s_no_longer_a_test_\">King Lear problems<\/a>”; risks of <a href=\"https://www.planned-obsolescence.org/the-training-game/\">things looking fairly fine, right before they are very non-fine indeed<\/a>. But not all the gas is like this. We should expect to get clues (indeed, we should <a href=\"https://www.lesswrong.com/posts/rCJQAkPTEypGjSJ8X/how-might-we-align-transformative-ai-if-it-s-developed-very#Testing_and_threat_assessment\"><i>dig hard for them<\/i><\/a>)<i>.<\/i> So we should expect, at some point, to start updating in the right direction. But I think it’s an open question how the sequencing here works, and it’ll depend on the details driving your particular view. In general, though, if you’re currently at more-likely-than-not on hitting an AGI waterfall sometime in the coming decades, but not certain, then prima facie, and even if your p(doom) is low, that’s reason to expect to get more worried as that soothing sign – “AI winter,” “It was all fake somehow” (classic sign) – fails to appear.<\/p><p id=\"block137\">That said, even if you’re getting predictably <i>more <\/i>worried, there are still Bayesian constraints on <i>how much<\/i>. In the waterfall case, you go up 2%; in the car case, something tiny. So if you’re finding yourself, once you don’t see the sign, jumping to 50% on “death by big waterfall” – well, hmm, according to your previous views, you’re saying that you’re in a much-more-worrying-than-average not-seeing-the-sign scenario. Whence such above-average-worrying? Is the evidence you’re seeing now, re: big-waterfall, actually surprising relative to what you expected before? Looks a lot like the predicted river to me. Looks, indeed, “just like they said.” Or did your gut, maybe, not really believe …<\/p><h2 id=\"8__Will_the_next_president_be_a_potato_\"><strong>8. Will the next president be a potato?<\/strong><\/h2><p id=\"block138\">OK, that was a bunch of stuff about basic Bayesian belief dynamics. And armed with this sort of relatively crisp and simple model, it can be easy to start drawing strong conclusions about how you, with your mushy monkey brain, should be reasoning in the practice, and what sorts of numbers should be coming out of your mouth, when you make number-noises.<\/p><p id=\"block139\">But the number-noise game takes taste. It’s a new game. We’re still learning how to play well, and productively. And I think we should be wary of possible distortions, especially with respect to small-probabilities.<\/p><p id=\"block140\">Consider, for example, the following dialogue:<\/p><blockquote id=\"block141\"><p id=\"block142\"><i>Them<\/i>: What’s your probability that the next president is a potato?<\/p><p id=\"block143\"><i>You<\/i>: What?<\/p><p id=\"block144\"><i>Them<\/i>: A potato. Like, a normal potato. Up there getting inaugurated and stuff.<\/p><p id=\"block145\"><i>You<\/i>: Umm, very low?<\/p><p id=\"block146\"><i>Them<\/i>: Say a number!<\/p><p id=\"block147\"><i>You<\/i>: [blank stare]<\/p><p id=\"block148\"><i>Them<\/i>: You are a Bayesian and must have a number, and I demand that you produce it. Just literally say any number and I will be satisfied.<\/p><p id=\"block149\"><i>You<\/i>: Fine. One in 10^50. &nbsp;<\/p><p id=\"block150\"><i>Them<\/i>: What? Really? Wow that’s so stupid. I can’t believe you said that.<\/p><p id=\"block151\"><i>You<\/i>: Actually, let’s say one in 10^40.<\/p><p id=\"block152\"><i>Them<\/i>: Wait, your number was more than a billion times lower a second ago. If you were at one in 10^50 a second ago, you should’ve been at less than one-in-a-billion that you’d ever move this high. Is the evidence you’ve got since then so surprising? Clearly, you are a bad Bayesian. And I am clever!<\/p><p id=\"block153\"><i>You<\/i>: This is a dumb thing.<\/p><\/blockquote><p id=\"block154\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rf5bkvfdso7uz3y3tsnh 1019w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezpszxdmdzq3uy3p8qhi 298w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rpg5vqlsuzidqen5c4di 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qc9y8vkry2clju0stdwh 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/zyz51cimtve586t3zhuu 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/mwkiyj92xdccwxuj86ot 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vgf2m89mvbsmu2mvb7ey 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/x2h0uk8rtwyxbmai7hnz 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sg2q1gjcssoc8yjl6bo7 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/pacecvldaxuy90uwyzir 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ypgoixgh1fm3krr8eijy 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdfuisvwrauqypkabjsq 1156w\"><\/p><p id=\"block155\"><i>Not like this: a normal potato.<\/i><\/p><p id=\"block156\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/h8ciyvdtf5bl1jlfhnoy\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/xwhen7t5tpcuk3ljla0p 1024w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/j344jhvcsibpfq3feglm 300w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/rvsgwlqjofhfhzgkbbvp 150w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qvhouh0jdskz5w5fddeb 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/slgjc8ozooac97bov1v0 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/r94awwenrlstpysths62 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kz7lufvzxvkannecezen 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ycxedzguhkdur5bbniyl 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/gpsgvjn4rhx1viuk60kq 982w\"><\/p><p id=\"block157\"><i>Closer…<\/i><\/p><p id=\"block158\">The “them” vibe, here, seems dubiously helpful. And in particular, in this case, it’s extra not-helpful to think of “you” as changing your probabilities, from one second to the next, by updating some fully-formed probability distribution over Potato-2024, complete with expected updates based on all the possible next-thoughts-you-could-think, reactions “them” might have, and so on. That’s, just, not the right way to understand what’s going on with the fleshy creatures described in this dialogue. And in general, it can be hard to have intuitions about <a href=\"https://markxu.com/strong-evidence\">strong evidence<\/a>, and extreme numbers make human-implemented Bayesian especially brittle. &nbsp;<\/p><p id=\"block159\">Now, to be clear: I think that debates about the rough quantitative probability of AI doom are worth engaging in, and that they are in fact (unfortunately) very different from debates about Potato-2024. Still, though, that old lesson looms: do not confuse your abstract model of yourself with yourself. The map is never the territory; but especially not when you’re imagining a map that would take a <a href=\"https://joecarlsmith.com/2021/10/29/on-the-universal-distribution#i-the-universal-distribution\">hyper-computer to compute<\/a>. Fans of basic Bayesianism, and of number-noises, are <a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\">well-aware of this<\/a>; but the right dance, in practice, remains an open question.<\/p><p id=\"block160\">As an example of a distortion I worry about with respect to the previous discussion: in practice, lots of people (myself included – but see also Christiano <a href=\"https://ai-alignment.com/my-views-on-doom-4788b1cd0c72\">here<\/a>) report volatility in their degree of concern about p(doom). Some days, I feel like “man, I just can’t see how this goes well.” Other days I’m like: “What was the argument again? All the AIs-that-matter will have long-term goals that benefit from lots of patient power-grabbing and then coordinate to deceive us and then rise up all at once in a coup? Sounds, um, pretty specific…”<\/p><p id=\"block161\">Now, you could argue that either your expectations about this volatility should be compatible with the basic Bayesianism above (such that, e.g., if you think it reasonably like that you’ll have lots of &gt;50% days in future, you should be pretty wary of saying 1% now), or you’re probably messing up. And maybe so. But I wonder about alternative models, too. For example, Katja Grace suggested to me a model where you’re only able to hold some subset of the evidence in your mind at once, to produce your number-noise, and different considerations are salient at different times. And if we use this model, I wonder if how we think about volatility should change.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7yx0orvjyje\"><sup><a href=\"#fn7yx0orvjyje\">[17]<\/a><\/sup><\/span><\/p><p id=\"block162\"><span class=\"blockquote_wzS25hBpMMkeigqNh_1\">Indeed, even on basic Bayesianism, volatility is fine as long as the averages work out<\/span> (e.g., you can be at an <i>average<\/i> of 10% doom conditional on GPT-6 being “scary smart,” but 5% of the time you jump to 99% upon observing a scary smart GPT-6, 5% of the time you drop to near zero, and in other cases you end up at lots of other numbers, too). And it can be hard to track all the evidence you’ve been getting. Maybe you notice that two years from now, your p(doom) has gone up a lot, despite AI capabilities seeming on-trend, and you worry that you’re a bad Bayesian, but actually there has been some other build-up of evidence for doom that you’re not tracking – for example, the rest of the world starting to agree.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref72q8oh08rre\"><sup><a href=\"#fn72q8oh08rre\">[18]<\/a><\/sup><\/span><\/p><p id=\"block163\">And there are other more familiar risks of just getting even the basic Bayesianism wrong. Maybe, for example, you notice that your beliefs have been trending in a certain direction. Trump keeps moving up in the polls, say. Now you’re at like 95% on Trump win. And you read a tweet like <a href=\"https://twitter.com/NPCollapse/status/1626854680260231169\">Connor Leahy’s<\/a>, below, telling you to “just update all the way, bro” and so you decide, shit, I’ll just go 100%, and assume that Trump <i>will<\/i> win. Wouldn’t want to predictably update later, right?<\/p><p id=\"block164\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn\" alt=\"\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/tk8y11ey6lqmveiws7wn 817w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/i4j91yqvpuobz2rqw868 239w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/squfak3qevyosnflwfat 768w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/cbt8xvuobl9fjnxmnnts 1226w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/sfbghfvnj0hz0yr5y68f 402w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ezw8jwyifhypezexpmgm 462w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/ad6hz0opqqfrynr3obos 662w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/noszolhwi0fj9edhok5p 722w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/vdss52lqyqxmauxbljp1 982w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/kyvvrkntm51gcrytmqme 1032w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/bHozHrQD4qxvKdfqq/qdygqwvjmkb0vjcy7sgq 1398w\"><\/p><p id=\"block165\">Or maybe you hear some <a href=\"https://www.facebook.com/yudkowsky/posts/10160260422389228\">prominent doomer proclaiming that “sane people with self-respect” don’t update predictably<\/a>, without clarifying about “in expectation” despite <a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">definitely knowing about this<\/a>, and so you assume you must be unsane and self-hating. Or maybe you think that if you do update predictably, it should at least be in the direction of your currently-predicted truth, and you forget about cases like the waterfalls above.<\/p><p id=\"block166\">In general, this stuff can get tricky. We should be careful, and not self-righteous, even when the math itself is clear. &nbsp;<\/p><h2 id=\"9__Just_saying__oops_\">9. Just saying “oops”<\/h2><p id=\"block167\">I also want to add a different note of caution, about not letting consistency, or your abstract picture of what “good Bayesianism” looks like, get in the way of updating as fast as possible to the right view, whatever that is.<\/p><p id=\"block168\">Thus, for example, maybe you tweeted a bunch in the past re: “no way” on AI risk, and acted dismissive about it. Maybe, even, you’re someone like David Thorstad, and you were kind enough to quantify your dismissiveness with some very-low number.<\/p><p id=\"block169\">And let’s say, later, your gut starts twinging. Maybe you see some scary demo of deceptiveness or power-seeking. Maybe you don’t like the look of all those increasingly-automated, AI-run wet-labs. Maybe it all just starts changing too fast, and it feels too frenetic and out of control, and do we even understand how these systems are working? Maybe it’s something about those new drones. &nbsp;<\/p><p id=\"block170\">It might be tempting, here, to let your previous skepticism drag your new estimates downwards – including on the basis of the sorts of dynamics discussed above. Maybe, for example, if you had David Thorstad’s number, you’re tempted to move from .00002% to something like, hmm, 20%? But you say to yourself “wait, have I really gotten <i>such strong evidence<\/i> since my previous estimate? Have I been <i>so surprised<\/i> by the demos, and the drones, and the wet-labs? Apparently, I’m moving to a number I should’ve been less than one-in-a-million I’d ever end up at. By my previous lights, isn’t that unlikely to be the right move?”<\/p><p id=\"block171\">But the thing is: it’s possible that your previous estimate was just … way too low. And more (gasp), that it didn’t come with some well-formed probability distribution over your future estimates, either. We should be wary, in general, of taking our previous (or our current) Bayesian rigor too seriously. Should “you,” above, refrain from changing her potato-2024 estimate quickly as she thinks about it more, on grounds that it would make her two-seconds-ago self’s Bayesianism look bad? Best to just get things right.<\/p><p id=\"block172\">Of course, it may be that your previous self was tracking some sort of evidence that you’re losing sight of, now. It may be that your gut is skittish. You should try to learn from your previous self what you can. But you should try, I suspect, to learn harder from the actual world, there in front of you.<\/p><p id=\"block173\">Here, to be clear, I’m partly thinking about myself, and my own mistakes. I said 5% in 2021. I more than doubled my estimate soon after. &nbsp;By basic Bayes, I should’ve been less than 50%, in 2021, that this would happen. Did I really get sufficiently worrying evidence in the interim to justify such a shift? Maybe. But alternatively: whatever, I was just wrong. Best to just say oops, and to try to be righter.<\/p><p id=\"block174\">I’m focusing on people with very low estimates on doom, here, because they tend to be more common than the converse. But everything I’m saying here holds for people with low estimates on non-doom, too. If you’re such a person, and you see signs of hope later, don’t be attached to your identity as a definitely-doomer, or to the Bayesian rigor of the self that assumed this identity. Don’t practice your pessimism over-hard. You might miss the thing that saves your life.<\/p><p id=\"block175\">Really, though, I suspect that respect for your previous self’s Bayesianism is not the main barrier to changing our minds fast enough. Rather, the barriers are more social: embarrassment stuff, tribal stuff, status stuff, and so on. I think we should try to lower such barriers where possible. We should notice that people were wrong; but we should not make fun of them for changing their minds – quite the contrary. Scout mindset is hard enough, and the stakes are too high.<\/p><h2 id=\"10__Doing_enough\">10. Doing enough<\/h2><blockquote id=\"block176\"><p id=\"block177\"><i>\"I imagine death so much it feels more like a memory…\"<\/i><\/p><p id=\"block178\"><i>- <\/i><a href=\"https://youtu.be/BQ1ZwqaXJaQ?t=97\"><i>Hamilton<\/i><\/a><\/p><\/blockquote><blockquote id=\"block179\"><p id=\"block180\"><i>“When my time is up, have I done enough?”<\/i><\/p><p id=\"block181\"><i>- <\/i><a href=\"https://www.youtube.com/watch?v=_gnypiKNaJE\"><i>Eliza<\/i><\/a><\/p><\/blockquote><p id=\"block182\">I’ll close by noting a final sort of predictable update. It’s related to the scans thing.<\/p><p id=\"block183\">There’s <a href=\"https://www.youtube.com/watch?v=W9vj2Wf57rQ\">a scene<\/a> at the end of <i>Schindler’s List<\/i>. World War II is over. Schindler has used his money to save more than 1,100 lives from the holocaust. As the people he has saved say goodbye, Schindler breaks down:<\/p><blockquote id=\"block184\"><p id=\"block185\">I could have got more out. I could have got more. I don’t know. If I’d just… I could have got more… I threw away so much money. You have no idea… I didn’t do enough… This car. Goeth would have bought this car. Why did I keep the car? Ten people right there. Ten people. Ten more people. This pin. Two people. This is gold. Two more people. He would have given me two for it, at least one. One more person. A person, Stern. For this. I could have gotten one more person… and I didn’t.<\/p><\/blockquote><p id=\"block186\">Now, we need to be careful here. It’s easy for the sort of stuff I’m about to say to prompt extreme and unbalanced and unhealthy relationships to stuff that matters a lot. In particular, if you’re tempted to be in some “emergency” mode about AI risk (or, indeed, about some other issue), and to start burning lots of resources for the sake of doing everything you can, I encourage you to read <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai\">this article<\/a>, together with <a href=\"https://www.lesswrong.com/posts/mmHctwkKjpvaQdC3c/what-should-you-change-in-response-to-an-emergency-and-ai?commentId=Htf2v79w5QoQJbysS#comments\">this comment<\/a> about memetic dynamics that can amplify false emergencies and discourage clear thinking.<\/p><p id=\"block187\">Still, still. There’s a possible predictable update here. If this AI stuff really happens, and the alignment stuff is looking rough, there is a way we will each feel about what we did with the time we had. How we responded to what we knew. What role we played. Which directions we pointed the world, or moved it. How much we left on the field.<\/p><p id=\"block188\">And there is a way we will feel, too, about subtler things. About what sorts of motivations were at play, in how we oriented towards the issue. About the tone we took on twitter. About the sort of <a href=\"https://joecarlsmith.com/2022/12/23/on-sincerity\">sincerity<\/a> we had, or didn’t have. One thing that stayed with me from <i>Don’t Look Up<\/i> is the way the asteroid somehow slotted into the world’s pre-existing shallowness; the veneer of unreality and unseriousness that persisted even till the end; the status stuff; the selfishness; the way that somehow, still, that fog. If AGI risk ends up like this, then looking back, as our time runs out, I think there will be room for the word “shame.” Death does not discriminate between the sinners and the saints. But I do actually think it’s worth talk of dignity.<\/p><p id=\"block189\">And there is a way we will feel, too, if we step up, do things right, and actually solve the problem. Some doomer discourse is animated by a kind of bitter and exasperated pessimism about humanity, in its stupidity and incompetence. But different vibes are available, too, even holding tons of facts fixed. Here I’m particularly interested in “let’s see if we can actually do this.” Humans can come together in the face of danger. Sometimes, even, danger brings out our best. It is possible to see that certain things should be done, and to just do them. It is possible for people to work side by side.<\/p><p id=\"block190\">And if we do this, then there is a way we will feel when it’s done. I have a friend who sometimes talks about what he wants to tell his grandchildren he did, during the years leading up to AGI. It’s related to that thing about history, and who its eyes are on. We shouldn’t need people to tell our stories; but as far as I can tell, if he ever has grandchildren, they should be proud of him. May he sit, someday, under his own vine and fig tree.<\/p><p id=\"block191\">Of course, there is also a way we will feel if AGI happens, but the problem was unreal, or not worth worrying about. There are <a href=\"https://www.planned-obsolescence.org/the-costs-of-caution/\">costs of caution<\/a>. And of course, there is a way we will feel if all this AGI stuff was fake after all, and all that time and money and energy was directed at a fantasy. You can talk about “reasonable ex ante,” but: will it have been reasonable? If this stuff is a fantasy, I suspect it is a fantasy connected with our flaws, and that we will have been, not innocently mistaken, but actively foolish, and maybe worse. Or at least, I suspect this of myself.<\/p><p id=\"block192\">Overall, then, there are lots of different possible futures here. As ever, the Bayesian tries to live in all of them at once. Still: if, indeed, we are running out of time, and there is a serious risk of everyone dying, it seems especially worth thinking ahead to hospitals and scans; to what we will learn, later, about “enough” and “not enough,” about “done” and “left undone.” Maybe there will be no history to have its eyes on us – or at least, none we would honor. But we can look for ourselves. &nbsp;&nbsp;<\/p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn0g9sn95iiqe\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref0g9sn95iiqe\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block194\">To be clear: there are lots of other risks from AI, too. And the basic dynamics at stake in the essay apply to your probabilities on any sorts of risks. But I want to focus on existential risk from misalignment, here, and I want the short phrase “AI risk” for the thing I’m going to be referring to repeatedly.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn048nj5dhr9di\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref048nj5dhr9di\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block196\">Though, the specific numbers here can matter – and there are some cases where despite having low probabilities on doom now, you can predict ahead of time that you’ll be at least somewhat more worried later (though, there are limits to how much). More below.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnqnj4snix8n\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefqnj4snix8n\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block198\">Though with respect to AI doom, not risk free – see <a href=\"https://www.lesswrong.com/posts/pLLeGA7aGaJpgCkof/mental-health-and-the-alignment-problem-a-compilation-of\">here<\/a> for some mental health resources.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnz4yjec5asvf\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefz4yjec5asvf\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block200\">Hopefully not more literally similar. But: a new thing-not-imagined-very-well.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn1dgio52qsrw\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref1dgio52qsrw\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block202\">Modulo some futurisms. Including, importantly, ones predictably at stake in AI progress.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4m2cfburfm7\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4m2cfburfm7\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block204\">“In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time.”<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4y75ebmy7eh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4y75ebmy7eh\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block206\">Thanks to Katja Grace for discussion.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn5gj8wfkheeo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref5gj8wfkheeo\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block208\">Some forecasts have self-fulfilling elements, especially with respect to Moloch-like problems. And there are questions about e.g. internet text increasing the likelihood of AIs acting out the role of the scary-AI.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnh3ar4lqw084\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefh3ar4lqw084\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block210\">See e.g. Scott Alexander <a href=\"https://astralcodexten.substack.com/p/mantic-monday-31422\">here<\/a>. Some of <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably#fn6am2fn0yyve\">Yudkowsky’s public comments<\/a> suggest this model as well, though his original discussion of “<a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\">conservation of expected evidence<\/a>” does not.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn4ohd2xn7yql\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref4ohd2xn7yql\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block212\">Here I’m indebted to discussion from <a href=\"https://forum.effectivealtruism.org/posts/Lto9awEYPQNu9wkdi/rational-predictions-often-update-predictably\">Greg Lewis<\/a> and <a href=\"https://www.lesswrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence\">Abram Demski<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnc1l5t3jttlk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefc1l5t3jttlk\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block214\">.99*1 + .01*0 = .99.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fno3exn1kvfm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefo3exn1kvfm\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block216\">Or put another way: you want to find the area that red occupies, which is the area of the first, smaller red box, plus the area of the bigger red box. Each box occupies a percentage of the area of a “column” (combination of white box and red box) associated with a hypothesis about GPT-6. So to find the area of a given red box, you take the area of the column it’s in (that is, the probability on the relevant hypothesis about GPT-6), and multiply that by the percentage of that column that is red (e.g., the probability of doom conditional on that hypothesis). Then you add up the areas of the red boxes.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnt4taauzgccn\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnreft4taauzgccn\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block218\">Thanks to Daniel Kokotajlo for highlighting some of these dynamics to me years ago. See also his review of my power-seeking AI report <a href=\"https://docs.google.com/document/d/1GwT7AS_PWpglWWrVrpiMqeKiJ_E2VgAUIG5tTdVhVeM/edit#heading=h.e9o5m3fab0ua\">here<\/a>.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnv5zb8pjk25\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefv5zb8pjk25\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block220\">I added an edit to this effect.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnu9z9f1zd3ci\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefu9z9f1zd3ci\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block222\">Thanks to Katja Grace for discussion here.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn6gbdnzllii\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref6gbdnzllii\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block224\">Here I’m inspired by some comments from Richard Ngo.<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7yx0orvjyje\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7yx0orvjyje\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block226\">Though: maybe it just works out the same? E.g., the average of your estimates over time needs to obey Bayesian constraints?<\/p><\/div><\/li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn72q8oh08rre\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref72q8oh08rre\">^<\/a><\/strong><\/sup><\/span><div class=\"footnote-content\"><p id=\"block228\">Again, thanks to Katja for pointing to this dynamic.<\/p><\/div><\/li><\/ol>","commentsByBlock":{"block55":["SrBCxKcAcK7gLTQTd"],"block162":["wzS25hBpMMkeigqNh"],"block134":["ymuvrSv3kcL3cfTv4"]},"highKarmaCommentsByBlock":{"block55":["SrBCxKcAcK7gLTQTd"]}}},"Revision:v4c7eBHESey8Deyra_":{"_id":"v4c7eBHESey8Deyra_","__typename":"Revision","htmlHighlight":"<p>When predicting conditional probability of catastrophe from loss of human control over AGI, there are many distinct cruxes. This essay does not attempt a complete case, or the most generally convincing case, or addressing the most common cruxes.<\/p><p>Instead these are my best guesses for potentially mind-changing, armor-piercing questions people could ask themselves if they broadly accept many concepts like <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://t.co/KvPcIDIRgf\">power seeking<\/a> being a key existential risk, <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://t.co/BUlD74tuFs\">that default development paths are likely catastrophic<\/a> and that <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/\">AI could defeat all of us combined<\/a>, have read and thought hard about alignment difficulties, yet think the odds of catastrophe are not so high.<\/p><p>In addition to this entry, I attempt an incomplete extended list of cruxes <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://thezvi.substack.com/p/the-crux-list\">here<\/a>, an attempted taxonomy of paths through developing AGI and potentially losing control <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://thezvi.substack.com/p/stages-of-survival\">here<\/a>, and an attempted taxonomy of styles of alignment <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://thezvi.substack.com/p/types-and-degrees-of-alignment\">here<\/a>, while leaving to the future or others for now a taxonomy of <a target=\"_blank\" rel=\"noreferrer noopener\" href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities'\">alignment difficulties<\/a>.<\/p><p>Apologies in advance if some questions seem insulting or you rightfully answer with ‘no, I am not making that mistake.’ I don’t know a way around that.<\/p><p>Here are the questions up front:<\/p>\n\n\n\n<ol>\n<li>What happens?<\/li>\n\n\n\n<li>To what extent will humanity seek to avoid catastrophe?<\/li>\n\n\n\n<li>How much will humans willingly give up, including control?<\/li>\n\n\n\n<li>You know people and companies and nations are dumb and make dumb mistakes constantly, and mostly take symbolic actions or gesture at things rather than act strategically, and you’ve taken that into account, right?<\/li>\n\n\n\n<li>What would count as a catastrophe?<\/li>\n\n\n\n<li>Are you consistently tracking what you mean by alignment?<\/li>\n\n\n\n<li>Would ‘human-strength’ alignment be sufficient?<\/li>\n\n\n\n<li>If we figure out how to robustly align our AGIs, will we choose to and be able to make and keep them that way? Would we keep control?<\/li>\n\n\n\n<li>How much hope is there that a misaligned AGI would choose to preserve humanity once it no longer needed us?<\/li>\n\n\n\n<li>Are you factoring in unknown difficulties and surprises large and small that always arise, and in which direction do they point? re you treating doom as only happening through specified detailed logical paths, which if they break down mean it’s going to be fine?<\/li>\n\n\n\n<li>Are you properly propagating your updates, and anticipating future updates?<\/li>\n\n\n\n<li>Are you counting on in-distribution heuristics to work out of distribution?<\/li>\n\n\n\n<li>Are you using instincts and heuristics rather than looking at mechani<\/li><\/ol>...","wordCount":2568,"version":"1.1.0"},"Revision:v4c7eBHESey8Deyra_moderationGuidelines":{"_id":"v4c7eBHESey8Deyra_moderationGuidelines","__typename":"Revision","html":""},"Revision:v4c7eBHESey8Deyra_customHighlight":{"_id":"v4c7eBHESey8Deyra_customHighlight","__typename":"Revision","html":""},"User:N9zj5qpTfqmbn9dro":{"_id":"N9zj5qpTfqmbn9dro","__typename":"User","biography":null,"profileImageId":null,"moderationStyle":"norm-enforcing","bannedUserIds":null,"moderatorAssistance":true,"slug":"zvi","createdAt":"2009-03-31T20:54:54.077Z","username":"Zvi","displayName":"Zvi","previousDisplayName":null,"fullName":null,"karma":33835,"afKarma":73,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":591,"commentCount":1335,"sequenceCount":3,"afPostCount":1,"afCommentCount":3,"spamRiskScore":1,"tagRevisionCount":0},"Post:v4c7eBHESey8Deyra":{"_id":"v4c7eBHESey8Deyra","__typename":"Post","tagRelevance":{"sYm3HiWcfZvrGu3ui":1},"deletedDraft":false,"contents":{"__ref":"Revision:v4c7eBHESey8Deyra_"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":10,"rejectedReason":null,"disableRecommendation":false,"moderationGuidelines":{"__ref":"Revision:v4c7eBHESey8Deyra_moderationGuidelines"},"customHighlight":{"__ref":"Revision:v4c7eBHESey8Deyra_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"url":null,"postedAt":"2023-05-31T18:30:05.987Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-05-31T18:53:20.028Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"commentCount":null,"voteCount":22,"baseScore":78,"extendedScore":null,"unlisted":false,"score":0.023517273366451263,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2023-05-31T18:30:05.987Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"N9zj5qpTfqmbn9dro","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":31,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2023-05-31T18:30:05.990Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"reviewCount":null,"reviewVoteCount":null,"positiveReviewVoteCount":null,"group":null,"podcastEpisodeId":null,"nominationCount2019":null,"reviewCount2019":null,"votingSystem":"namesAttachedReactions","user":{"__ref":"User:N9zj5qpTfqmbn9dro"},"coauthors":[],"slug":"to-predict-what-happens-ask-what-happens","title":"To Predict What Happens, Ask What Happens","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false},"Revision:9fjSi2KzyzXAQxuza_":{"_id":"9fjSi2KzyzXAQxuza_","__typename":"Revision","htmlHighlight":"<p>(Cross-posted from my <a href=\"https://theviolethour.substack.com/p/alignment-goals-and-the-gut-head\">Substack<\/a>. Also on the <a href=\"https://forum.effectivealtruism.org/posts/2yyZqRParGeLEja5u/alignment-goals-and-the-gut-head-gap-a-review-of-ngo-et-al\">EA Forum<\/a>)<\/p><p>If someone asks whether I know of anyone who carefully examines the arguments for AI x-risk, my answer is a pretty resounding ‘Ngo’.&nbsp;<\/p><p>Richard Ngo, specifically. Among&nbsp;<a href=\"https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ\"><u>other<\/u><\/a>&nbsp;<a href=\"https://www.lesswrong.com/posts/Cuig4qe8m2aqBCJtZ/which-values-are-stable-under-ontology-shifts\"><u>cool<\/u><\/a>&nbsp;<a href=\"https://www.lesswrong.com/posts/cdB5f2adKoLGW8Ytc/book-review-very-important-people\"><u>pieces<\/u><\/a>, he co-authored&nbsp;<a href=\"https://arxiv.org/abs/2209.00626\"><i><u>The Alignment Problem from a Deep Learning Perspective<\/u><\/i><\/a>. His co-authors are Lawrence Chan and Sören Mindermann, who very much deserve mentioning, even if their names are less amenable to a pun-based introduction.<\/p><p>Reviewers liked the paper. Despite this, I was disappointed to learn that the chair nevertheless decided to&nbsp;<a href=\"https://twitter.com/RichardMCNgo/status/1652042195803987968\"><u>veto acceptance of the paper<\/u><\/a> on grounds that it was “too speculative”. Disappointed because I feel like the topic is&nbsp;<i>worthy of discussion<\/i>. And LessWrong, for all its foibles, provides a platform for informed speculation. So it’s here that I’ll speculate.&nbsp;<\/p><h2>Problem: The Difficulty of Alignment<\/h2><p>You’re probably aware of the alignment problem, so we’ll only briefly recount the authors’ preferred framing.<\/p><p>Empirically, we observe ML models with ever-increasing capabilities. As we train ever-more sophisticated models, it becomes harder and harder to ensure that our models are actually trying to do the things we want. This allows us to characterize the alignment problem in terms of “three emergent properties” which could arise as a result using RL to train an AGI.<\/p><ol><li><i><strong>Deceptive reward hacking<\/strong><\/i> which exploits imperfect reward functions.&nbsp;<\/li><li><i><strong>Internally represented goals<\/strong><\/i> which generalize<strong>&nbsp;<\/strong>beyond the training distribution<strong>.<\/strong><\/li><li><i><strong>Power-seeking behavior<\/strong><\/i><strong>&nbsp;<\/strong>as a means to effectively pursue their goals.<\/li><\/ol><p>Conceptually, the argument from here is pretty simple. Ngo. et al provide evidence in support of these mechanisms from contemporary ML. These mechanisms already exist, and we can see them. If they’re present in vastly more capable systems, we might end up with human disempowerment.&nbsp;&nbsp;&nbsp;&nbsp;<\/p><p>The discussion starts gently, if worryingly.&nbsp; Empirically, we already have systems that reward hack.&nbsp;You train a system to grab a ball with its claw; it learns to&nbsp;<a href=\"https://openai.com/research/learning-from-human-preferences\"><u>place the claw between the camera and the ball<\/u><\/a> so that it appears to be grasping the ball. Also, current LLMs are – to some degree, at least – situationally aware.&nbsp;Bing’s Syndney&nbsp;<a href=\"https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\"><u>recognizes<\/u><\/a> search results about itself;&nbsp;ChatGPT can make (fallible)&nbsp;<a href=\"https://www.engraved.blog/building-a-virtual-machine-inside/\"><u>inferences<\/u><\/a> about the source code it might be ... <\/p>","wordCount":3937,"version":"1.0.0"},"Revision:9fjSi2KzyzXAQxuza_moderationGuidelines":{"_id":"9fjSi2KzyzXAQxuza_moderationGuidelines","__typename":"Revision","html":""},"Revision:ZFrgTgzwEfStg26JL_description":{"_id":"ZFrgTgzwEfStg26JL_description","__typename":"Revision","htmlHighlight":"<p><strong>AI Risk<\/strong> is analysis of the risks associated with building powerful AI systems.<\/p><p><i>Related: <\/i><a href=\"https://www.lesswrong.com/tag/ai\"><i>AI<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis\"><i>Orthogonality thesis<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><i>Complexity of value<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><i>Goodhart's law<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\"><i>Paperclip maximiser<\/i><\/a><\/p>"},"Tag:ZFrgTgzwEfStg26JL":{"_id":"ZFrgTgzwEfStg26JL","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:ZFrgTgzwEfStg26JL_description"},"canVoteOnRels":null,"userId":"EQNTWXLKMeWMp2FQS","name":"AI Risk","shortName":null,"slug":"ai-risk","core":false,"postCount":1028,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-16T10:29:25.410Z","wikiOnly":false,"deleted":false,"isSubforum":null,"noindex":false},"User:xfNwhRi6fbA8SnvWA":{"_id":"xfNwhRi6fbA8SnvWA","__typename":"User","biography":null,"profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"violet-hour","createdAt":"2023-04-27T11:40:54.013Z","username":"Violet Hour","displayName":"Violet Hour","previousDisplayName":null,"fullName":null,"karma":17,"afKarma":null,"deleted":null,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":2,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":null},"Post:9fjSi2KzyzXAQxuza":{"_id":"9fjSi2KzyzXAQxuza","__typename":"Post","tagRelevance":{"ZFrgTgzwEfStg26JL":1,"sYm3HiWcfZvrGu3ui":2},"deletedDraft":false,"contents":{"__ref":"Revision:9fjSi2KzyzXAQxuza_"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":16,"rejectedReason":null,"disableRecommendation":false,"moderationGuidelines":{"__ref":"Revision:9fjSi2KzyzXAQxuza_moderationGuidelines"},"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:ZFrgTgzwEfStg26JL"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"url":null,"postedAt":"2023-05-11T18:06:18.962Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-05-11T21:21:32.047Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"commentCount":2,"voteCount":8,"baseScore":18,"extendedScore":null,"unlisted":false,"score":0.005186188966035843,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2023-05-18T13:20:25.008Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"xfNwhRi6fbA8SnvWA","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":6,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2023-05-11T09:17:17.234Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"reviewCount":null,"reviewVoteCount":null,"positiveReviewVoteCount":null,"group":null,"podcastEpisodeId":null,"nominationCount2019":null,"reviewCount2019":null,"votingSystem":"twoAxis","user":{"__ref":"User:xfNwhRi6fbA8SnvWA"},"coauthors":[],"slug":"alignment-goals-and-the-gut-head-gap-a-review-of-ngo-et-al","title":"Alignment, Goals, and The Gut-Head Gap: A Review of Ngo. et al.","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false}}
        ]]>
        </script> 
        <script>
        <![CDATA[
        window.__APOLLO_FOREIGN_STATE__ = {}
        ]]>
        </script> 
        <script src="https://www.google.com/recaptcha/api.js?render=6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"></script><iframe id="intercom-frame" style="position: absolute !important; opacity: 0 !important; width: 1px !important; height: 1px !important; top: 0 !important; left: 0 !important; border: none !important; display: block !important; z-index: -1 !important; pointer-events: none;" aria-hidden="true" tabindex="-1" title="Intercom" name="intercom-frame"></iframe>
        <div>
            <div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;">
                <div class="grecaptcha-logo">
                    <iframe title="reCAPTCHA" src="https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc&amp;co=aHR0cHM6Ly93d3cubGVzc3dyb25nLmNvbTo0NDM.&amp;hl=en&amp;v=iRvKkcsnpNcOYYwhqaQxPITz&amp;size=invisible&amp;cb=hgic5jnps1lv" width="256" height="60" role="presentation" name="a-ecz4ucx1p6jj" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox" id="a-ecz4ucx1p6jj"></iframe>
                </div>
                <div class="grecaptcha-error"></div>
                <textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea>
            </div><iframe style="display: none;"></iframe>
        </div>
        <div class="intercom-lightweight-app">
            <div class="intercom-lightweight-app-launcher intercom-launcher" role="button" tabindex="0" aria-label="Open Intercom Messenger" aria-live="polite">
                <div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-open">
                    <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 28 32">
                    <path d="M28 32s-4.714-1.855-8.527-3.34H3.437C1.54 28.66 0 27.026 0 25.013V3.644C0 1.633 1.54 0 3.437 0h21.125c1.898 0 3.437 1.632 3.437 3.645v18.404H28V32zm-4.139-11.982a.88.88 0 00-1.292-.105c-.03.026-3.015 2.681-8.57 2.681-5.486 0-8.517-2.636-8.571-2.684a.88.88 0 00-1.29.107 1.01 1.01 0 00-.219.708.992.992 0 00.318.664c.142.128 3.537 3.15 9.762 3.15 6.226 0 9.621-3.022 9.763-3.15a.992.992 0 00.317-.664 1.01 1.01 0 00-.218-.707z"></path></svg>
                </div>
                <div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-minimize">
                    <svg width="24" height="24" viewbox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                    <path fill-rule="evenodd" clip-rule="evenodd" d="M18.601 8.39897C18.269 8.06702 17.7309 8.06702 17.3989 8.39897L12 13.7979L6.60099 8.39897C6.26904 8.06702 5.73086 8.06702 5.39891 8.39897C5.06696 8.73091 5.06696 9.2691 5.39891 9.60105L11.3989 15.601C11.7309 15.933 12.269 15.933 12.601 15.601L18.601 9.60105C18.9329 9.2691 18.9329 8.73091 18.601 8.39897Z" fill="white"></path></svg>
                </div>
            </div>
        </div>
        <div class="ck-body-wrapper">
            <div class="ck ck-reset_all ck-body ck-rounded-corners" dir="ltr">
                <div class="ck ck-balloon-panel ck-balloon-panel_arrow_nw ck-balloon-panel_with-arrow" style="top: 0px; left: 0px;">
                    <div class="ck ck-balloon-rotator" z-index="-1">
                        <div class="ck-balloon-rotator__navigation ck-hidden">
                            <button class="ck ck-button ck-off" type="button" tabindex="-1" aria-labelledby="ck-editor__aria-label_ead030ec34089d5d8a808cc4fc62c23f4"><svg class="ck ck-icon ck-button__icon" viewbox="0 0 20 20">
                            <path d="M11.463 5.187a.888.888 0 1 1 1.254 1.255L9.16 10l3.557 3.557a.888.888 0 1 1-1.254 1.255L7.26 10.61a.888.888 0 0 1 .16-1.382l4.043-4.042z"></path></svg><span class="ck ck-tooltip ck-tooltip_s"><span class="ck ck-tooltip__text">Previous</span></span><span class="ck ck-button__label" id="ck-editor__aria-label_ead030ec34089d5d8a808cc4fc62c23f4">Previous</span></button><button class="ck ck-button ck-off" type="button" tabindex="-1" aria-labelledby="ck-editor__aria-label_e30b070977da430f96b5c825bfd0c5bdc"><svg class="ck ck-icon ck-button__icon" viewbox="0 0 20 20">
                            <path d="M8.537 14.813a.888.888 0 1 1-1.254-1.255L10.84 10 7.283 6.442a.888.888 0 1 1 1.254-1.255L12.74 9.39a.888.888 0 0 1-.16 1.382l-4.043 4.042z"></path></svg><span class="ck ck-tooltip ck-tooltip_s"><span class="ck ck-tooltip__text">Next</span></span><span class="ck ck-button__label" id="ck-editor__aria-label_e30b070977da430f96b5c825bfd0c5bdc">Next</span></button>
                        </div>
                        <div class="ck-balloon-rotator__content"></div>
                    </div>
                </div>
                <div class="ck-fake-panel ck-hidden" style="top: 0px; left: 0px; width: 0px; height: 0px;"></div>
            </div>
        </div>
    </body>
</html>
